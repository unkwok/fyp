{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949850d7",
   "metadata": {},
   "source": [
    "### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad005eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from stockstats import StockDataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03bed7",
   "metadata": {},
   "source": [
    "### Set the data source path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2436b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data source path\n",
    "interval = \"daily\"\n",
    "region = \"us\"\n",
    "ex_product = \"nasdaq stocks\"\n",
    "section = \"1\"\n",
    "stock = \"aapl\"\n",
    "data_path = \"test_data/\"+interval+\"/\"+region+\"/\"+ex_product+\"/\"+section+\"/\"+stock+\".\"+region+\".txt\"\n",
    "\n",
    "# Use Apple .Inc stock for training\n",
    "\n",
    "# Extract only the OLHC\n",
    "column_to_use = [\"OPEN\",\"LOW\",\"HIGH\",\"CLOSE\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f91fe",
   "metadata": {},
   "source": [
    "### Load the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92c24ac-3419-4a42-973b-e21ae036c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ori_data = pd.read_csv(data_path, sep=\",\")\n",
    "\n",
    "# Rename the column names\n",
    "ori_data.columns = [colname[1:-1] for colname in ori_data.columns]\n",
    "\n",
    "# Drop the unnecessary\n",
    "ori_data.index = ori_data[\"DATE\"]\n",
    "ori_data = ori_data.drop(columns=['DATE','PER','TIME', 'TICKER', 'OPENINT'])\n",
    "ori_data.columns = [\"open\",\"high\",\"low\",\"close\",\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966120a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use online package to generate additional features\n",
    "x = StockDataFrame(ori_data)\n",
    "data = x[['open','high','low','close','volume',\n",
    "          'boll', 'boll_ub', 'boll_lb',\n",
    "          'macd', 'macdh', 'macds',\n",
    "          'rsi_11', 'rsi_14', 'rsi_21']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f7671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>boll</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdh</th>\n",
       "      <th>macds</th>\n",
       "      <th>rsi_11</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19840907</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10028</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>96970899</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840910</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.09905</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>75265237</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.102049</td>\n",
       "      <td>0.100351</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840911</th>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10456</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>177479896</td>\n",
       "      <td>0.101713</td>\n",
       "      <td>0.103590</td>\n",
       "      <td>0.099837</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>77.134146</td>\n",
       "      <td>76.758045</td>\n",
       "      <td>76.303318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840912</th>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10334</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>155043826</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.103762</td>\n",
       "      <td>0.098638</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>32.201239</td>\n",
       "      <td>32.592743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840913</th>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10548</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>241475025</td>\n",
       "      <td>0.101996</td>\n",
       "      <td>0.106191</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>68.412723</td>\n",
       "      <td>68.025100</td>\n",
       "      <td>67.561551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211021</th>\n",
       "      <td>148.81000</td>\n",
       "      <td>149.64000</td>\n",
       "      <td>147.87000</td>\n",
       "      <td>149.48000</td>\n",
       "      <td>61420990</td>\n",
       "      <td>143.875000</td>\n",
       "      <td>149.793245</td>\n",
       "      <td>137.956755</td>\n",
       "      <td>0.375617</td>\n",
       "      <td>1.069014</td>\n",
       "      <td>-0.693396</td>\n",
       "      <td>65.673205</td>\n",
       "      <td>61.532287</td>\n",
       "      <td>57.199864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022</th>\n",
       "      <td>149.69000</td>\n",
       "      <td>150.18000</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>148.69000</td>\n",
       "      <td>58883443</td>\n",
       "      <td>143.963500</td>\n",
       "      <td>150.121546</td>\n",
       "      <td>137.805454</td>\n",
       "      <td>0.577001</td>\n",
       "      <td>1.016318</td>\n",
       "      <td>-0.439317</td>\n",
       "      <td>61.924694</td>\n",
       "      <td>58.867114</td>\n",
       "      <td>55.611436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025</th>\n",
       "      <td>148.68000</td>\n",
       "      <td>149.37000</td>\n",
       "      <td>147.62110</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>50720556</td>\n",
       "      <td>144.127000</td>\n",
       "      <td>150.607481</td>\n",
       "      <td>137.646519</td>\n",
       "      <td>0.724216</td>\n",
       "      <td>0.930826</td>\n",
       "      <td>-0.206610</td>\n",
       "      <td>61.679591</td>\n",
       "      <td>58.693837</td>\n",
       "      <td>55.508996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211026</th>\n",
       "      <td>149.33000</td>\n",
       "      <td>150.84000</td>\n",
       "      <td>149.01010</td>\n",
       "      <td>149.32000</td>\n",
       "      <td>60893395</td>\n",
       "      <td>144.497500</td>\n",
       "      <td>151.284341</td>\n",
       "      <td>137.710659</td>\n",
       "      <td>0.885547</td>\n",
       "      <td>0.873726</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>63.821803</td>\n",
       "      <td>60.401009</td>\n",
       "      <td>56.649320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211027</th>\n",
       "      <td>149.36000</td>\n",
       "      <td>149.73000</td>\n",
       "      <td>148.49000</td>\n",
       "      <td>148.85000</td>\n",
       "      <td>55925403</td>\n",
       "      <td>144.798500</td>\n",
       "      <td>151.804399</td>\n",
       "      <td>137.792601</td>\n",
       "      <td>0.964362</td>\n",
       "      <td>0.762033</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>61.219811</td>\n",
       "      <td>58.598318</td>\n",
       "      <td>55.614834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9362 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               open       high        low      close     volume        boll  \\\n",
       "DATE                                                                          \n",
       "19840907    0.10150    0.10274    0.10028    0.10150   96970899    0.101500   \n",
       "19840910    0.10150    0.10181    0.09905    0.10090   75265237    0.101200   \n",
       "19840911    0.10181    0.10456    0.10181    0.10274  177479896    0.101713   \n",
       "19840912    0.10274    0.10334    0.09966    0.09966  155043826    0.101200   \n",
       "19840913    0.10518    0.10548    0.10518    0.10518  241475025    0.101996   \n",
       "...             ...        ...        ...        ...        ...         ...   \n",
       "20211021  148.81000  149.64000  147.87000  149.48000   61420990  143.875000   \n",
       "20211022  149.69000  150.18000  148.64000  148.69000   58883443  143.963500   \n",
       "20211025  148.68000  149.37000  147.62110  148.64000   50720556  144.127000   \n",
       "20211026  149.33000  150.84000  149.01010  149.32000   60893395  144.497500   \n",
       "20211027  149.36000  149.73000  148.49000  148.85000   55925403  144.798500   \n",
       "\n",
       "             boll_ub     boll_lb      macd     macdh     macds     rsi_11  \\\n",
       "DATE                                                                        \n",
       "19840907         NaN         NaN  0.000000  0.000000  0.000000        NaN   \n",
       "19840910    0.102049    0.100351 -0.000013 -0.000006 -0.000007   0.000000   \n",
       "19840911    0.103590    0.099837  0.000040  0.000028  0.000012  77.134146   \n",
       "19840912    0.103762    0.098638 -0.000048 -0.000040 -0.000008  31.870001   \n",
       "19840913    0.106191    0.097801  0.000125  0.000094  0.000031  68.412723   \n",
       "...              ...         ...       ...       ...       ...        ...   \n",
       "20211021  149.793245  137.956755  0.375617  1.069014 -0.693396  65.673205   \n",
       "20211022  150.121546  137.805454  0.577001  1.016318 -0.439317  61.924694   \n",
       "20211025  150.607481  137.646519  0.724216  0.930826 -0.206610  61.679591   \n",
       "20211026  151.284341  137.710659  0.885547  0.873726  0.011821  63.821803   \n",
       "20211027  151.804399  137.792601  0.964362  0.762033  0.202329  61.219811   \n",
       "\n",
       "             rsi_14     rsi_21  \n",
       "DATE                            \n",
       "19840907        NaN        NaN  \n",
       "19840910   0.000000   0.000000  \n",
       "19840911  76.758045  76.303318  \n",
       "19840912  32.201239  32.592743  \n",
       "19840913  68.025100  67.561551  \n",
       "...             ...        ...  \n",
       "20211021  61.532287  57.199864  \n",
       "20211022  58.867114  55.611436  \n",
       "20211025  58.693837  55.508996  \n",
       "20211026  60.401009  56.649320  \n",
       "20211027  58.598318  55.614834  \n",
       "\n",
       "[9362 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1be9c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test data\n",
    "\n",
    "def custom_split(data,start,end):\n",
    "    train = (data.index >= start) & (data.index <= end)\n",
    "    train_X = data[train]\n",
    "    \n",
    "    return train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb65f22",
   "metadata": {},
   "source": [
    "# CNN_LSTM (Price Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cd6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = custom_split(data,start = 20130101,end = 20171031)\n",
    "valid_X = custom_split(data,start = 20171101,end = 20181231)\n",
    "test_X = custom_split(data,start = 20190101,end = 20201231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce609a",
   "metadata": {},
   "source": [
    "### Label the target result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e4e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we use 10 days price data to predict opening price of the 11th day\n",
    "num_day_to_predict = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f127afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_result_target_price(X,num_day,result_col_name = \"result_price\"):\n",
    "    y = pd.DataFrame(np.nan, index=X.index, columns=[result_col_name])\n",
    "    for i in range(len(X)-num_day):\n",
    "        y.iloc[i+num_day_to_predict,0] = X.iloc[i+num_day,0]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fcaaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = produce_result_target_price(train_X,num_day_to_predict)\n",
    "valid_y = produce_result_target_price(valid_X,num_day_to_predict)\n",
    "test_y = produce_result_target_price(test_X,num_day_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1064e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20171101</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171102</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171103</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171106</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171107</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181224</th>\n",
       "      <td>36.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181226</th>\n",
       "      <td>36.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181227</th>\n",
       "      <td>37.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181228</th>\n",
       "      <td>38.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181231</th>\n",
       "      <td>38.526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          result_price\n",
       "DATE                  \n",
       "20171101           NaN\n",
       "20171102           NaN\n",
       "20171103           NaN\n",
       "20171106           NaN\n",
       "20171107           NaN\n",
       "...                ...\n",
       "20181224        36.007\n",
       "20181226        36.044\n",
       "20181227        37.876\n",
       "20181228        38.280\n",
       "20181231        38.526\n",
       "\n",
       "[292 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a8ca1",
   "metadata": {},
   "source": [
    "### Transform the X, y data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b8773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_to_tensor(X,y,num_day):\n",
    "    # Initiate tensor for X\n",
    "    x_first = X.iloc[0:num_day,:]\n",
    "    x_mean = x_first.mean(axis=0) # Get the mean of the 10-day frame\n",
    "    x_std = x_first.std(axis=0) # Get the std of the 10-day frame\n",
    "    x_first = x_first.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "    \n",
    "    # Initiate tensor for y\n",
    "    x_open = X.iloc[0:num_day,0]\n",
    "    y_val = y.iloc[num_day,:] # Get the corresponding y\n",
    "    y_val = y_val.sub(x_open.mean(axis=0)).div(x_open.std(axis=0)) # Normalize the y\n",
    "    \n",
    "    x_tf_data = [tf.convert_to_tensor(np.array(x_first),dtype = tf.float32)]\n",
    "    y_tf_data = [tf.convert_to_tensor(np.array(y_val),dtype = tf.float32)]\n",
    "    \n",
    "    for i in range(1,len(X)-num_day):   \n",
    "        x_window = X.iloc[i:i+num_day,:] # Set the window as a 10-day frame \n",
    "        x_mean = x_window.mean(axis=0) # Get the mean of the 10-day frame\n",
    "        x_std = x_window.std(axis=0) # Get the std of the 10-day frame\n",
    "        x_window = x_window.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "        \n",
    "        x_open = X.iloc[i:i+num_day,0] # Get the opening price of the 10-day frame\n",
    "        y_val = y.iloc[i+num_day,:] # Get the corresponding y\n",
    "        y_val = y_val.sub(x_open.mean(axis=0)).div(x_open.std(axis=0)) # Normalize the y\n",
    "        \n",
    "        x_next_tf = tf.convert_to_tensor(np.array(x_window),dtype = tf.float32)\n",
    "        x_tf_data = tf.concat([x_tf_data, [x_next_tf]], 0)\n",
    "        \n",
    "        y_next_tf = tf.convert_to_tensor(np.array(y_val),dtype = tf.float32)\n",
    "        y_tf_data = tf.concat([y_tf_data, [y_next_tf]], 0)\n",
    "    return (tf.reshape(x_tf_data,(-1,10,14,1)),y_tf_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b66168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:10:12.920175: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-12 16:10:12.920707: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "tf_train_X,tf_train_y = transform_data_to_tensor(train_X,train_y,num_day_to_predict)\n",
    "tf_valid_X,tf_valid_y = transform_data_to_tensor(valid_X,valid_y,num_day_to_predict)\n",
    "tf_test_X,tf_test_y = transform_data_to_tensor(test_X,test_y,num_day_to_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f49a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1208, 10, 14, 1)\n",
      "(1208, 1)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n",
      "(282, 10, 14, 1)\n",
      "(282, 1)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n",
      "(495, 10, 14, 1)\n",
      "(495, 1)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(tf_train_X.shape)\n",
    "print(tf_train_y.shape)\n",
    "print(tf_train_X.dtype)\n",
    "print(tf_train_y.dtype)\n",
    "\n",
    "print(tf_valid_X.shape)\n",
    "print(tf_valid_y.shape)\n",
    "print(tf_valid_X.dtype)\n",
    "print(tf_valid_y.dtype)\n",
    "\n",
    "print(tf_test_X.shape)\n",
    "print(tf_test_y.shape)\n",
    "print(tf_test_X.dtype)\n",
    "print(tf_test_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e64c36",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def myModel(input_shape,\n",
    "            encoder_unit = 100,\n",
    "            repeat_vector_n = 10):\n",
    "    \n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    print(\"Input: \",inputs.shape)\n",
    "    \n",
    "    # First Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 64,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"1 Cov: \",x.shape)\n",
    "    \n",
    "    # Second Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 16,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"2 Cov: \",x.shape)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = layers.Flatten()(x)\n",
    "    print(\"Flatten: \",x.shape)\n",
    "    \n",
    "    # Repeat Vector Layer\n",
    "    x = layers.RepeatVector(n = repeat_vector_n)(x)\n",
    "    print(\"RepeatVector: \",x.shape)\n",
    "    \n",
    "    # Connect to LSTM\n",
    "    x = layers.LSTM(units = encoder_unit, input_shape=(5,1))(x)\n",
    "    print(\"LSTM: \",x.shape)\n",
    "    \n",
    "    # Add the Dense Layer with relu activation\n",
    "    x = layers.Dense(units = 50,activation = \"relu\")(x)\n",
    "    print(\"1 Dense: \",x.shape)\n",
    "    \n",
    "    # Add the last Dense Layer with sigmoid activation\n",
    "    outputs = layers.Dense(units = 1,activation = \"sigmoid\")(x)\n",
    "    print(\"Output: \",outputs.shape)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fc4f2",
   "metadata": {},
   "source": [
    "### Model Training and Fitting and Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72215124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:10:51.276742: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_18606_18788' and '__inference___backward_standard_lstm_18912_19397_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_19570' both implement 'lstm_de1b7df4-cf8a-448b-bc07-e0e3230875cf' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 18s 15ms/sample - loss: 1.2080 - mean_squared_error: 2.6683\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1435 - mean_squared_error: 2.4437\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.1031 - mean_squared_error: 2.3636\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0714 - mean_squared_error: 2.2311\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0908 - mean_squared_error: 2.2741\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0545 - mean_squared_error: 2.1642\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0304 - mean_squared_error: 2.0970\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0464 - mean_squared_error: 2.1867\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0144 - mean_squared_error: 2.1571\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0458 - mean_squared_error: 2.2095\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.9829 - mean_squared_error: 1.9308\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0258 - mean_squared_error: 2.1814\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9830 - mean_squared_error: 1.9898\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0069 - mean_squared_error: 2.0349\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0262 - mean_squared_error: 2.1668\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0236 - mean_squared_error: 2.1446\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9640 - mean_squared_error: 1.9436\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0226 - mean_squared_error: 2.2288\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9610 - mean_squared_error: 1.8974\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0461 - mean_squared_error: 2.2522\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9541 - mean_squared_error: 1.9077\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0002 - mean_squared_error: 2.0878\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9999 - mean_squared_error: 2.0339\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9675 - mean_squared_error: 1.9471\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0209 - mean_squared_error: 2.3095\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 0.9656 - mean_squared_error: 1.9095\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9850 - mean_squared_error: 2.0778\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9743 - mean_squared_error: 2.0345\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9689 - mean_squared_error: 1.9845\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0139 - mean_squared_error: 2.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:15:18.738011: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_24508' and '__inference_standard_lstm_24397_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_24748' both implement 'lstm_903eee1f-fcd3-41b6-be3a-632ce4c5c846' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 1.2395 - mean_squared_error: 1.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0452159355718194, 1.8809282]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0452159355718194, 1.8809282]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:15:23.625665: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_27785_28270' and '__inference___backward_standard_lstm_27785_28270_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_28443' both implement 'lstm_944ea858-5a41-4ef5-a973-bfe2dda40ffc' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 15s 12ms/sample - loss: 1.1378 - mean_squared_error: 2.3878\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.2572 - mean_squared_error: 2.7414\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.3630 - mean_squared_error: 3.0532\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.3521 - mean_squared_error: 3.0044\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3552 - mean_squared_error: 3.0118\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.3606 - mean_squared_error: 3.0503\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.4003 - mean_squared_error: 3.2458\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3146 - mean_squared_error: 2.8238\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3619 - mean_squared_error: 2.9964\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3696 - mean_squared_error: 3.0902\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3456 - mean_squared_error: 3.0314\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0969 - mean_squared_error: 2.3115\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0427 - mean_squared_error: 2.1240\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0624 - mean_squared_error: 2.2614\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0437 - mean_squared_error: 2.1201\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0521 - mean_squared_error: 2.2678\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0880 - mean_squared_error: 2.1369\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0676 - mean_squared_error: 2.2931\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0649 - mean_squared_error: 2.1470\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0454 - mean_squared_error: 2.2020\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0188 - mean_squared_error: 2.1416\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0772 - mean_squared_error: 2.3557\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0139 - mean_squared_error: 1.9749\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0746 - mean_squared_error: 2.3465\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0328 - mean_squared_error: 1.9743\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0565 - mean_squared_error: 2.2442\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0642 - mean_squared_error: 2.2262\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9998 - mean_squared_error: 2.0601\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0441 - mean_squared_error: 2.1510\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0031 - mean_squared_error: 2.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:18:52.073476: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_33381' and '__inference_standard_lstm_33270_specialized_for_model_1_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_33621' both implement 'lstm_c270ec5e-8b70-4218-9f6c-ea09c904fbe7' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2396 - mean_squared_error: 1.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.052262969896303, 1.8932962]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.052262969896303, 1.8932962]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:18:55.022257: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_36352_36534' and '__inference___backward_standard_lstm_36658_37143_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_37316' both implement 'lstm_7b5e2f54-3c27-4cde-9af4-5009b0effe01' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.1765 - mean_squared_error: 2.5422\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1004 - mean_squared_error: 2.3625\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0705 - mean_squared_error: 2.2289\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0419 - mean_squared_error: 2.1247\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0343 - mean_squared_error: 2.1979\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0316 - mean_squared_error: 2.0994\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0209 - mean_squared_error: 2.0619\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0728 - mean_squared_error: 2.2918\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0449 - mean_squared_error: 2.2291\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0705 - mean_squared_error: 2.2267\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0276 - mean_squared_error: 1.9535\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0502 - mean_squared_error: 2.2495\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0946 - mean_squared_error: 2.2579\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0681 - mean_squared_error: 2.2630\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0689 - mean_squared_error: 2.1843\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0957 - mean_squared_error: 2.3985\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0296 - mean_squared_error: 2.1871\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1161 - mean_squared_error: 2.3147\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1369 - mean_squared_error: 2.3502\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0732 - mean_squared_error: 2.1708\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0697 - mean_squared_error: 2.1890\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1059 - mean_squared_error: 2.4329\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0533 - mean_squared_error: 2.0984\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0550 - mean_squared_error: 2.1984\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0473 - mean_squared_error: 2.0553\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1053 - mean_squared_error: 2.3953\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0524 - mean_squared_error: 2.1154\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0278 - mean_squared_error: 2.0249\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0387 - mean_squared_error: 2.1049\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1061 - mean_squared_error: 2.4132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:22:03.575298: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_42143' and '__inference_standard_lstm_42143_specialized_for_model_2_lstm_2_StatefulPartitionedCall_at___inference_distributed_function_42494' both implement 'lstm_4e434bdf-98b3-4b24-a06d-7c554b74396d' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2824 - mean_squared_error: 1.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0715465038380725, 1.9652798]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0715465038380725, 1.9652798]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:22:07.149239: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_45531_46016' and '__inference___backward_standard_lstm_45531_46016_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_46189' both implement 'lstm_41df7be8-0af2-452e-a77d-3caf197f50e1' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1552 - mean_squared_error: 2.4699\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1211 - mean_squared_error: 2.4261\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0842 - mean_squared_error: 2.3545\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0354 - mean_squared_error: 2.0506\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0596 - mean_squared_error: 2.1810\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0441 - mean_squared_error: 2.2779\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0253 - mean_squared_error: 2.0260\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0277 - mean_squared_error: 2.0909\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0140 - mean_squared_error: 2.1501\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0235 - mean_squared_error: 2.0271\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0427 - mean_squared_error: 2.1102\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0357 - mean_squared_error: 2.1529\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1329 - mean_squared_error: 2.5613\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0525 - mean_squared_error: 2.2104\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0292 - mean_squared_error: 2.0946\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0641 - mean_squared_error: 2.2981\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9875 - mean_squared_error: 1.8722\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0459 - mean_squared_error: 2.1722\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0297 - mean_squared_error: 2.2379\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0362 - mean_squared_error: 1.9961\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0454 - mean_squared_error: 2.3038\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0219 - mean_squared_error: 2.1183\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0269 - mean_squared_error: 2.1105\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0024 - mean_squared_error: 1.9486\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0475 - mean_squared_error: 2.2702\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0542 - mean_squared_error: 2.1564\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0588 - mean_squared_error: 2.3054\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0197 - mean_squared_error: 2.0540\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0446 - mean_squared_error: 2.1361\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0732 - mean_squared_error: 2.3403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:25:39.819050: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_51016' and '__inference_standard_lstm_51016_specialized_for_model_3_lstm_3_StatefulPartitionedCall_at___inference_distributed_function_51367' both implement 'lstm_9c9c8712-a52f-4abf-8074-c1f7d6a3abfa' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 1.2552 - mean_squared_error: 1.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0779119193131197, 1.934911]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0779119193131197, 1.934911]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:25:43.325351: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_54404_54889' and '__inference___backward_standard_lstm_54404_54889_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_55062' both implement 'lstm_b47c8c97-60b6-4dc4-b66f-65156bfcb62f' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.1920 - mean_squared_error: 2.5645\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0804 - mean_squared_error: 2.2378\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0509 - mean_squared_error: 2.1651\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0865 - mean_squared_error: 2.3298\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0309 - mean_squared_error: 2.0043\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0709 - mean_squared_error: 2.2982\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9981 - mean_squared_error: 1.9407\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0363 - mean_squared_error: 2.2148\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0615 - mean_squared_error: 2.2069\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0057 - mean_squared_error: 2.0097\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0349 - mean_squared_error: 2.2381\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9809 - mean_squared_error: 1.8109\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0227 - mean_squared_error: 2.1933\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0030 - mean_squared_error: 2.0486\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9934 - mean_squared_error: 2.0422\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0153 - mean_squared_error: 2.1188\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9697 - mean_squared_error: 1.9675\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9959 - mean_squared_error: 2.1067\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0103 - mean_squared_error: 2.0592\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0265 - mean_squared_error: 2.3334\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9728 - mean_squared_error: 1.8541\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9916 - mean_squared_error: 2.0089\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 13s 11ms/sample - loss: 1.0288 - mean_squared_error: 2.2119\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 16s 13ms/sample - loss: 0.9607 - mean_squared_error: 2.1138\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9878 - mean_squared_error: 1.9256\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9803 - mean_squared_error: 2.1044\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9638 - mean_squared_error: 1.8847\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0039 - mean_squared_error: 2.1696\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9565 - mean_squared_error: 2.0107\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0037 - mean_squared_error: 2.0126\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9765 - mean_squared_error: 2.0854\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9942 - mean_squared_error: 2.1619\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9149 - mean_squared_error: 1.8328\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9798 - mean_squared_error: 2.0932\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9501 - mean_squared_error: 1.9844\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9797 - mean_squared_error: 2.1311\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9614 - mean_squared_error: 2.0621\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9754 - mean_squared_error: 2.0935\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9248 - mean_squared_error: 1.9126\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9891 - mean_squared_error: 2.0675\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9523 - mean_squared_error: 2.0027\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9513 - mean_squared_error: 1.9638\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9606 - mean_squared_error: 2.0250\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9701 - mean_squared_error: 2.0894\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9513 - mean_squared_error: 1.9585\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9512 - mean_squared_error: 1.9394\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9732 - mean_squared_error: 2.1526\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9420 - mean_squared_error: 1.9797\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 0.9540 - mean_squared_error: 2.0220\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 0.9574 - mean_squared_error: 2.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:31:05.389125: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_63040' and '__inference_standard_lstm_62929_specialized_for_model_4_lstm_4_StatefulPartitionedCall_at___inference_distributed_function_63280' both implement 'lstm_c119b11f-a367-435c-97bd-40c736795289' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2399 - mean_squared_error: 1.8220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0216271382697084, 1.8219554]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0216271382697084, 1.8219554]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:31:08.495262: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_66317_66802_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_66975' and '__inference___backward_cudnn_lstm_with_fallback_66011_66193' both implement 'lstm_8aabda42-578b-44ae-a9f6-9b9ca0aaf801' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.1744 - mean_squared_error: 2.5390\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0743 - mean_squared_error: 2.2381\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0821 - mean_squared_error: 2.3211\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0443 - mean_squared_error: 2.0995\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0261 - mean_squared_error: 2.1586\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0467 - mean_squared_error: 2.2216\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.2305 - mean_squared_error: 2.7607\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3077 - mean_squared_error: 3.1827\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1418 - mean_squared_error: 2.4099\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1100 - mean_squared_error: 2.2227\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1133 - mean_squared_error: 2.3535\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0711 - mean_squared_error: 2.3766\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0887 - mean_squared_error: 2.2934\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0916 - mean_squared_error: 2.2741\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0580 - mean_squared_error: 2.3212\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0753 - mean_squared_error: 2.1302\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0581 - mean_squared_error: 2.1828\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0690 - mean_squared_error: 2.2203\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0574 - mean_squared_error: 2.3146\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0644 - mean_squared_error: 2.2341\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0134 - mean_squared_error: 1.8793\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0751 - mean_squared_error: 2.3635\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0094 - mean_squared_error: 2.1669\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0384 - mean_squared_error: 2.0553\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0928 - mean_squared_error: 2.2555\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0550 - mean_squared_error: 2.2884\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0273 - mean_squared_error: 1.9794\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0715 - mean_squared_error: 2.2429\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0101 - mean_squared_error: 2.0437\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0619 - mean_squared_error: 2.3140\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0114 - mean_squared_error: 2.0344\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0176 - mean_squared_error: 2.0793\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0614 - mean_squared_error: 2.2450\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0372 - mean_squared_error: 2.1931\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0040 - mean_squared_error: 2.0314\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0136 - mean_squared_error: 2.0627\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0356 - mean_squared_error: 2.2448\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0006 - mean_squared_error: 2.1113\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0281 - mean_squared_error: 2.1359\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9913 - mean_squared_error: 1.8346\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0144 - mean_squared_error: 2.3047\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0208 - mean_squared_error: 2.0858\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0065 - mean_squared_error: 2.0880\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0397 - mean_squared_error: 2.2289\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0270 - mean_squared_error: 2.1152\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0363 - mean_squared_error: 2.0398\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0198 - mean_squared_error: 2.1901\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0184 - mean_squared_error: 2.1087\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0330 - mean_squared_error: 2.1253\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0171 - mean_squared_error: 2.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:36:22.834132: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_74953' and '__inference_standard_lstm_74842_specialized_for_model_5_lstm_5_StatefulPartitionedCall_at___inference_distributed_function_75193' both implement 'lstm_ff0d7fec-6e75-4388-8522-9ae330742285' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2277 - mean_squared_error: 1.8623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0325432677640982, 1.8623041]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0325432677640982, 1.8623041]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:36:25.840425: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_77924_78106' and '__inference___backward_standard_lstm_78230_78715_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_78888' both implement 'lstm_ba9c2e2e-9ea1-4c22-b413-715ea5de7765' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.1376 - mean_squared_error: 2.4261\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0924 - mean_squared_error: 2.3180\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0618 - mean_squared_error: 2.2146\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0686 - mean_squared_error: 2.2097\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0598 - mean_squared_error: 2.2061\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0874 - mean_squared_error: 2.3548\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0437 - mean_squared_error: 2.0159\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1080 - mean_squared_error: 2.4391\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0782 - mean_squared_error: 2.1814\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0262 - mean_squared_error: 2.1038\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0539 - mean_squared_error: 2.2151\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0800 - mean_squared_error: 2.2602\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0556 - mean_squared_error: 2.1630\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0592 - mean_squared_error: 2.2477\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0649 - mean_squared_error: 2.1294\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0644 - mean_squared_error: 2.2643\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0748 - mean_squared_error: 2.2804\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0871 - mean_squared_error: 2.2478\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0118 - mean_squared_error: 1.9557\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0960 - mean_squared_error: 2.4729\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0486 - mean_squared_error: 2.1094\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0418 - mean_squared_error: 2.1362\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0665 - mean_squared_error: 2.1480\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0543 - mean_squared_error: 2.1792\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0175 - mean_squared_error: 2.0308\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0395 - mean_squared_error: 2.1592\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0558 - mean_squared_error: 2.3977\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0189 - mean_squared_error: 2.1430\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0327 - mean_squared_error: 1.9416\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.2310 - mean_squared_error: 2.8728\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1057 - mean_squared_error: 2.3380\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0963 - mean_squared_error: 2.2488\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0605 - mean_squared_error: 2.2543\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0530 - mean_squared_error: 2.1377\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0507 - mean_squared_error: 2.2352\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0596 - mean_squared_error: 2.2077\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0541 - mean_squared_error: 2.3513\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0410 - mean_squared_error: 2.0951\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0304 - mean_squared_error: 2.1188\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0335 - mean_squared_error: 1.9613\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0658 - mean_squared_error: 2.3794\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0489 - mean_squared_error: 2.1334\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0722 - mean_squared_error: 2.3236\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0517 - mean_squared_error: 2.1664\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0574 - mean_squared_error: 2.2154\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0743 - mean_squared_error: 2.2453\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0805 - mean_squared_error: 2.2338\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0702 - mean_squared_error: 2.2478\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0752 - mean_squared_error: 2.2124\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0815 - mean_squared_error: 2.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:41:18.014921: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_86755' and '__inference_standard_lstm_86755_specialized_for_model_6_lstm_6_StatefulPartitionedCall_at___inference_distributed_function_87106' both implement 'lstm_de9d6bb2-b828-4d71-888b-9d0e11672778' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2638 - mean_squared_error: 1.9938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0826821665391855, 1.9937755]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0826821665391855, 1.9937755]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:41:20.691029: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_90143_90628' and '__inference___backward_standard_lstm_90143_90628_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_90801' both implement 'lstm_ba274c1b-e72f-4712-be2b-aaeeac866a22' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.1423 - mean_squared_error: 2.4555\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0809 - mean_squared_error: 2.2349\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0717 - mean_squared_error: 2.2282\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0350 - mean_squared_error: 2.1483\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0720 - mean_squared_error: 2.2203\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0310 - mean_squared_error: 2.1980\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0324 - mean_squared_error: 2.0911\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0286 - mean_squared_error: 2.0925\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0096 - mean_squared_error: 2.0972\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0709 - mean_squared_error: 2.2990\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0385 - mean_squared_error: 2.1861\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0247 - mean_squared_error: 2.0405\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0432 - mean_squared_error: 2.2284\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0811 - mean_squared_error: 2.2677\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9709 - mean_squared_error: 1.8868\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0920 - mean_squared_error: 2.3866\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0293 - mean_squared_error: 2.1542\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0740 - mean_squared_error: 2.1631\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0292 - mean_squared_error: 2.2462\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0615 - mean_squared_error: 2.0387\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1191 - mean_squared_error: 2.4306\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0682 - mean_squared_error: 2.1742\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0790 - mean_squared_error: 2.2031\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0807 - mean_squared_error: 2.4170\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0866 - mean_squared_error: 2.3085\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0602 - mean_squared_error: 2.2086\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0378 - mean_squared_error: 2.1539\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0491 - mean_squared_error: 2.0753\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0835 - mean_squared_error: 2.3410\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.3732\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0526 - mean_squared_error: 2.1051\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0326 - mean_squared_error: 2.0093\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1329 - mean_squared_error: 2.6043\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1041 - mean_squared_error: 2.1798\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0881 - mean_squared_error: 2.3242\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1549 - mean_squared_error: 2.4375\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1489 - mean_squared_error: 2.3595\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1228 - mean_squared_error: 2.4678\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0899 - mean_squared_error: 2.2704\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1594 - mean_squared_error: 2.5484\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1209 - mean_squared_error: 2.4606\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1046 - mean_squared_error: 2.3059\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1410 - mean_squared_error: 2.3152\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1002 - mean_squared_error: 2.4619\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1881 - mean_squared_error: 2.7140\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.2327\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0969 - mean_squared_error: 2.3716\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0800 - mean_squared_error: 2.2880\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0731 - mean_squared_error: 2.2676\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0587 - mean_squared_error: 2.2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:46:29.306962: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_98668' and '__inference_standard_lstm_98668_specialized_for_model_7_lstm_7_StatefulPartitionedCall_at___inference_distributed_function_99019' both implement 'lstm_92041608-55fe-440c-870e-93fc48b35040' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.3026 - mean_squared_error: 1.9762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0859702288681734, 1.9761673]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0859702288681734, 1.9761673]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:46:32.859927: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_101750_101932' and '__inference___backward_standard_lstm_102056_102541_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_102714' both implement 'lstm_a0a49fa6-8737-439e-8a04-ed4d5a498aac' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3953 - mean_squared_error: 3.4142\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.4243 - mean_squared_error: 3.5722\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.4190 - mean_squared_error: 3.5444\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1108 - mean_squared_error: 2.3394\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0493 - mean_squared_error: 2.2261\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0149 - mean_squared_error: 2.0116\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0212 - mean_squared_error: 2.1172\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0277 - mean_squared_error: 2.2432\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0068 - mean_squared_error: 1.9564\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9975 - mean_squared_error: 2.0394\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0404 - mean_squared_error: 2.2080\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9956 - mean_squared_error: 2.0748\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0070 - mean_squared_error: 2.1421\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9944 - mean_squared_error: 2.0038\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0670 - mean_squared_error: 2.3270\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9867 - mean_squared_error: 1.9878\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0129 - mean_squared_error: 2.0850\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0113 - mean_squared_error: 2.0965\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9720 - mean_squared_error: 1.9463\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0368 - mean_squared_error: 2.1416\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0117 - mean_squared_error: 2.2217\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0264 - mean_squared_error: 2.1194\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0061 - mean_squared_error: 2.0871\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9846 - mean_squared_error: 1.9849\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0311 - mean_squared_error: 2.2243\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9479 - mean_squared_error: 1.8154\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0206 - mean_squared_error: 2.2372\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9917 - mean_squared_error: 2.0179\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9567 - mean_squared_error: 1.8848\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0325 - mean_squared_error: 2.1396\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9926 - mean_squared_error: 2.0730\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9775 - mean_squared_error: 2.1245\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9675 - mean_squared_error: 1.9887\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0089 - mean_squared_error: 2.1186\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9667 - mean_squared_error: 1.9342\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9998 - mean_squared_error: 2.2536\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9687 - mean_squared_error: 1.9113\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0057 - mean_squared_error: 2.1105\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9685 - mean_squared_error: 1.9118\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9774 - mean_squared_error: 2.1191\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9851 - mean_squared_error: 2.0677\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9692 - mean_squared_error: 2.0521\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9654 - mean_squared_error: 1.9805\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9796 - mean_squared_error: 1.9702\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9785 - mean_squared_error: 2.1499\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9717 - mean_squared_error: 2.0270\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9692 - mean_squared_error: 2.0083\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9652 - mean_squared_error: 2.0294\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 0.9541 - mean_squared_error: 2.0278\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9665 - mean_squared_error: 2.0122\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9543 - mean_squared_error: 2.0127\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9709 - mean_squared_error: 2.0572\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9530 - mean_squared_error: 1.9868\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9952 - mean_squared_error: 2.1008\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9611 - mean_squared_error: 1.9525\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9776 - mean_squared_error: 2.0034\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9845 - mean_squared_error: 2.1145\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9800 - mean_squared_error: 1.9914\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9549 - mean_squared_error: 2.0958\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9741 - mean_squared_error: 2.0765\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9707 - mean_squared_error: 2.0087\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9374 - mean_squared_error: 1.8136\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9638 - mean_squared_error: 2.0200\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9796 - mean_squared_error: 2.1098\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9498 - mean_squared_error: 1.9540\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9847 - mean_squared_error: 2.1848\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9559 - mean_squared_error: 1.9552\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9448 - mean_squared_error: 1.8844\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9506 - mean_squared_error: 2.1077\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9336 - mean_squared_error: 1.8018\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9822 - mean_squared_error: 2.2750\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9462 - mean_squared_error: 2.0362\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9252 - mean_squared_error: 1.8957\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9592 - mean_squared_error: 2.0125\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9673 - mean_squared_error: 2.1065\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9386 - mean_squared_error: 1.9045\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9306 - mean_squared_error: 1.8559\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9591 - mean_squared_error: 2.0697\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9710 - mean_squared_error: 2.0962\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9138 - mean_squared_error: 1.7766\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9837 - mean_squared_error: 2.2769\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9514 - mean_squared_error: 2.0018\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9349 - mean_squared_error: 1.9110\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9488 - mean_squared_error: 2.0379\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9417 - mean_squared_error: 2.0723\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9470 - mean_squared_error: 2.0191\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 0.9045 - mean_squared_error: 1.7892\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9964 - mean_squared_error: 2.1983\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.9210 - mean_squared_error: 1.8977\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9407 - mean_squared_error: 2.0182\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9533 - mean_squared_error: 1.9828\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9621 - mean_squared_error: 2.1063\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9318 - mean_squared_error: 1.9797\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9567 - mean_squared_error: 2.0152\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9619 - mean_squared_error: 2.0284\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9612 - mean_squared_error: 1.9995\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9579 - mean_squared_error: 2.0157\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 9s 7ms/sample - loss: 0.9529 - mean_squared_error: 2.0106\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9561 - mean_squared_error: 2.0355\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9422 - mean_squared_error: 2.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:57:24.872491: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_118292' and '__inference_standard_lstm_118181_specialized_for_model_8_lstm_8_StatefulPartitionedCall_at___inference_distributed_function_118532' both implement 'lstm_f4e1fdf5-c031-497f-b0f8-f63237e37e53' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 1.2306 - mean_squared_error: 1.8863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0403979792662545, 1.8863058]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0403979792662545, 1.8863058]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:57:31.759752: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_121569_122054_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_122227' and '__inference___backward_cudnn_lstm_with_fallback_121263_121445' both implement 'lstm_8039e148-f607-41a1-8369-115303c585e5' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 19s 16ms/sample - loss: 1.1464 - mean_squared_error: 2.5365\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0664 - mean_squared_error: 2.1787\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0549 - mean_squared_error: 2.1754\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1064 - mean_squared_error: 2.3500\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1187 - mean_squared_error: 2.3425\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0790 - mean_squared_error: 2.2836\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0522 - mean_squared_error: 2.1979\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0671 - mean_squared_error: 2.2623\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0750 - mean_squared_error: 2.2918\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0216 - mean_squared_error: 2.0472\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0743 - mean_squared_error: 2.2837\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0131 - mean_squared_error: 2.0715\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0540 - mean_squared_error: 2.2115\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0380 - mean_squared_error: 2.1143\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0555 - mean_squared_error: 2.2905\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9941 - mean_squared_error: 2.0135\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0061 - mean_squared_error: 2.0266\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0525 - mean_squared_error: 2.2949\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0361 - mean_squared_error: 2.1330\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0349 - mean_squared_error: 2.1784\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0182 - mean_squared_error: 1.9514\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9947 - mean_squared_error: 2.1322\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0145 - mean_squared_error: 2.0841\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0376 - mean_squared_error: 2.1806\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9998 - mean_squared_error: 1.9883\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0042 - mean_squared_error: 2.1648\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0309 - mean_squared_error: 2.0355\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0507 - mean_squared_error: 2.2796\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9914 - mean_squared_error: 1.9324\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 1.0506 - mean_squared_error: 2.3442\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 0.9837 - mean_squared_error: 2.0082\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0032 - mean_squared_error: 1.9888\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9912 - mean_squared_error: 2.1009\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.9503 - mean_squared_error: 1.8050\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0542 - mean_squared_error: 2.2852\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9798 - mean_squared_error: 1.9446\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0079 - mean_squared_error: 2.1933\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0031 - mean_squared_error: 2.1160\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0141 - mean_squared_error: 2.0289\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0406 - mean_squared_error: 2.1900\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0292 - mean_squared_error: 2.0653\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0377 - mean_squared_error: 2.1985\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0523 - mean_squared_error: 2.0611\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0370 - mean_squared_error: 2.2442\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0312 - mean_squared_error: 2.1373\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0186 - mean_squared_error: 2.1018\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0103 - mean_squared_error: 2.0179\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0178 - mean_squared_error: 2.1588\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0006 - mean_squared_error: 2.1097\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0061 - mean_squared_error: 2.0932\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0025 - mean_squared_error: 2.0871\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9876 - mean_squared_error: 2.0564\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0748 - mean_squared_error: 2.3112\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1058 - mean_squared_error: 2.3601\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0982 - mean_squared_error: 2.3755\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0621 - mean_squared_error: 2.1497\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0511 - mean_squared_error: 2.2451\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0128 - mean_squared_error: 2.0649\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0526 - mean_squared_error: 2.1708\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0342 - mean_squared_error: 2.1808\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0014 - mean_squared_error: 1.9928\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0626 - mean_squared_error: 2.3413\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0286 - mean_squared_error: 2.1403\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0174 - mean_squared_error: 2.0319\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0143 - mean_squared_error: 2.1007\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0192 - mean_squared_error: 2.0813\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1402 - mean_squared_error: 2.6003\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0483 - mean_squared_error: 1.9950\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0284 - mean_squared_error: 2.2115\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0202 - mean_squared_error: 2.0745\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0354 - mean_squared_error: 2.0439\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0342 - mean_squared_error: 2.2447\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0455 - mean_squared_error: 2.2128\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0030 - mean_squared_error: 2.0434\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0648 - mean_squared_error: 2.2511\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0256 - mean_squared_error: 2.1910\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0029 - mean_squared_error: 2.0382\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0018 - mean_squared_error: 2.0075\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0353 - mean_squared_error: 2.1167\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9907 - mean_squared_error: 1.9587\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0475 - mean_squared_error: 2.2490\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0377 - mean_squared_error: 2.1422\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0222 - mean_squared_error: 1.9813\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0264 - mean_squared_error: 2.2730\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0168 - mean_squared_error: 2.1690\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9957 - mean_squared_error: 1.9696\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0392 - mean_squared_error: 2.2627\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0317 - mean_squared_error: 2.0369\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0101 - mean_squared_error: 2.1016\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0100 - mean_squared_error: 2.0671\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0449 - mean_squared_error: 2.2650\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0293 - mean_squared_error: 2.1416\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0336 - mean_squared_error: 2.0853\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0070 - mean_squared_error: 2.1429\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0201 - mean_squared_error: 2.0851\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0072 - mean_squared_error: 2.1084\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0058 - mean_squared_error: 2.0867\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0007 - mean_squared_error: 2.0820\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0026 - mean_squared_error: 2.0672\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9993 - mean_squared_error: 2.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:10:22.977341: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_137694_specialized_for_model_9_lstm_9_StatefulPartitionedCall_at___inference_distributed_function_138045' and '__inference_cudnn_lstm_with_fallback_137805' both implement 'lstm_ba389048-02bc-409a-9953-c7a739579dbe' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 1.2334 - mean_squared_error: 1.8434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0343166961737558, 1.8434093]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0343166961737558, 1.8434093]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:10:26.129923: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_140776_140958' and '__inference___backward_standard_lstm_141082_141567_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_141740' both implement 'lstm_b2ac370b-fc9a-4987-8ca8-1e6419eca5a8' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.1757 - mean_squared_error: 2.5349\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0659 - mean_squared_error: 2.2342\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0538 - mean_squared_error: 2.1692\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0631 - mean_squared_error: 2.2460\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0207 - mean_squared_error: 2.0962\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0096 - mean_squared_error: 2.0112\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0558 - mean_squared_error: 2.2017\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0487 - mean_squared_error: 2.1878\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1277 - mean_squared_error: 2.3805\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0587 - mean_squared_error: 2.1715\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0497 - mean_squared_error: 2.1714\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0691 - mean_squared_error: 2.2447\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0795 - mean_squared_error: 2.3965\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0182 - mean_squared_error: 1.9524\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0405 - mean_squared_error: 2.2420\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0364 - mean_squared_error: 2.0768\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1116 - mean_squared_error: 2.3774\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0992 - mean_squared_error: 2.4099\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0459 - mean_squared_error: 2.1689\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0542 - mean_squared_error: 2.0668\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0856 - mean_squared_error: 2.2352\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1749 - mean_squared_error: 2.5857\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0850 - mean_squared_error: 2.3094\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0855 - mean_squared_error: 2.2682\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0713 - mean_squared_error: 2.1625\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1245 - mean_squared_error: 2.3069\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1336 - mean_squared_error: 2.5850\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0824 - mean_squared_error: 2.2084\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0818 - mean_squared_error: 2.1947\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0888 - mean_squared_error: 2.3252\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0631 - mean_squared_error: 2.1879\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0763 - mean_squared_error: 2.3123\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1868 - mean_squared_error: 2.4972\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.2773 - mean_squared_error: 2.5984\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1089 - mean_squared_error: 2.4132\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1304 - mean_squared_error: 2.3723\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1249 - mean_squared_error: 2.3719\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0845 - mean_squared_error: 2.2760\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.1149 - mean_squared_error: 2.3618\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0977 - mean_squared_error: 2.3151\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0735 - mean_squared_error: 2.1875\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0625 - mean_squared_error: 2.2281\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0572 - mean_squared_error: 2.1376\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0748 - mean_squared_error: 2.3392\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0681 - mean_squared_error: 2.2404\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1412 - mean_squared_error: 2.4842\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0939 - mean_squared_error: 2.2737\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0597 - mean_squared_error: 2.1656\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0520 - mean_squared_error: 2.1824\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0533 - mean_squared_error: 2.2313\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0566 - mean_squared_error: 2.1674\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0543 - mean_squared_error: 2.1782\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0464 - mean_squared_error: 2.1135\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0524 - mean_squared_error: 2.2017\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0565 - mean_squared_error: 2.2231\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0418 - mean_squared_error: 2.1779\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0671 - mean_squared_error: 2.2103\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0809 - mean_squared_error: 2.2156\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0688 - mean_squared_error: 2.2327\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0646 - mean_squared_error: 2.1587\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0678 - mean_squared_error: 2.2121\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0649 - mean_squared_error: 2.2871\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0513 - mean_squared_error: 2.0349\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1657 - mean_squared_error: 2.5927\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0895 - mean_squared_error: 2.2925\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1138 - mean_squared_error: 2.3419\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0995 - mean_squared_error: 2.3331\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0562 - mean_squared_error: 2.1449\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0884 - mean_squared_error: 2.2764\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0634 - mean_squared_error: 2.2783\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0535 - mean_squared_error: 2.0757\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0815 - mean_squared_error: 2.3692\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1187 - mean_squared_error: 2.4683\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0264 - mean_squared_error: 1.9476\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0820 - mean_squared_error: 2.2717\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0628 - mean_squared_error: 2.1296\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0852 - mean_squared_error: 2.3410\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0809 - mean_squared_error: 2.2738\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0486 - mean_squared_error: 2.1757\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1103 - mean_squared_error: 2.3176\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0825 - mean_squared_error: 2.1885\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0701 - mean_squared_error: 2.2148\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0866 - mean_squared_error: 2.2779\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0903 - mean_squared_error: 2.2816\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0765 - mean_squared_error: 2.3663\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.2651 - mean_squared_error: 2.6886\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1891 - mean_squared_error: 2.6297\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0533 - mean_squared_error: 2.1715\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0793 - mean_squared_error: 2.1982\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0530 - mean_squared_error: 2.2333\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0642 - mean_squared_error: 2.2027\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0450 - mean_squared_error: 2.1570\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.1618\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0712 - mean_squared_error: 2.2257\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0702 - mean_squared_error: 2.2152\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1098 - mean_squared_error: 2.4231\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.3006 - mean_squared_error: 3.1333\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.2928 - mean_squared_error: 2.9270\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.3637 - mean_squared_error: 3.0643\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.3537 - mean_squared_error: 3.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:20:17.138647: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_157207' and '__inference_standard_lstm_157207_specialized_for_model_10_lstm_10_StatefulPartitionedCall_at___inference_distributed_function_157558' both implement 'lstm_5909eb33-7209-4ef2-a426-b041f0c23ee9' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.3647 - mean_squared_error: 2.6459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3403674930545455, 2.6459386]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.3403674930545455, 2.6459386]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:20:22.224905: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_160595_161080' and '__inference___backward_standard_lstm_160595_161080_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_161253' both implement 'lstm_272e8383-0155-4fa0-9c07-6dc714f5d585' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1269 - mean_squared_error: 2.3878\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0933 - mean_squared_error: 2.2779\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0794 - mean_squared_error: 2.2517\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0599 - mean_squared_error: 2.2046\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0495 - mean_squared_error: 2.1546\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0485 - mean_squared_error: 2.1820\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0508 - mean_squared_error: 2.1778\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0324 - mean_squared_error: 2.1668\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0375 - mean_squared_error: 2.1081\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0222 - mean_squared_error: 2.1454\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0531 - mean_squared_error: 2.2229\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0195 - mean_squared_error: 2.0971\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9662 - mean_squared_error: 1.8381\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0628 - mean_squared_error: 2.3868\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9861 - mean_squared_error: 1.9707\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0051 - mean_squared_error: 2.0772\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9979 - mean_squared_error: 2.1259\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0199 - mean_squared_error: 2.0438\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0462 - mean_squared_error: 2.3392\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0353 - mean_squared_error: 2.1282\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 71s 57ms/sample - loss: 1.0419 - mean_squared_error: 2.0659\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 30s 24ms/sample - loss: 1.0455 - mean_squared_error: 2.2522\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0530 - mean_squared_error: 2.2388\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0259 - mean_squared_error: 1.9229\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0612 - mean_squared_error: 2.1623\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1482 - mean_squared_error: 2.6546\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1645 - mean_squared_error: 2.5342\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1719 - mean_squared_error: 2.4014\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1198 - mean_squared_error: 2.3112\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1044 - mean_squared_error: 2.4268\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1132 - mean_squared_error: 2.3729\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1293 - mean_squared_error: 2.4087\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0762 - mean_squared_error: 2.1248\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0590 - mean_squared_error: 2.3027\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0062 - mean_squared_error: 1.9785\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1019 - mean_squared_error: 2.2999\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0476 - mean_squared_error: 2.1515\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1266 - mean_squared_error: 2.4180\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0992 - mean_squared_error: 2.3051\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0637 - mean_squared_error: 2.1883\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1242 - mean_squared_error: 2.4275\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0865 - mean_squared_error: 2.2225\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0894 - mean_squared_error: 2.2624\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0813 - mean_squared_error: 2.2628\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0473 - mean_squared_error: 2.1392\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0501 - mean_squared_error: 2.1870\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0594 - mean_squared_error: 2.1832\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0509 - mean_squared_error: 2.1476\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 7s 5ms/sample - loss: 1.0576 - mean_squared_error: 2.2117\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0654 - mean_squared_error: 2.2065\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0714 - mean_squared_error: 2.2293\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0591 - mean_squared_error: 2.2572\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1129 - mean_squared_error: 2.2878\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0679 - mean_squared_error: 2.1887\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0825 - mean_squared_error: 2.2912\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0473 - mean_squared_error: 2.1242\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0522 - mean_squared_error: 2.1662\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0684 - mean_squared_error: 2.3440\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0882 - mean_squared_error: 2.3074\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0534 - mean_squared_error: 2.1052\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0412 - mean_squared_error: 2.1014\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0894 - mean_squared_error: 2.4105\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0524 - mean_squared_error: 2.1827\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0759 - mean_squared_error: 2.2213\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0625 - mean_squared_error: 2.1153\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0488 - mean_squared_error: 2.2656\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0364 - mean_squared_error: 2.0274\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1981 - mean_squared_error: 2.6320\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1484 - mean_squared_error: 2.3493\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3062 - mean_squared_error: 3.1107\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3050 - mean_squared_error: 2.8130\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.4055 - mean_squared_error: 3.2624\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3265 - mean_squared_error: 2.8813\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1876 - mean_squared_error: 2.6424\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0652 - mean_squared_error: 2.1083\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0867 - mean_squared_error: 2.2490\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0873 - mean_squared_error: 2.2867\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0716 - mean_squared_error: 2.1055\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0990 - mean_squared_error: 2.4624\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0568 - mean_squared_error: 2.0658\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0986 - mean_squared_error: 2.3940\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0853 - mean_squared_error: 2.3373\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1128 - mean_squared_error: 2.3913\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0388 - mean_squared_error: 2.0394\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1033 - mean_squared_error: 2.3784\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1016 - mean_squared_error: 2.2333\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0684 - mean_squared_error: 2.2714\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1099 - mean_squared_error: 2.3303\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0824 - mean_squared_error: 2.2619\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0536 - mean_squared_error: 2.2070\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0408 - mean_squared_error: 2.1169\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0643 - mean_squared_error: 2.2156\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0937 - mean_squared_error: 2.2843\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0810 - mean_squared_error: 2.1785\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1877 - mean_squared_error: 2.6210\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1083 - mean_squared_error: 2.3255\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0906 - mean_squared_error: 2.2710\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 7s 6ms/sample - loss: 1.0723 - mean_squared_error: 2.2492\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0737 - mean_squared_error: 2.2275\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.2341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:33:07.857121: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_176720_specialized_for_model_11_lstm_11_StatefulPartitionedCall_at___inference_distributed_function_177071' and '__inference_standard_lstm_176720' both implement 'lstm_474204ea-a89d-47cf-a8c8-7563773e6996' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 1.2583 - mean_squared_error: 1.9554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0799023783798758, 1.9554123]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0799023783798758, 1.9554123]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:33:12.079012: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model/cnn_lstm_best/assets\n"
     ]
    }
   ],
   "source": [
    "loss_list = [\"MAE\"]\n",
    "optimizer_list = [\"Adam\"]\n",
    "epoch_list = [30,50,100]\n",
    "batch_list = [50]\n",
    "encoder_list = [50,100]\n",
    "lr_list = [0.005,0.01]\n",
    "train_df = pd.DataFrame(columns = [\"Epoch\",\"Batch\",\"Optimizer\",\"LR\",\"Encoder Unit\",\"Loss\",\"Metrics\",\"Validation\"])\n",
    "best_model = \"\"\n",
    "best_valid = 99999\n",
    "metrics = [keras.metrics.MeanSquaredError()]\n",
    "\n",
    "for los in loss_list:\n",
    "    for opti in optimizer_list:\n",
    "        for epochs in epoch_list:\n",
    "            for batchs in batch_list:\n",
    "                for lr in lr_list:\n",
    "                    for encoder_u in encoder_list:\n",
    "\n",
    "                        model = myModel(input_shape=(num_day_to_predict,train_X.shape[1],1),\n",
    "                                        encoder_unit = encoder_u,\n",
    "                                        repeat_vector_n = 50\n",
    "                                       )\n",
    "\n",
    "                        if opti == \"Adam\":\n",
    "                            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "                        if los == \"MAE\":\n",
    "                            loss = keras.losses.MeanAbsoluteError()\n",
    "                        elif los == \"MSE\":\n",
    "                            loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "                        model.compile(\n",
    "                            optimizer=optimizer,\n",
    "                            loss=loss,\n",
    "                            metrics=metrics,\n",
    "                        )\n",
    "\n",
    "                        history = model.fit(\n",
    "                                tf_train_X,\n",
    "                                tf_train_y,\n",
    "                                epochs = epochs,\n",
    "                                steps_per_epoch = batchs,\n",
    "                            )\n",
    "\n",
    "                        results = model.evaluate(tf_valid_X, tf_valid_y, batch_size=batchs)\n",
    "                        print(results)\n",
    "                        print(\"===== Summary =====\")\n",
    "                        print(\"Epoch: \",epochs)\n",
    "                        print(\"Batch Size: \",batchs)\n",
    "                        print(\"Optimizer: \",opti)\n",
    "                        print(\"Learning Rate: \",lr)\n",
    "                        print(\"Encoder Units: \",encoder_u)\n",
    "                        print(\"Loss Function: \", los)\n",
    "                        print(\"Metrics: \", metrics)\n",
    "                        print(\"Validation: \",results)\n",
    "                        if results[0] < best_valid:\n",
    "                            best_valid = results[0]\n",
    "                            best_model = model\n",
    "                        train_df = train_df.append({\"Epoch\": epochs,\n",
    "                                                    \"Batch\": batchs,\n",
    "                                                    \"Optimizer\": opti,\n",
    "                                                    \"LR\": lr,\n",
    "                                                    \"Encoder Unit\": encoder_u,\n",
    "                                                    \"Loss\": los,\n",
    "                                                    \"Metrics\": metrics,\n",
    "                                                    \"Validation\":results}, ignore_index=True)\n",
    "best_model.save(\"model/cnn_lstm_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1049860",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by=[\"Validation\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6201ca",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c93eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (495, 1)\n",
      "[[9.99935865e-01]\n",
      " [9.99999225e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999762e-01]\n",
      " [6.37081385e-01]\n",
      " [1.27184570e-01]\n",
      " [3.74007225e-03]\n",
      " [9.98438239e-01]\n",
      " [9.99805331e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998987e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99991655e-01]\n",
      " [9.06998873e-01]\n",
      " [9.24459994e-02]\n",
      " [4.23881710e-02]\n",
      " [3.89069319e-04]\n",
      " [8.59656632e-02]\n",
      " [7.81863928e-04]\n",
      " [2.86102295e-06]\n",
      " [2.23398209e-02]\n",
      " [9.99546766e-01]\n",
      " [9.99998808e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99998331e-01]\n",
      " [9.99999523e-01]\n",
      " [9.13837552e-01]\n",
      " [9.16434944e-01]\n",
      " [9.99710798e-01]\n",
      " [9.24031019e-01]\n",
      " [9.43064690e-04]\n",
      " [1.49011612e-07]\n",
      " [1.19209290e-07]\n",
      " [9.98399138e-01]\n",
      " [9.99970555e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999106e-01]\n",
      " [9.99998510e-01]\n",
      " [9.99938130e-01]\n",
      " [9.99997556e-01]\n",
      " [9.99997616e-01]\n",
      " [9.99999642e-01]\n",
      " [9.98965144e-01]\n",
      " [6.97687507e-01]\n",
      " [9.50902700e-04]\n",
      " [9.53674316e-07]\n",
      " [5.66244125e-07]\n",
      " [1.90448582e-01]\n",
      " [1.18739009e-02]\n",
      " [9.23145413e-01]\n",
      " [9.99983013e-01]\n",
      " [9.99992788e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999225e-01]\n",
      " [9.91631269e-01]\n",
      " [5.14423966e-01]\n",
      " [2.77360678e-02]\n",
      " [4.40082550e-01]\n",
      " [9.99997258e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998808e-01]\n",
      " [8.12213242e-01]\n",
      " [5.83559275e-04]\n",
      " [7.98702240e-06]\n",
      " [3.57627869e-07]\n",
      " [9.99999523e-01]\n",
      " [9.99993324e-01]\n",
      " [9.99998748e-01]\n",
      " [6.45449758e-03]\n",
      " [2.08616257e-07]\n",
      " [2.98023224e-08]\n",
      " [3.27825546e-07]\n",
      " [3.57627869e-07]\n",
      " [1.46031380e-06]\n",
      " [9.23871994e-07]\n",
      " [6.55651093e-07]\n",
      " [1.31130219e-06]\n",
      " [2.81035900e-05]\n",
      " [2.98023224e-08]\n",
      " [3.27825546e-07]\n",
      " [1.49011612e-07]\n",
      " [2.98023224e-07]\n",
      " [6.25848770e-07]\n",
      " [7.15255737e-07]\n",
      " [0.00000000e+00]\n",
      " [5.17010689e-04]\n",
      " [1.72853470e-06]\n",
      " [1.19209290e-07]\n",
      " [9.99975681e-01]\n",
      " [9.99999702e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99942124e-01]\n",
      " [9.99989033e-01]\n",
      " [9.95539784e-01]\n",
      " [9.99063969e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99608994e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99970734e-01]\n",
      " [9.83703613e-01]\n",
      " [9.99796629e-01]\n",
      " [9.99487102e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99979854e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99993145e-01]\n",
      " [2.48402357e-04]\n",
      " [4.98682261e-04]\n",
      " [7.30454922e-04]\n",
      " [2.68518925e-05]\n",
      " [6.20321631e-01]\n",
      " [9.99715328e-01]\n",
      " [9.99946952e-01]\n",
      " [1.06104612e-02]\n",
      " [9.99997973e-01]\n",
      " [6.81582987e-02]\n",
      " [9.97015059e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999404e-01]\n",
      " [9.75770473e-01]\n",
      " [7.38902867e-01]\n",
      " [9.98165905e-01]\n",
      " [5.53074479e-02]\n",
      " [9.99999642e-01]\n",
      " [9.99999285e-01]\n",
      " [2.37095356e-03]\n",
      " [2.98023224e-08]\n",
      " [1.49011612e-07]\n",
      " [1.49011612e-07]\n",
      " [1.67340040e-04]\n",
      " [1.21721625e-03]\n",
      " [1.78813934e-07]\n",
      " [9.99997973e-01]\n",
      " [6.87472165e-01]\n",
      " [2.57310510e-01]\n",
      " [9.99998927e-01]\n",
      " [9.91830587e-01]\n",
      " [9.99952376e-01]\n",
      " [9.99989986e-01]\n",
      " [9.99883771e-01]\n",
      " [5.66959381e-04]\n",
      " [5.51177800e-01]\n",
      " [4.34845686e-04]\n",
      " [1.69873238e-06]\n",
      " [4.42332834e-01]\n",
      " [3.55243683e-05]\n",
      " [3.74317169e-05]\n",
      " [5.43294728e-01]\n",
      " [9.99906898e-01]\n",
      " [9.99999225e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999404e-01]\n",
      " [9.98997688e-01]\n",
      " [1.77279711e-01]\n",
      " [1.24043226e-03]\n",
      " [6.50286674e-04]\n",
      " [3.17756414e-01]\n",
      " [5.81145287e-06]\n",
      " [1.55031681e-04]\n",
      " [2.08616257e-07]\n",
      " [3.69411707e-03]\n",
      " [2.98771650e-01]\n",
      " [4.09215689e-04]\n",
      " [9.99999046e-01]\n",
      " [9.99999642e-01]\n",
      " [5.41959107e-02]\n",
      " [8.43653083e-03]\n",
      " [9.99999583e-01]\n",
      " [8.32157493e-01]\n",
      " [9.99999523e-01]\n",
      " [4.68289703e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99158382e-01]\n",
      " [9.99978721e-01]\n",
      " [9.99445081e-01]\n",
      " [9.99998569e-01]\n",
      " [9.99990582e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99999523e-01]\n",
      " [9.40379202e-02]\n",
      " [3.96071970e-02]\n",
      " [9.98443127e-01]\n",
      " [9.99976397e-01]\n",
      " [9.99999225e-01]\n",
      " [9.99999523e-01]\n",
      " [7.20288277e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99971628e-01]\n",
      " [9.99997675e-01]\n",
      " [9.99796450e-01]\n",
      " [9.99995232e-01]\n",
      " [9.99960005e-01]\n",
      " [9.99993265e-01]\n",
      " [9.99999404e-01]\n",
      " [9.92240906e-01]\n",
      " [4.33146954e-03]\n",
      " [8.85128975e-06]\n",
      " [1.49011612e-07]\n",
      " [9.55969095e-04]\n",
      " [8.31189156e-01]\n",
      " [9.99866486e-01]\n",
      " [9.99998152e-01]\n",
      " [3.37362289e-04]\n",
      " [2.38418579e-07]\n",
      " [1.19209290e-07]\n",
      " [4.94718552e-06]\n",
      " [9.98902261e-01]\n",
      " [9.99970973e-01]\n",
      " [6.03388190e-01]\n",
      " [9.99989092e-01]\n",
      " [9.98977900e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99999464e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99785304e-01]\n",
      " [9.99968052e-01]\n",
      " [9.99944389e-01]\n",
      " [9.99317408e-01]\n",
      " [9.99997735e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99997497e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99996483e-01]\n",
      " [9.68820572e-01]\n",
      " [5.88398814e-01]\n",
      " [8.73096466e-01]\n",
      " [9.99987900e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99988616e-01]\n",
      " [3.55830848e-01]\n",
      " [8.74320805e-01]\n",
      " [5.73018253e-01]\n",
      " [7.54948854e-01]\n",
      " [4.35537696e-01]\n",
      " [9.97424245e-01]\n",
      " [9.99998629e-01]\n",
      " [5.96046448e-08]\n",
      " [6.02461934e-01]\n",
      " [9.86331642e-01]\n",
      " [9.98135567e-01]\n",
      " [5.14745712e-04]\n",
      " [2.02655792e-06]\n",
      " [1.69873238e-06]\n",
      " [5.42402267e-06]\n",
      " [9.99920607e-01]\n",
      " [9.79900360e-05]\n",
      " [8.10027122e-05]\n",
      " [2.57581472e-04]\n",
      " [8.19458067e-01]\n",
      " [9.99995708e-01]\n",
      " [2.47942537e-01]\n",
      " [3.21865082e-06]\n",
      " [2.97944129e-01]\n",
      " [2.05636024e-06]\n",
      " [5.03063202e-04]\n",
      " [1.19209290e-07]\n",
      " [8.94069672e-08]\n",
      " [8.94069672e-08]\n",
      " [0.00000000e+00]\n",
      " [8.94069672e-08]\n",
      " [4.56243753e-04]\n",
      " [1.15470290e-02]\n",
      " [1.74678564e-01]\n",
      " [5.44205308e-03]\n",
      " [4.21404839e-05]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [5.00679016e-06]\n",
      " [1.46031380e-06]\n",
      " [1.15092099e-01]\n",
      " [2.54631042e-03]\n",
      " [3.84449959e-06]\n",
      " [6.16610050e-04]\n",
      " [9.23871994e-07]\n",
      " [1.19209290e-07]\n",
      " [8.94069672e-08]\n",
      " [4.52092916e-01]\n",
      " [4.78000343e-02]\n",
      " [8.84959698e-01]\n",
      " [1.48397893e-01]\n",
      " [9.97308612e-01]\n",
      " [1.81452900e-01]\n",
      " [1.57952309e-06]\n",
      " [2.47776508e-04]\n",
      " [7.16149807e-05]\n",
      " [4.05087233e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99477506e-01]\n",
      " [9.99986410e-01]\n",
      " [9.99997854e-01]\n",
      " [9.99992847e-01]\n",
      " [9.99901533e-01]\n",
      " [9.98796105e-01]\n",
      " [6.41970336e-02]\n",
      " [2.86470056e-02]\n",
      " [1.48394644e-01]\n",
      " [5.71133792e-02]\n",
      " [3.03461134e-01]\n",
      " [3.97369623e-01]\n",
      " [6.85888529e-03]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99998689e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99997854e-01]\n",
      " [9.99996662e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99998927e-01]\n",
      " [9.99998450e-01]\n",
      " [5.36937773e-01]\n",
      " [2.90410250e-01]\n",
      " [2.71113336e-01]\n",
      " [8.16369534e-01]\n",
      " [6.70991898e-01]\n",
      " [9.99994040e-01]\n",
      " [9.95961189e-01]\n",
      " [9.96407986e-01]\n",
      " [9.73135591e-01]\n",
      " [7.25436330e-01]\n",
      " [8.56520891e-01]\n",
      " [9.99418020e-01]\n",
      " [9.99875665e-01]\n",
      " [9.99864697e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99856949e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99998331e-01]\n",
      " [9.97941494e-01]\n",
      " [9.99999225e-01]\n",
      " [7.64110684e-01]\n",
      " [9.99954700e-01]\n",
      " [9.98729825e-01]\n",
      " [4.91828799e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999046e-01]\n",
      " [4.53310341e-01]\n",
      " [5.37095666e-02]\n",
      " [7.70688057e-05]\n",
      " [4.22984362e-04]\n",
      " [9.00804520e-01]\n",
      " [3.12084675e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99996603e-01]\n",
      " [9.99997139e-01]\n",
      " [9.99960542e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99988317e-01]\n",
      " [7.63323903e-03]\n",
      " [9.97866273e-01]\n",
      " [6.60747290e-04]\n",
      " [4.51971591e-01]\n",
      " [5.96046448e-08]\n",
      " [8.94069672e-08]\n",
      " [8.64267349e-07]\n",
      " [1.34408474e-05]\n",
      " [1.13811493e-02]\n",
      " [6.04096830e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99935448e-01]\n",
      " [9.99943197e-01]\n",
      " [6.72018826e-01]\n",
      " [7.35868216e-01]\n",
      " [9.99786615e-01]\n",
      " [9.99997914e-01]\n",
      " [1.64497256e-01]\n",
      " [6.85726166e-01]\n",
      " [9.99857426e-01]\n",
      " [9.99964833e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999106e-01]\n",
      " [9.99990284e-01]\n",
      " [7.20635235e-01]\n",
      " [9.99996424e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99999285e-01]\n",
      " [8.49366188e-06]\n",
      " [3.03983688e-06]\n",
      " [2.98023224e-08]\n",
      " [5.96046448e-08]\n",
      " [5.06639481e-07]\n",
      " [1.49011612e-06]\n",
      " [8.94069672e-08]\n",
      " [1.85549259e-04]\n",
      " [2.98023224e-07]\n",
      " [2.98023224e-08]\n",
      " [2.98023224e-08]\n",
      " [8.94069672e-08]\n",
      " [2.59768486e-01]\n",
      " [6.33099675e-03]\n",
      " [1.17421150e-05]\n",
      " [8.97111535e-01]\n",
      " [6.41469002e-01]\n",
      " [6.20875657e-01]\n",
      " [9.99999344e-01]\n",
      " [9.99999583e-01]\n",
      " [9.82311726e-01]\n",
      " [9.99944270e-01]\n",
      " [9.99974251e-01]\n",
      " [5.43374419e-01]\n",
      " [8.74484479e-01]\n",
      " [9.99405921e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99998152e-01]\n",
      " [9.99857903e-01]\n",
      " [3.89617682e-03]\n",
      " [3.30209732e-05]\n",
      " [1.22487545e-05]\n",
      " [1.64273381e-02]\n",
      " [4.17232513e-07]\n",
      " [4.17232513e-07]\n",
      " [8.94069672e-08]\n",
      " [1.10268593e-06]\n",
      " [8.94069672e-08]\n",
      " [8.53859305e-01]\n",
      " [1.13248825e-06]\n",
      " [0.00000000e+00]\n",
      " [4.47034836e-07]\n",
      " [1.22767687e-03]\n",
      " [5.24982452e-01]\n",
      " [9.99997973e-01]\n",
      " [8.50013614e-01]\n",
      " [8.93590271e-01]\n",
      " [9.99899387e-01]\n",
      " [9.97734547e-01]\n",
      " [9.97021198e-01]\n",
      " [9.89935696e-01]\n",
      " [9.99994040e-01]\n",
      " [5.83072007e-02]\n",
      " [1.07255578e-03]\n",
      " [2.69711018e-04]\n",
      " [8.94069672e-08]\n",
      " [2.03520060e-04]\n",
      " [1.52356625e-02]\n",
      " [1.53729320e-03]\n",
      " [9.99981880e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998391e-01]\n",
      " [9.99998569e-01]\n",
      " [9.99990940e-01]\n",
      " [9.99992609e-01]\n",
      " [9.99843717e-01]\n",
      " [7.83081293e-01]\n",
      " [1.02639198e-04]\n",
      " [1.49634778e-02]\n",
      " [8.94069672e-08]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99116063e-01]\n",
      " [9.98905182e-01]\n",
      " [9.99998212e-01]\n",
      " [9.99998569e-01]\n",
      " [9.59208310e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998808e-01]\n",
      " [8.92563879e-01]]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('model/cnn_lstm_best')\n",
    "\n",
    "predictions = loaded_model.predict(tf_test_X)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5404bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanAndStd(X,y,num_day):\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    for i in range(0,len(X)-num_day): \n",
    "        x_open = X.iloc[i:i+num_day,0]\n",
    "        mean_list.append(x_open.mean(axis=0))\n",
    "        std_list.append(x_open.std(axis=0))\n",
    "    mean_df = pd.DataFrame(mean_list, columns = [\"mean\"])\n",
    "    std_df = pd.DataFrame(std_list, columns = [\"std\"])\n",
    "    return (mean_df,std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa6ca8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean, test_std = getMeanAndStd(test_X, test_y, num_day_to_predict)\n",
    "\n",
    "\n",
    "final_test_y = test_y.iloc[num_day_to_predict: , :]\n",
    "\n",
    "\n",
    "final_pred = np.array(predictions*np.array(test_std) + np.array(test_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71b54cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.29035142],\n",
       "       [ 37.18608539],\n",
       "       [ 37.32548867],\n",
       "       [ 37.58556612],\n",
       "       [ 37.55125726],\n",
       "       [ 37.33432626],\n",
       "       [ 37.33292246],\n",
       "       [ 37.9528955 ],\n",
       "       [ 38.03473699],\n",
       "       [ 38.10515598],\n",
       "       [ 38.61917405],\n",
       "       [ 39.23200851],\n",
       "       [ 39.74283334],\n",
       "       [ 40.15409194],\n",
       "       [ 40.85501918],\n",
       "       [ 41.51633103],\n",
       "       [ 41.73286913],\n",
       "       [ 40.62369368],\n",
       "       [ 40.92198723],\n",
       "       [ 41.22053677],\n",
       "       [ 41.49469509],\n",
       "       [ 41.54095493],\n",
       "       [ 41.66230136],\n",
       "       [ 41.74389926],\n",
       "       [ 42.0487353 ],\n",
       "       [ 41.89885457],\n",
       "       [ 41.89469633],\n",
       "       [ 42.09972826],\n",
       "       [ 42.21872919],\n",
       "       [ 42.29749707],\n",
       "       [ 42.37915286],\n",
       "       [ 42.4713048 ],\n",
       "       [ 42.6644042 ],\n",
       "       [ 42.7437438 ],\n",
       "       [ 42.4562321 ],\n",
       "       [ 42.50600004],\n",
       "       [ 42.47550005],\n",
       "       [ 42.90279959],\n",
       "       [ 43.25817253],\n",
       "       [ 43.69082445],\n",
       "       [ 44.12962487],\n",
       "       [ 44.54114504],\n",
       "       [ 44.92490634],\n",
       "       [ 45.39497021],\n",
       "       [ 45.67340156],\n",
       "       [ 46.06929676],\n",
       "       [ 46.5436151 ],\n",
       "       [ 46.3807105 ],\n",
       "       [ 45.88725301],\n",
       "       [ 46.04560083],\n",
       "       [ 46.16950044],\n",
       "       [ 46.42025963],\n",
       "       [ 46.44097498],\n",
       "       [ 47.04005536],\n",
       "       [ 47.16056241],\n",
       "       [ 47.33514794],\n",
       "       [ 47.41636406],\n",
       "       [ 47.63290763],\n",
       "       [ 48.07477599],\n",
       "       [ 48.3219779 ],\n",
       "       [ 48.61793162],\n",
       "       [ 48.34957088],\n",
       "       [ 48.09856054],\n",
       "       [ 48.54736931],\n",
       "       [ 48.90648243],\n",
       "       [ 49.12221905],\n",
       "       [ 49.28334151],\n",
       "       [ 49.49269798],\n",
       "       [ 49.87665748],\n",
       "       [ 49.99649244],\n",
       "       [ 49.45956599],\n",
       "       [ 49.586006  ],\n",
       "       [ 49.69540023],\n",
       "       [ 50.64874585],\n",
       "       [ 50.85438539],\n",
       "       [ 51.1152246 ],\n",
       "       [ 50.43101323],\n",
       "       [ 50.46210014],\n",
       "       [ 50.32920002],\n",
       "       [ 50.17200029],\n",
       "       [ 50.00810037],\n",
       "       [ 49.6183024 ],\n",
       "       [ 49.22940191],\n",
       "       [ 48.67030145],\n",
       "       [ 48.20110277],\n",
       "       [ 47.63375286],\n",
       "       [ 47.14400006],\n",
       "       [ 46.65670052],\n",
       "       [ 46.2533002 ],\n",
       "       [ 45.76760034],\n",
       "       [ 45.34580049],\n",
       "       [ 45.13010062],\n",
       "       [ 44.8855    ],\n",
       "       [ 44.68235102],\n",
       "       [ 44.34700162],\n",
       "       [ 44.0698001 ],\n",
       "       [ 44.73732089],\n",
       "       [ 44.67200173],\n",
       "       [ 44.56603449],\n",
       "       [ 44.93896224],\n",
       "       [ 45.62072877],\n",
       "       [ 46.38580746],\n",
       "       [ 46.95098533],\n",
       "       [ 47.44927174],\n",
       "       [ 47.68049956],\n",
       "       [ 47.85307792],\n",
       "       [ 47.93517525],\n",
       "       [ 48.33311889],\n",
       "       [ 48.57940503],\n",
       "       [ 48.67703597],\n",
       "       [ 48.79309107],\n",
       "       [ 48.90315258],\n",
       "       [ 48.96783435],\n",
       "       [ 49.12162588],\n",
       "       [ 49.09932725],\n",
       "       [ 49.26470746],\n",
       "       [ 49.33718549],\n",
       "       [ 49.51109452],\n",
       "       [ 49.64730446],\n",
       "       [ 49.14613261],\n",
       "       [ 49.16275888],\n",
       "       [ 49.24685722],\n",
       "       [ 49.38241157],\n",
       "       [ 49.69711502],\n",
       "       [ 49.93110545],\n",
       "       [ 50.00439238],\n",
       "       [ 49.67213139],\n",
       "       [ 50.10422294],\n",
       "       [ 49.77798002],\n",
       "       [ 50.25480222],\n",
       "       [ 50.49250147],\n",
       "       [ 50.65512212],\n",
       "       [ 50.85309327],\n",
       "       [ 50.81636072],\n",
       "       [ 51.06483732],\n",
       "       [ 50.68148689],\n",
       "       [ 51.82345568],\n",
       "       [ 52.10315957],\n",
       "       [ 51.19416004],\n",
       "       [ 51.05310004],\n",
       "       [ 50.75540023],\n",
       "       [ 50.45530026],\n",
       "       [ 50.24240007],\n",
       "       [ 50.11158272],\n",
       "       [ 49.91110032],\n",
       "       [ 51.47313713],\n",
       "       [ 50.33094505],\n",
       "       [ 49.41176984],\n",
       "       [ 50.0026048 ],\n",
       "       [ 50.62645789],\n",
       "       [ 51.11227964],\n",
       "       [ 51.57048708],\n",
       "       [ 51.97838271],\n",
       "       [ 50.88110966],\n",
       "       [ 51.63616556],\n",
       "       [ 51.20260833],\n",
       "       [ 51.22570154],\n",
       "       [ 51.70670153],\n",
       "       [ 51.49412526],\n",
       "       [ 51.39142753],\n",
       "       [ 51.72002306],\n",
       "       [ 51.9863687 ],\n",
       "       [ 52.04884714],\n",
       "       [ 52.32732849],\n",
       "       [ 52.53933746],\n",
       "       [ 52.96321084],\n",
       "       [ 53.74973365],\n",
       "       [ 54.06351367],\n",
       "       [ 53.12441035],\n",
       "       [ 53.22143329],\n",
       "       [ 53.53243311],\n",
       "       [ 54.06123771],\n",
       "       [ 53.95810475],\n",
       "       [ 54.05911172],\n",
       "       [ 54.23550011],\n",
       "       [ 54.24893256],\n",
       "       [ 54.22835935],\n",
       "       [ 54.14253703],\n",
       "       [ 54.48762737],\n",
       "       [ 54.79027587],\n",
       "       [ 54.4208414 ],\n",
       "       [ 54.3114034 ],\n",
       "       [ 55.04714447],\n",
       "       [ 55.1831429 ],\n",
       "       [ 55.47391696],\n",
       "       [ 55.27346473],\n",
       "       [ 55.89193933],\n",
       "       [ 56.39172868],\n",
       "       [ 56.904743  ],\n",
       "       [ 57.39390782],\n",
       "       [ 57.63886114],\n",
       "       [ 57.77766747],\n",
       "       [ 57.94740112],\n",
       "       [ 58.21745076],\n",
       "       [ 58.61124414],\n",
       "       [ 58.95428722],\n",
       "       [ 59.33801651],\n",
       "       [ 59.60893415],\n",
       "       [ 60.09199057],\n",
       "       [ 59.35211133],\n",
       "       [ 59.55377361],\n",
       "       [ 60.91330723],\n",
       "       [ 61.10231272],\n",
       "       [ 61.81790966],\n",
       "       [ 62.35451427],\n",
       "       [ 62.37658219],\n",
       "       [ 63.23537391],\n",
       "       [ 63.58434876],\n",
       "       [ 63.8633535 ],\n",
       "       [ 64.25547411],\n",
       "       [ 64.43375436],\n",
       "       [ 64.65177642],\n",
       "       [ 64.72964024],\n",
       "       [ 65.05832285],\n",
       "       [ 65.43933266],\n",
       "       [ 64.76408607],\n",
       "       [ 64.88270675],\n",
       "       [ 64.9791001 ],\n",
       "       [ 65.08848915],\n",
       "       [ 65.65450444],\n",
       "       [ 65.77445683],\n",
       "       [ 65.85355432],\n",
       "       [ 65.48935629],\n",
       "       [ 65.30480017],\n",
       "       [ 65.13600009],\n",
       "       [ 65.09310348],\n",
       "       [ 65.94270196],\n",
       "       [ 66.23120343],\n",
       "       [ 66.04534989],\n",
       "       [ 66.47178607],\n",
       "       [ 66.53879874],\n",
       "       [ 66.75523253],\n",
       "       [ 67.28745258],\n",
       "       [ 67.86060028],\n",
       "       [ 68.31733728],\n",
       "       [ 68.66011204],\n",
       "       [ 69.10589305],\n",
       "       [ 69.37789085],\n",
       "       [ 69.79984057],\n",
       "       [ 70.10713457],\n",
       "       [ 70.62845615],\n",
       "       [ 70.92025322],\n",
       "       [ 71.23464084],\n",
       "       [ 71.90555029],\n",
       "       [ 72.47636424],\n",
       "       [ 72.67712524],\n",
       "       [ 72.62371425],\n",
       "       [ 73.27585667],\n",
       "       [ 74.25453405],\n",
       "       [ 75.07427782],\n",
       "       [ 75.81213236],\n",
       "       [ 76.69801335],\n",
       "       [ 75.77341492],\n",
       "       [ 77.24645391],\n",
       "       [ 77.10147161],\n",
       "       [ 77.81632498],\n",
       "       [ 77.66817528],\n",
       "       [ 78.42503656],\n",
       "       [ 78.64139465],\n",
       "       [ 77.81050005],\n",
       "       [ 78.32453697],\n",
       "       [ 79.08511568],\n",
       "       [ 79.28434414],\n",
       "       [ 78.42183237],\n",
       "       [ 78.12620301],\n",
       "       [ 78.07970253],\n",
       "       [ 78.20140857],\n",
       "       [ 79.95565492],\n",
       "       [ 78.38736534],\n",
       "       [ 78.50772767],\n",
       "       [ 78.79850162],\n",
       "       [ 79.98849294],\n",
       "       [ 80.43571589],\n",
       "       [ 79.373419  ],\n",
       "       [ 79.25580329],\n",
       "       [ 79.65486779],\n",
       "       [ 79.38730182],\n",
       "       [ 79.30895396],\n",
       "       [ 78.68780024],\n",
       "       [ 78.36060022],\n",
       "       [ 77.4439003 ],\n",
       "       [ 76.4459    ],\n",
       "       [ 74.79120049],\n",
       "       [ 73.74342234],\n",
       "       [ 73.51101598],\n",
       "       [ 73.69480618],\n",
       "       [ 72.22103027],\n",
       "       [ 71.29324273],\n",
       "       [ 70.4643    ],\n",
       "       [ 69.8752    ],\n",
       "       [ 69.64891775],\n",
       "       [ 69.02650595],\n",
       "       [ 69.65620134],\n",
       "       [ 68.22990708],\n",
       "       [ 66.8291178 ],\n",
       "       [ 65.43031915],\n",
       "       [ 64.23730356],\n",
       "       [ 63.37640041],\n",
       "       [ 62.49410036],\n",
       "       [ 63.08686509],\n",
       "       [ 60.94903015],\n",
       "       [ 62.7150939 ],\n",
       "       [ 60.56712416],\n",
       "       [ 62.40957029],\n",
       "       [ 61.08911022],\n",
       "       [ 60.87820319],\n",
       "       [ 60.70441189],\n",
       "       [ 60.59554824],\n",
       "       [ 61.76144448],\n",
       "       [ 64.09192228],\n",
       "       [ 64.58930795],\n",
       "       [ 65.41809894],\n",
       "       [ 66.0248665 ],\n",
       "       [ 67.28047997],\n",
       "       [ 68.4046559 ],\n",
       "       [ 69.66722513],\n",
       "       [ 70.35541828],\n",
       "       [ 67.78297481],\n",
       "       [ 68.28880074],\n",
       "       [ 68.59068951],\n",
       "       [ 68.71568857],\n",
       "       [ 69.25170146],\n",
       "       [ 69.5944606 ],\n",
       "       [ 69.30328019],\n",
       "       [ 70.54560626],\n",
       "       [ 70.72391128],\n",
       "       [ 70.80032128],\n",
       "       [ 71.19342145],\n",
       "       [ 71.87993915],\n",
       "       [ 72.73132694],\n",
       "       [ 73.56254191],\n",
       "       [ 74.38895032],\n",
       "       [ 75.21023382],\n",
       "       [ 76.51973078],\n",
       "       [ 75.91095063],\n",
       "       [ 75.57720854],\n",
       "       [ 75.78188862],\n",
       "       [ 77.27175361],\n",
       "       [ 77.40441813],\n",
       "       [ 78.30528134],\n",
       "       [ 78.71198751],\n",
       "       [ 78.91809709],\n",
       "       [ 79.44492202],\n",
       "       [ 78.96771409],\n",
       "       [ 79.31090729],\n",
       "       [ 79.73158405],\n",
       "       [ 79.36551709],\n",
       "       [ 79.49988664],\n",
       "       [ 79.86311237],\n",
       "       [ 80.12976729],\n",
       "       [ 80.28364998],\n",
       "       [ 80.84943241],\n",
       "       [ 81.33862299],\n",
       "       [ 83.06119141],\n",
       "       [ 84.36501447],\n",
       "       [ 85.09691218],\n",
       "       [ 85.2273067 ],\n",
       "       [ 85.45503037],\n",
       "       [ 87.025279  ],\n",
       "       [ 87.51978217],\n",
       "       [ 86.71903534],\n",
       "       [ 88.10449229],\n",
       "       [ 88.82290519],\n",
       "       [ 89.53921737],\n",
       "       [ 88.62732392],\n",
       "       [ 88.18403862],\n",
       "       [ 88.55341001],\n",
       "       [ 88.76747163],\n",
       "       [ 90.30352935],\n",
       "       [ 89.86395747],\n",
       "       [ 91.28552127],\n",
       "       [ 91.88206444],\n",
       "       [ 92.47317644],\n",
       "       [ 93.52134454],\n",
       "       [ 94.1592347 ],\n",
       "       [ 95.12975187],\n",
       "       [ 95.25732163],\n",
       "       [ 96.275945  ],\n",
       "       [ 96.54887416],\n",
       "       [ 94.90896138],\n",
       "       [ 96.82142999],\n",
       "       [ 95.81206226],\n",
       "       [ 96.6791555 ],\n",
       "       [ 96.13310008],\n",
       "       [ 95.7020002 ],\n",
       "       [ 95.34960209],\n",
       "       [ 95.30283293],\n",
       "       [ 94.8095971 ],\n",
       "       [ 95.95365122],\n",
       "       [ 98.44828898],\n",
       "       [101.39279237],\n",
       "       [103.6324717 ],\n",
       "       [105.77310308],\n",
       "       [107.81413331],\n",
       "       [110.15166456],\n",
       "       [111.87679731],\n",
       "       [110.62981681],\n",
       "       [111.67064068],\n",
       "       [112.77467318],\n",
       "       [113.03455526],\n",
       "       [111.88029048],\n",
       "       [113.55023813],\n",
       "       [114.75881109],\n",
       "       [115.08913299],\n",
       "       [116.29385861],\n",
       "       [120.40527771],\n",
       "       [122.07860754],\n",
       "       [123.66787167],\n",
       "       [125.21431533],\n",
       "       [124.66733596],\n",
       "       [127.3358179 ],\n",
       "       [129.19056047],\n",
       "       [131.86804218],\n",
       "       [126.86604087],\n",
       "       [126.94701417],\n",
       "       [125.48100019],\n",
       "       [124.74300041],\n",
       "       [124.16500361],\n",
       "       [122.91601157],\n",
       "       [121.79500073],\n",
       "       [120.87750188],\n",
       "       [119.13500217],\n",
       "       [116.36500014],\n",
       "       [114.72500011],\n",
       "       [113.18100041],\n",
       "       [114.25469566],\n",
       "       [112.52142609],\n",
       "       [110.98305138],\n",
       "       [114.2828812 ],\n",
       "       [113.21543101],\n",
       "       [112.40497817],\n",
       "       [113.54976744],\n",
       "       [114.92526276],\n",
       "       [115.10599473],\n",
       "       [115.35015894],\n",
       "       [115.78674556],\n",
       "       [114.42908319],\n",
       "       [115.68463461],\n",
       "       [115.57486851],\n",
       "       [116.82300747],\n",
       "       [119.47514047],\n",
       "       [120.29703902],\n",
       "       [120.43148315],\n",
       "       [117.46817682],\n",
       "       [118.05511023],\n",
       "       [118.1050403 ],\n",
       "       [118.35917759],\n",
       "       [118.42900123],\n",
       "       [118.53900117],\n",
       "       [117.93800029],\n",
       "       [116.96700267],\n",
       "       [116.3760002 ],\n",
       "       [117.97694986],\n",
       "       [114.72800287],\n",
       "       [113.651     ],\n",
       "       [113.00100134],\n",
       "       [112.752458  ],\n",
       "       [114.31667091],\n",
       "       [116.19582736],\n",
       "       [116.96613295],\n",
       "       [117.16884945],\n",
       "       [117.92424536],\n",
       "       [118.86741973],\n",
       "       [119.60640252],\n",
       "       [119.79215851],\n",
       "       [119.55420368],\n",
       "       [118.07202697],\n",
       "       [117.97453889],\n",
       "       [118.00638653],\n",
       "       [117.67600012],\n",
       "       [117.51334991],\n",
       "       [117.37668469],\n",
       "       [117.04779687],\n",
       "       [118.5061995 ],\n",
       "       [119.02705889],\n",
       "       [119.65513618],\n",
       "       [120.74954873],\n",
       "       [121.52107997],\n",
       "       [122.06532136],\n",
       "       [123.05324938],\n",
       "       [122.99065988],\n",
       "       [120.87428386],\n",
       "       [121.48968743],\n",
       "       [122.01700011],\n",
       "       [123.60874466],\n",
       "       [124.72596804],\n",
       "       [125.93562467],\n",
       "       [126.94548539],\n",
       "       [127.09814589],\n",
       "       [128.51492992],\n",
       "       [129.79908952],\n",
       "       [130.40764544],\n",
       "       [131.76223598],\n",
       "       [133.69831538],\n",
       "       [134.15165408]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f381de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoluted Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6349001500594025"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean Absoluted Error\")\n",
    "np.mean(abs(np.array(final_test_y)-final_pred)) ## Mean Absolute Error of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1659f5e",
   "metadata": {},
   "source": [
    "### Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8d53847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_start_date = 20190101\n",
    "plot_end_date = 20201231\n",
    "keep = (final_test_y.index >= plot_start_date) & (final_test_y.index <= plot_end_date)\n",
    "final_pred = pd.DataFrame(data=final_pred,index = final_test_y.index, columns = [\"Predicted\"])\n",
    "plot_test_y = final_test_y[keep]\n",
    "plot_pred = final_pred[keep]\n",
    "\n",
    "string_index =  plot_test_y.index.map(str)\n",
    "\n",
    "plt.plot(string_index, plot_test_y[\"result_price\"], label = \"Actual\", color = 'Black')\n",
    "plt.plot(string_index, plot_pred[\"Predicted\"], label = \"Predicted\", color = 'Orange')\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.title(\"Prediction of \"+stock.upper()+\" using CNN-LSTM\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"plot/CNN_LSTM/\"+stock.upper()+\"-day(\"+str(num_day_to_predict)+\").jpg\",\n",
    "            dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617290e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_23895/3608706721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_test_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Actual\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Predicted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_test_y' is not defined"
     ]
    }
   ],
   "source": [
    "abc = pd.concat([plot_test_y,plot_pred], ignore_index=True, sort=False,axis=1)\n",
    "abc.columns = [\"Actual\",\"Predicted\"]\n",
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d419b7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>boll</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdh</th>\n",
       "      <th>macds</th>\n",
       "      <th>rsi_11</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19840907</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10028</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>96970899</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840910</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.09905</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>75265237</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.102049</td>\n",
       "      <td>0.100351</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840911</th>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10456</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>177479896</td>\n",
       "      <td>0.101713</td>\n",
       "      <td>0.103590</td>\n",
       "      <td>0.099837</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>77.134146</td>\n",
       "      <td>76.758045</td>\n",
       "      <td>76.303318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840912</th>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10334</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>155043826</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.103762</td>\n",
       "      <td>0.098638</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>32.201239</td>\n",
       "      <td>32.592743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840913</th>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10548</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>241475025</td>\n",
       "      <td>0.101996</td>\n",
       "      <td>0.106191</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>68.412723</td>\n",
       "      <td>68.025100</td>\n",
       "      <td>67.561551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211021</th>\n",
       "      <td>148.81000</td>\n",
       "      <td>149.64000</td>\n",
       "      <td>147.87000</td>\n",
       "      <td>149.48000</td>\n",
       "      <td>61420990</td>\n",
       "      <td>143.875000</td>\n",
       "      <td>149.793245</td>\n",
       "      <td>137.956755</td>\n",
       "      <td>0.375617</td>\n",
       "      <td>1.069014</td>\n",
       "      <td>-0.693396</td>\n",
       "      <td>65.673205</td>\n",
       "      <td>61.532287</td>\n",
       "      <td>57.199864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022</th>\n",
       "      <td>149.69000</td>\n",
       "      <td>150.18000</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>148.69000</td>\n",
       "      <td>58883443</td>\n",
       "      <td>143.963500</td>\n",
       "      <td>150.121546</td>\n",
       "      <td>137.805454</td>\n",
       "      <td>0.577001</td>\n",
       "      <td>1.016318</td>\n",
       "      <td>-0.439317</td>\n",
       "      <td>61.924694</td>\n",
       "      <td>58.867114</td>\n",
       "      <td>55.611436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025</th>\n",
       "      <td>148.68000</td>\n",
       "      <td>149.37000</td>\n",
       "      <td>147.62110</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>50720556</td>\n",
       "      <td>144.127000</td>\n",
       "      <td>150.607481</td>\n",
       "      <td>137.646519</td>\n",
       "      <td>0.724216</td>\n",
       "      <td>0.930826</td>\n",
       "      <td>-0.206610</td>\n",
       "      <td>61.679591</td>\n",
       "      <td>58.693837</td>\n",
       "      <td>55.508996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211026</th>\n",
       "      <td>149.33000</td>\n",
       "      <td>150.84000</td>\n",
       "      <td>149.01010</td>\n",
       "      <td>149.32000</td>\n",
       "      <td>60893395</td>\n",
       "      <td>144.497500</td>\n",
       "      <td>151.284341</td>\n",
       "      <td>137.710659</td>\n",
       "      <td>0.885547</td>\n",
       "      <td>0.873726</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>63.821803</td>\n",
       "      <td>60.401009</td>\n",
       "      <td>56.649320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211027</th>\n",
       "      <td>149.36000</td>\n",
       "      <td>149.73000</td>\n",
       "      <td>148.49000</td>\n",
       "      <td>148.85000</td>\n",
       "      <td>55925403</td>\n",
       "      <td>144.798500</td>\n",
       "      <td>151.804399</td>\n",
       "      <td>137.792601</td>\n",
       "      <td>0.964362</td>\n",
       "      <td>0.762033</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>61.219811</td>\n",
       "      <td>58.598318</td>\n",
       "      <td>55.614834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9362 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               open       high        low      close     volume        boll  \\\n",
       "DATE                                                                          \n",
       "19840907    0.10150    0.10274    0.10028    0.10150   96970899    0.101500   \n",
       "19840910    0.10150    0.10181    0.09905    0.10090   75265237    0.101200   \n",
       "19840911    0.10181    0.10456    0.10181    0.10274  177479896    0.101713   \n",
       "19840912    0.10274    0.10334    0.09966    0.09966  155043826    0.101200   \n",
       "19840913    0.10518    0.10548    0.10518    0.10518  241475025    0.101996   \n",
       "...             ...        ...        ...        ...        ...         ...   \n",
       "20211021  148.81000  149.64000  147.87000  149.48000   61420990  143.875000   \n",
       "20211022  149.69000  150.18000  148.64000  148.69000   58883443  143.963500   \n",
       "20211025  148.68000  149.37000  147.62110  148.64000   50720556  144.127000   \n",
       "20211026  149.33000  150.84000  149.01010  149.32000   60893395  144.497500   \n",
       "20211027  149.36000  149.73000  148.49000  148.85000   55925403  144.798500   \n",
       "\n",
       "             boll_ub     boll_lb      macd     macdh     macds     rsi_11  \\\n",
       "DATE                                                                        \n",
       "19840907         NaN         NaN  0.000000  0.000000  0.000000        NaN   \n",
       "19840910    0.102049    0.100351 -0.000013 -0.000006 -0.000007   0.000000   \n",
       "19840911    0.103590    0.099837  0.000040  0.000028  0.000012  77.134146   \n",
       "19840912    0.103762    0.098638 -0.000048 -0.000040 -0.000008  31.870001   \n",
       "19840913    0.106191    0.097801  0.000125  0.000094  0.000031  68.412723   \n",
       "...              ...         ...       ...       ...       ...        ...   \n",
       "20211021  149.793245  137.956755  0.375617  1.069014 -0.693396  65.673205   \n",
       "20211022  150.121546  137.805454  0.577001  1.016318 -0.439317  61.924694   \n",
       "20211025  150.607481  137.646519  0.724216  0.930826 -0.206610  61.679591   \n",
       "20211026  151.284341  137.710659  0.885547  0.873726  0.011821  63.821803   \n",
       "20211027  151.804399  137.792601  0.964362  0.762033  0.202329  61.219811   \n",
       "\n",
       "             rsi_14     rsi_21  \n",
       "DATE                            \n",
       "19840907        NaN        NaN  \n",
       "19840910   0.000000   0.000000  \n",
       "19840911  76.758045  76.303318  \n",
       "19840912  32.201239  32.592743  \n",
       "19840913  68.025100  67.561551  \n",
       "...             ...        ...  \n",
       "20211021  61.532287  57.199864  \n",
       "20211022  58.867114  55.611436  \n",
       "20211025  58.693837  55.508996  \n",
       "20211026  60.401009  56.649320  \n",
       "20211027  58.598318  55.614834  \n",
       "\n",
       "[9362 rows x 14 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8badefc5",
   "metadata": {},
   "source": [
    "# CNN_LSTM (Direction Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645096b",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc822736",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = custom_split(data,start = 20130101,end = 20171031)\n",
    "valid_X = custom_split(data,start = 20171101,end = 20181231)\n",
    "test_X = custom_split(data,start = 20190101,end = 20201231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4337ac",
   "metadata": {},
   "source": [
    "### Label the target result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14b820c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we use 10 days price data to predict opening price of the 11th day\n",
    "num_day_to_predict = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18abe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_result_target_price(X,num_day,result_col_name = \"Action\"):\n",
    "    y = pd.DataFrame(np.nan, index=X.index, columns=[result_col_name])\n",
    "    status = \"Hold\"\n",
    "    for i in range(len(X)-num_day):\n",
    "        last_10_day_mean = np.mean(X.iloc[i:i+num_day,0])\n",
    "        if X.iloc[i+num_day,0]>last_10_day_mean*1.01:\n",
    "            y.iloc[i+num_day_to_predict,0] = 1\n",
    "            status = \"Buy\"\n",
    "            if i <= 10:\n",
    "                print(status)\n",
    "        elif X.iloc[i+num_day,0]<last_10_day_mean/1.01:\n",
    "            y.iloc[i+num_day_to_predict,0] = 0\n",
    "            status = \"Sell\"\n",
    "            if i <= 10:\n",
    "                print(status)\n",
    "        else:\n",
    "            if status == \"Hold\" or status == \"Sell\":\n",
    "                y.iloc[i+num_day_to_predict,0] = 0\n",
    "            elif status == \"Buy\":\n",
    "                y.iloc[i+num_day_to_predict,0] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "131acc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Sell\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Sell\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Buy\n",
      "Buy\n"
     ]
    }
   ],
   "source": [
    "# y value meaning {1: Buy, 0: Sell}\n",
    "train_y = produce_result_target_price(train_X,num_day_to_predict)\n",
    "valid_y = produce_result_target_price(valid_X,num_day_to_predict)\n",
    "test_y = produce_result_target_price(test_X,num_day_to_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7c6f85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20201118</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201119</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201120</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201123</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201124</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201125</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201127</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201130</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201201</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201202</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201203</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201204</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201207</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201208</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201209</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201210</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201211</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201215</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201216</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201217</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201218</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201221</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201222</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201223</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201224</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201228</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201229</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201230</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201231</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Action\n",
       "DATE            \n",
       "20201118     1.0\n",
       "20201119     1.0\n",
       "20201120     1.0\n",
       "20201123     0.0\n",
       "20201124     0.0\n",
       "20201125     0.0\n",
       "20201127     0.0\n",
       "20201130     0.0\n",
       "20201201     1.0\n",
       "20201202     1.0\n",
       "20201203     1.0\n",
       "20201204     1.0\n",
       "20201207     1.0\n",
       "20201208     1.0\n",
       "20201209     1.0\n",
       "20201210     1.0\n",
       "20201211     1.0\n",
       "20201214     1.0\n",
       "20201215     1.0\n",
       "20201216     1.0\n",
       "20201217     1.0\n",
       "20201218     1.0\n",
       "20201221     1.0\n",
       "20201222     1.0\n",
       "20201223     1.0\n",
       "20201224     1.0\n",
       "20201228     1.0\n",
       "20201229     1.0\n",
       "20201230     1.0\n",
       "20201231     1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a29963",
   "metadata": {},
   "source": [
    "### Transform the X, y data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e33e67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_X_data_to_tensor(X,num_day):\n",
    "    # Initiate tensor for X\n",
    "    x_first = X.iloc[0:num_day,:]\n",
    "    x_mean = x_first.mean(axis=0) # Get the mean of the 10-day frame\n",
    "    x_std = x_first.std(axis=0) # Get the std of the 10-day frame\n",
    "    x_first = x_first.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "    x_tf_data = [tf.convert_to_tensor(np.array(x_first),dtype = tf.float32)]\n",
    "    \n",
    "    for i in range(1,len(X)-num_day):   \n",
    "        x_window = X.iloc[i:i+num_day,:] # Set the window as a 10-day frame \n",
    "        x_mean = x_window.mean(axis=0) # Get the mean of the 10-day frame\n",
    "        x_std = x_window.std(axis=0) # Get the std of the 10-day frame\n",
    "        x_window = x_window.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "        \n",
    "        x_next_tf = tf.convert_to_tensor(np.array(x_window),dtype = tf.float32)\n",
    "        x_tf_data = tf.concat([x_tf_data, [x_next_tf]], 0)\n",
    "        \n",
    "    return tf.reshape(x_tf_data,(-1,10,14,1))\n",
    "def transform_y_data_to_tensor(y,num_day):\n",
    "    temp_y = y.dropna()\n",
    "    y_tf_data = []\n",
    "    for ind in temp_y.index:\n",
    "        if temp_y.loc[ind,\"Action\"] == 1:\n",
    "            y_tf_data.append([1,0])\n",
    "        elif temp_y.loc[ind,\"Action\"] == 0:\n",
    "            y_tf_data.append([0,1])\n",
    "    y_tf_data = tf.convert_to_tensor(y_tf_data)\n",
    "        \n",
    "    return y_tf_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3962a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_X = transform_X_data_to_tensor(train_X,num_day_to_predict)\n",
    "tf_train_y = transform_y_data_to_tensor(train_y,num_day_to_predict)\n",
    "tf_valid_X = transform_X_data_to_tensor(valid_X,num_day_to_predict)\n",
    "tf_valid_y = transform_y_data_to_tensor(valid_y,num_day_to_predict)\n",
    "tf_test_X = transform_X_data_to_tensor(test_X,num_day_to_predict)\n",
    "tf_test_y = transform_y_data_to_tensor(test_y,num_day_to_predict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd87fe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1208, 10, 14, 1)\n",
      "(1208, 2)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int32'>\n",
      "(282, 10, 14, 1)\n",
      "(282, 2)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int32'>\n",
      "(495, 10, 14, 1)\n",
      "(495, 2)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print(tf_train_X.shape)\n",
    "print(tf_train_y.shape)\n",
    "print(tf_train_X.dtype)\n",
    "print(tf_train_y.dtype)\n",
    "\n",
    "print(tf_valid_X.shape)\n",
    "print(tf_valid_y.shape)\n",
    "print(tf_valid_X.dtype)\n",
    "print(tf_valid_y.dtype)\n",
    "\n",
    "print(tf_test_X.shape)\n",
    "print(tf_test_y.shape)\n",
    "print(tf_test_X.dtype)\n",
    "print(tf_test_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993d4d5",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a78b2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def myModel(input_shape,\n",
    "            encoder_unit = 100,\n",
    "            repeat_vector_n = 10):\n",
    "    \n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    print(\"Input: \",inputs.shape)\n",
    "    \n",
    "    # First Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 64,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"1 Cov: \",x.shape)\n",
    "    \n",
    "    # Second Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 16,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"2 Cov: \",x.shape)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = layers.Flatten()(x)\n",
    "    print(\"Flatten: \",x.shape)\n",
    "    \n",
    "    # Repeat Vector Layer\n",
    "    x = layers.RepeatVector(n = repeat_vector_n)(x)\n",
    "    print(\"RepeatVector: \",x.shape)\n",
    "    \n",
    "    # Connect to LSTM\n",
    "    x = layers.LSTM(units = encoder_unit, input_shape=(5,1))(x)\n",
    "    print(\"LSTM: \",x.shape)\n",
    "    \n",
    "    # Second Flatten Layer\n",
    "    x = layers.Flatten()(x)\n",
    "    print(\"Flatten: \",x.shape)\n",
    "    \n",
    "    # Add the Dense Layer with relu activation\n",
    "    x = layers.Dense(units = 50,activation = \"relu\")(x)\n",
    "    print(\"1 Dense: \",x.shape)\n",
    "    \n",
    "    # Add the last Dense Layer with sigmoid activation\n",
    "    outputs = layers.Dense(units = 2,activation = \"softmax\")(x)\n",
    "    print(\"Output: \",outputs.shape)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab4a87",
   "metadata": {},
   "source": [
    "### Model Training and Fitting and Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5917822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 50)\n",
      "Flatten:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:51:17.225821: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_211387_211872_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_212045' and '__inference___backward_standard_lstm_211387_211872' both implement 'lstm_88fafc1d-6bb9-4aa0-aa31-2944b70de569' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 14s 12ms/sample - loss: 0.1182 - root_mean_squared_error: 0.3428\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0770 - root_mean_squared_error: 0.2817\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0920 - root_mean_squared_error: 0.2992\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0674 - root_mean_squared_error: 0.2609\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0598 - root_mean_squared_error: 0.2448\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0544 - root_mean_squared_error: 0.2352\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0615 - root_mean_squared_error: 0.2440\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0488 - root_mean_squared_error: 0.2171\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0491 - root_mean_squared_error: 0.2210\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0433 - root_mean_squared_error: 0.2099\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0482 - root_mean_squared_error: 0.2211\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0409 - root_mean_squared_error: 0.2031\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 12s 10ms/sample - loss: 0.0475 - root_mean_squared_error: 0.2138\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0395 - root_mean_squared_error: 0.1970\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0405 - root_mean_squared_error: 0.2018\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0276 - root_mean_squared_error: 0.1688\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0508 - root_mean_squared_error: 0.2294\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.0383 - root_mean_squared_error: 0.1971\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0419 - root_mean_squared_error: 0.2030\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0345 - root_mean_squared_error: 0.1813\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0333 - root_mean_squared_error: 0.1819\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0354 - root_mean_squared_error: 0.1883\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.0328 - root_mean_squared_error: 0.1815\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 12s 10ms/sample - loss: 0.0319 - root_mean_squared_error: 0.1792\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 13s 10ms/sample - loss: 0.0377 - root_mean_squared_error: 0.1936\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 13s 10ms/sample - loss: 0.0188 - root_mean_squared_error: 0.1423\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 13s 11ms/sample - loss: 0.0220 - root_mean_squared_error: 0.1484\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 12s 9ms/sample - loss: 0.0282 - root_mean_squared_error: 0.1664\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 12s 10ms/sample - loss: 0.0354 - root_mean_squared_error: 0.1880\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 14s 11ms/sample - loss: 0.0285 - root_mean_squared_error: 0.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:56:51.098765: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_216872' and '__inference_standard_lstm_216872_specialized_for_model_12_lstm_12_StatefulPartitionedCall_at___inference_distributed_function_217225' both implement 'lstm_1a9e4fce-4fec-4456-b355-05b02b657384' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 0.0628 - root_mean_squared_error: 0.2467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06087604331526351, 0.24673072]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fb36741ef50>]\n",
      "Validation:  [0.06087604331526351, 0.24673072]\n",
      "INFO:tensorflow:Assets written to: model/cnn_lstm_classify_best/assets\n"
     ]
    }
   ],
   "source": [
    "optimizer_list = [\"Adam\"]\n",
    "epoch_list = [30]\n",
    "batch_list = [50]\n",
    "encoder_list = [50]\n",
    "lr_list = [0.005]\n",
    "train_df = pd.DataFrame(columns = [\"Epoch\",\"Batch\",\"Optimizer\",\"LR\",\"Encoder Unit\",\"Loss\",\"Metrics\",\"Validation\"])\n",
    "best_model = \"\"\n",
    "best_valid = 99999\n",
    "metrics = [keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "\n",
    "for opti in optimizer_list:\n",
    "    for epochs in epoch_list:\n",
    "        for batchs in batch_list:\n",
    "            for lr in lr_list:\n",
    "                for encoder_u in encoder_list:\n",
    "\n",
    "                    model = myModel(input_shape=(num_day_to_predict,train_X.shape[1],1),\n",
    "                                    encoder_unit = encoder_u,\n",
    "                                    repeat_vector_n = 100\n",
    "                                   )\n",
    "\n",
    "                    if opti == \"Adam\":\n",
    "                        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "                    model.compile(\n",
    "                        optimizer=optimizer,\n",
    "                        loss=keras.losses.MeanSquaredError(),\n",
    "                        metrics=metrics,\n",
    "                    )\n",
    "\n",
    "                    history = model.fit(\n",
    "                            tf_train_X,\n",
    "                            tf_train_y,\n",
    "                            epochs = epochs,\n",
    "                            steps_per_epoch = batchs,\n",
    "                        )\n",
    "\n",
    "                    results = model.evaluate(tf_valid_X, tf_valid_y, batch_size=batchs)\n",
    "                    print(results)\n",
    "                    print(\"===== Summary =====\")\n",
    "                    print(\"Epoch: \",epochs)\n",
    "                    print(\"Batch Size: \",batchs)\n",
    "                    print(\"Optimizer: \",opti)\n",
    "                    print(\"Learning Rate: \",lr)\n",
    "                    print(\"Encoder Units: \",encoder_u)\n",
    "                    print(\"Loss Function: \", \"Categorical CrossEntropy\")\n",
    "                    print(\"Metrics: \", metrics)\n",
    "                    print(\"Validation: \",results)\n",
    "                    if results[0] < best_valid:\n",
    "                        best_valid = results[0]\n",
    "                        best_model = model\n",
    "                    train_df = train_df.append({\"Epoch\": epochs,\n",
    "                                                \"Batch\": batchs,\n",
    "                                                \"Optimizer\": opti,\n",
    "                                                \"LR\": lr,\n",
    "                                                \"Encoder Unit\": encoder_u,\n",
    "                                                \"Loss\": \"Categorical CrossEntropy\",\n",
    "                                                \"Metrics\": metrics,\n",
    "                                                \"Validation\":results}, ignore_index=True)\n",
    "best_model.save(\"model/cnn_lstm_classify_best\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2c75f",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf389a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('model/cnn_lstm_classify_best')\n",
    "predictions = loaded_model.predict(tf_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24f05e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (495, 2)\n",
      "[[9.41344440e-01 5.86555488e-02]\n",
      " [9.91463602e-01 8.53640027e-03]\n",
      " [9.93585050e-01 6.41490053e-03]\n",
      " [9.93623793e-01 6.37628557e-03]\n",
      " [9.93424892e-01 6.57508429e-03]\n",
      " [9.92170036e-01 7.82998279e-03]\n",
      " [9.70246077e-01 2.97539532e-02]\n",
      " [9.93039966e-01 6.96004974e-03]\n",
      " [9.71129656e-01 2.88703591e-02]\n",
      " [9.94750440e-01 5.24956500e-03]\n",
      " [9.95162070e-01 4.83799540e-03]\n",
      " [9.96351838e-01 3.64819448e-03]\n",
      " [9.91980791e-01 8.01918563e-03]\n",
      " [9.96442258e-01 3.55768763e-03]\n",
      " [9.96479452e-01 3.52055486e-03]\n",
      " [9.96010184e-01 3.98982549e-03]\n",
      " [9.94975567e-01 5.02443127e-03]\n",
      " [9.94274914e-01 5.72506525e-03]\n",
      " [9.91799474e-01 8.20049550e-03]\n",
      " [9.89132583e-01 1.08673805e-02]\n",
      " [9.91679251e-01 8.32067430e-03]\n",
      " [9.89179015e-01 1.08209690e-02]\n",
      " [9.37246263e-01 6.27537593e-02]\n",
      " [9.60366905e-01 3.96330394e-02]\n",
      " [9.93621886e-01 6.37807371e-03]\n",
      " [9.90061760e-01 9.93829593e-03]\n",
      " [9.92518663e-01 7.48128165e-03]\n",
      " [9.92781222e-01 7.21882842e-03]\n",
      " [9.92989898e-01 7.01004500e-03]\n",
      " [9.87472653e-01 1.25272805e-02]\n",
      " [9.92940187e-01 7.05985958e-03]\n",
      " [9.95060623e-01 4.93937964e-03]\n",
      " [9.95963454e-01 4.03655740e-03]\n",
      " [9.97612834e-01 2.38717161e-03]\n",
      " [9.81361091e-01 1.86389256e-02]\n",
      " [8.54208648e-01 1.45791382e-01]\n",
      " [5.86297596e-03 9.94137049e-01]\n",
      " [9.88717794e-01 1.12821897e-02]\n",
      " [9.94335830e-01 5.66409621e-03]\n",
      " [9.98027027e-01 1.97291211e-03]\n",
      " [9.97863948e-01 2.13606749e-03]\n",
      " [9.97868896e-01 2.13114196e-03]\n",
      " [9.96908486e-01 3.09153413e-03]\n",
      " [9.95827973e-01 4.17199684e-03]\n",
      " [9.94113505e-01 5.88656357e-03]\n",
      " [9.96078789e-01 3.92121077e-03]\n",
      " [9.96618986e-01 3.38104041e-03]\n",
      " [9.96600330e-01 3.39967851e-03]\n",
      " [9.94561493e-01 5.43849077e-03]\n",
      " [9.94968951e-01 5.03111258e-03]\n",
      " [9.57238257e-01 4.27617431e-02]\n",
      " [9.57031071e-01 4.29689102e-02]\n",
      " [9.76770639e-01 2.32293718e-02]\n",
      " [9.93091822e-01 6.90818019e-03]\n",
      " [9.93289053e-01 6.71097264e-03]\n",
      " [9.93103623e-01 6.89645298e-03]\n",
      " [9.93938208e-01 6.06182730e-03]\n",
      " [9.95661795e-01 4.33816155e-03]\n",
      " [9.93825793e-01 6.17428264e-03]\n",
      " [9.94603693e-01 5.39625296e-03]\n",
      " [9.93264914e-01 6.73505105e-03]\n",
      " [9.94717658e-01 5.28238341e-03]\n",
      " [9.21630085e-01 7.83698931e-02]\n",
      " [5.62809646e-01 4.37190294e-01]\n",
      " [9.90174413e-01 9.82563291e-03]\n",
      " [9.90760803e-01 9.23922937e-03]\n",
      " [9.92412508e-01 7.58743472e-03]\n",
      " [9.93489027e-01 6.51102699e-03]\n",
      " [9.93612349e-01 6.38760254e-03]\n",
      " [9.93301630e-01 6.69841282e-03]\n",
      " [9.95065272e-01 4.93472675e-03]\n",
      " [8.85715365e-01 1.14284635e-01]\n",
      " [1.21772690e-02 9.87822711e-01]\n",
      " [9.72715616e-01 2.72844341e-02]\n",
      " [9.76775348e-01 2.32246481e-02]\n",
      " [9.96991396e-01 3.00856936e-03]\n",
      " [9.96789515e-01 3.21047660e-03]\n",
      " [9.88338709e-01 1.16612576e-02]\n",
      " [1.29330710e-01 8.70669246e-01]\n",
      " [2.52706208e-03 9.97472942e-01]\n",
      " [2.02157791e-03 9.97978389e-01]\n",
      " [2.41213269e-03 9.97587919e-01]\n",
      " [2.33614910e-03 9.97663856e-01]\n",
      " [1.83425471e-03 9.98165786e-01]\n",
      " [2.60235835e-03 9.97397661e-01]\n",
      " [1.40881201e-03 9.98591125e-01]\n",
      " [1.17359904e-03 9.98826444e-01]\n",
      " [2.91125779e-03 9.97088730e-01]\n",
      " [6.43923366e-03 9.93560791e-01]\n",
      " [2.07031239e-03 9.97929692e-01]\n",
      " [3.64696025e-03 9.96353030e-01]\n",
      " [2.32174364e-03 9.97678220e-01]\n",
      " [3.28346156e-03 9.96716559e-01]\n",
      " [6.11546403e-03 9.93884504e-01]\n",
      " [3.87896737e-03 9.96120989e-01]\n",
      " [2.47939606e-03 9.97520626e-01]\n",
      " [2.12573521e-02 9.78742599e-01]\n",
      " [9.84851062e-01 1.51489004e-02]\n",
      " [9.95720327e-01 4.27967310e-03]\n",
      " [9.93718624e-01 6.28141034e-03]\n",
      " [9.91647243e-01 8.35276768e-03]\n",
      " [9.92291391e-01 7.70860678e-03]\n",
      " [9.90401566e-01 9.59850382e-03]\n",
      " [9.90324378e-01 9.67562757e-03]\n",
      " [9.89455879e-01 1.05441287e-02]\n",
      " [9.90116179e-01 9.88383032e-03]\n",
      " [9.94636595e-01 5.36341546e-03]\n",
      " [9.95019794e-01 4.98024654e-03]\n",
      " [9.97500360e-01 2.49962229e-03]\n",
      " [9.96379077e-01 3.62090068e-03]\n",
      " [9.95665252e-01 4.33473149e-03]\n",
      " [9.58726883e-01 4.12731171e-02]\n",
      " [9.89093184e-01 1.09067746e-02]\n",
      " [9.68876481e-01 3.11235432e-02]\n",
      " [9.80881453e-01 1.91185791e-02]\n",
      " [9.92454112e-01 7.54590193e-03]\n",
      " [9.94566202e-01 5.43384766e-03]\n",
      " [9.93417978e-01 6.58207806e-03]\n",
      " [9.91599441e-01 8.40060320e-03]\n",
      " [9.85998154e-01 1.40018072e-02]\n",
      " [9.21345472e-01 7.86544979e-02]\n",
      " [9.90257621e-01 9.74237174e-03]\n",
      " [1.24360360e-01 8.75639737e-01]\n",
      " [9.87783492e-01 1.22165708e-02]\n",
      " [9.94274795e-01 5.72522869e-03]\n",
      " [9.90217984e-01 9.78199020e-03]\n",
      " [9.91607428e-01 8.39264318e-03]\n",
      " [9.91162658e-01 8.83740000e-03]\n",
      " [9.86375511e-01 1.36244539e-02]\n",
      " [9.32104230e-01 6.78957701e-02]\n",
      " [9.91450489e-01 8.54953099e-03]\n",
      " [9.02589917e-01 9.74101573e-02]\n",
      " [9.93860602e-01 6.13933941e-03]\n",
      " [9.92128730e-01 7.87126087e-03]\n",
      " [9.92564499e-01 7.43558770e-03]\n",
      " [9.94446516e-01 5.55343321e-03]\n",
      " [9.94900286e-01 5.09975757e-03]\n",
      " [9.93712842e-01 6.28710026e-03]\n",
      " [5.26153110e-03 9.94738519e-01]\n",
      " [3.94072849e-03 9.96059299e-01]\n",
      " [3.97489499e-03 9.96025085e-01]\n",
      " [3.12010897e-03 9.96879935e-01]\n",
      " [4.04488156e-03 9.95955110e-01]\n",
      " [8.24482273e-03 9.91755128e-01]\n",
      " [1.21785766e-02 9.87821400e-01]\n",
      " [9.74020660e-01 2.59793289e-02]\n",
      " [9.76643324e-01 2.33567245e-02]\n",
      " [9.78671908e-01 2.13280804e-02]\n",
      " [9.91038322e-01 8.96173250e-03]\n",
      " [9.95447814e-01 4.55212221e-03]\n",
      " [9.48553979e-01 5.14460728e-02]\n",
      " [9.95872438e-01 4.12750244e-03]\n",
      " [9.95962441e-01 4.03761026e-03]\n",
      " [9.89671409e-01 1.03285816e-02]\n",
      " [2.21686903e-02 9.77831244e-01]\n",
      " [4.81466623e-03 9.95185316e-01]\n",
      " [2.16178969e-03 9.97838199e-01]\n",
      " [4.38193697e-03 9.95618105e-01]\n",
      " [1.47145090e-03 9.98528481e-01]\n",
      " [4.59186640e-03 9.95408118e-01]\n",
      " [2.88075097e-02 9.71192479e-01]\n",
      " [9.97698486e-01 2.30150670e-03]\n",
      " [9.98081803e-01 1.91822567e-03]\n",
      " [9.98283386e-01 1.71662506e-03]\n",
      " [9.97806847e-01 2.19319738e-03]\n",
      " [9.96187389e-01 3.81266116e-03]\n",
      " [9.95985687e-01 4.01432905e-03]\n",
      " [9.96598899e-01 3.40106804e-03]\n",
      " [9.96299684e-01 3.70035204e-03]\n",
      " [9.90531921e-01 9.46807675e-03]\n",
      " [9.88398492e-01 1.16015291e-02]\n",
      " [9.90677834e-01 9.32216737e-03]\n",
      " [9.84614372e-01 1.53856408e-02]\n",
      " [1.35034144e-01 8.64965856e-01]\n",
      " [3.18869762e-03 9.96811330e-01]\n",
      " [1.69324912e-02 9.83067453e-01]\n",
      " [8.34165607e-03 9.91658270e-01]\n",
      " [2.53534913e-02 9.74646509e-01]\n",
      " [9.78792727e-01 2.12072711e-02]\n",
      " [9.96456802e-01 3.54321697e-03]\n",
      " [9.76393998e-01 2.36059744e-02]\n",
      " [9.57494915e-01 4.25050780e-02]\n",
      " [9.71372306e-01 2.86277365e-02]\n",
      " [9.88715708e-01 1.12843188e-02]\n",
      " [9.90536630e-01 9.46340803e-03]\n",
      " [9.97859299e-01 2.14069104e-03]\n",
      " [9.95592177e-01 4.40780213e-03]\n",
      " [9.94569421e-01 5.43065602e-03]\n",
      " [9.95201111e-01 4.79889289e-03]\n",
      " [9.96044874e-01 3.95515747e-03]\n",
      " [9.95586038e-01 4.41398192e-03]\n",
      " [9.93578792e-01 6.42124750e-03]\n",
      " [9.92184699e-01 7.81530049e-03]\n",
      " [9.95051920e-01 4.94813873e-03]\n",
      " [9.94676828e-01 5.32319164e-03]\n",
      " [9.94281530e-01 5.71850128e-03]\n",
      " [9.93988872e-01 6.01118151e-03]\n",
      " [9.93994296e-01 6.00573048e-03]\n",
      " [9.93869424e-01 6.13049511e-03]\n",
      " [9.92960632e-01 7.03934161e-03]\n",
      " [9.92327154e-01 7.67281326e-03]\n",
      " [9.87275898e-01 1.27240289e-02]\n",
      " [9.96351719e-01 3.64828110e-03]\n",
      " [9.94488955e-01 5.51101333e-03]\n",
      " [9.97281671e-01 2.71829381e-03]\n",
      " [9.95873749e-01 4.12631920e-03]\n",
      " [9.93642151e-01 6.35790732e-03]\n",
      " [9.92806435e-01 7.19350437e-03]\n",
      " [9.92054701e-01 7.94527400e-03]\n",
      " [9.91854191e-01 8.14573932e-03]\n",
      " [9.92904127e-01 7.09587708e-03]\n",
      " [9.86412168e-01 1.35877635e-02]\n",
      " [9.84146416e-01 1.58536565e-02]\n",
      " [9.95105386e-01 4.89460304e-03]\n",
      " [9.92581725e-01 7.41829304e-03]\n",
      " [9.58753824e-01 4.12461832e-02]\n",
      " [9.07568157e-01 9.24318880e-02]\n",
      " [3.11176572e-03 9.96888220e-01]\n",
      " [4.80391691e-03 9.95196044e-01]\n",
      " [2.99358182e-03 9.97006476e-01]\n",
      " [9.53397930e-01 4.66020294e-02]\n",
      " [9.72694993e-01 2.73050312e-02]\n",
      " [9.93912458e-01 6.08751364e-03]\n",
      " [3.86349950e-03 9.96136487e-01]\n",
      " [1.29347574e-03 9.98706460e-01]\n",
      " [2.92868610e-03 9.97071266e-01]\n",
      " [5.78359127e-01 4.21640813e-01]\n",
      " [9.94398296e-01 5.60164219e-03]\n",
      " [9.97004807e-01 2.99517601e-03]\n",
      " [9.93852258e-01 6.14777720e-03]\n",
      " [9.91870284e-01 8.12967494e-03]\n",
      " [9.94416237e-01 5.58373099e-03]\n",
      " [9.96884882e-01 3.11508495e-03]\n",
      " [9.97305274e-01 2.69474182e-03]\n",
      " [9.95588303e-01 4.41172346e-03]\n",
      " [9.96892035e-01 3.10801668e-03]\n",
      " [9.94426012e-01 5.57398563e-03]\n",
      " [9.96747017e-01 3.25299962e-03]\n",
      " [9.95814264e-01 4.18572826e-03]\n",
      " [9.96167600e-01 3.83231929e-03]\n",
      " [9.95912313e-01 4.08773031e-03]\n",
      " [9.95560288e-01 4.43969993e-03]\n",
      " [9.95116591e-01 4.88339318e-03]\n",
      " [9.93817270e-01 6.18272368e-03]\n",
      " [9.95430231e-01 4.56981454e-03]\n",
      " [9.95611370e-01 4.38863644e-03]\n",
      " [9.96056080e-01 3.94393271e-03]\n",
      " [9.84584272e-01 1.54156620e-02]\n",
      " [9.96104956e-01 3.89507087e-03]\n",
      " [9.95889366e-01 4.11070045e-03]\n",
      " [9.97213423e-01 2.78661959e-03]\n",
      " [9.97580409e-01 2.41957209e-03]\n",
      " [9.95941937e-01 4.05805930e-03]\n",
      " [9.94566798e-01 5.43319527e-03]\n",
      " [9.94356692e-01 5.64334262e-03]\n",
      " [9.90949214e-01 9.05082375e-03]\n",
      " [9.92002487e-01 7.99758360e-03]\n",
      " [9.92003739e-01 7.99618289e-03]\n",
      " [9.90847707e-01 9.15228203e-03]\n",
      " [4.14640922e-03 9.95853662e-01]\n",
      " [7.41799735e-03 9.92581964e-01]\n",
      " [9.48169887e-01 5.18301353e-02]\n",
      " [9.72659588e-01 2.73404922e-02]\n",
      " [9.62340653e-01 3.76592986e-02]\n",
      " [7.73431301e-01 2.26568788e-01]\n",
      " [3.12821288e-03 9.96871769e-01]\n",
      " [5.53562352e-03 9.94464338e-01]\n",
      " [9.88489807e-01 1.15101673e-02]\n",
      " [9.91767645e-01 8.23233556e-03]\n",
      " [9.93570209e-01 6.42983522e-03]\n",
      " [9.90570426e-01 9.42955073e-03]\n",
      " [9.90374684e-01 9.62533895e-03]\n",
      " [9.94702876e-01 5.29705826e-03]\n",
      " [9.91395533e-01 8.60445201e-03]\n",
      " [8.68988872e-01 1.31011188e-01]\n",
      " [9.58537403e-03 9.90414619e-01]\n",
      " [2.13490389e-02 9.78651047e-01]\n",
      " [2.97157071e-03 9.97028410e-01]\n",
      " [3.26716155e-03 9.96732831e-01]\n",
      " [2.70218216e-03 9.97297823e-01]\n",
      " [2.15463364e-03 9.97845411e-01]\n",
      " [2.19682418e-03 9.97803152e-01]\n",
      " [1.78800977e-03 9.98211980e-01]\n",
      " [1.44409086e-03 9.98555958e-01]\n",
      " [1.46121770e-01 8.53878260e-01]\n",
      " [9.77725148e-01 2.22749002e-02]\n",
      " [2.43921995e-01 7.56078005e-01]\n",
      " [9.90725100e-01 9.27496795e-03]\n",
      " [7.33772293e-03 9.92662311e-01]\n",
      " [3.01788212e-03 9.96982157e-01]\n",
      " [7.37157650e-03 9.92628336e-01]\n",
      " [6.19038427e-03 9.93809640e-01]\n",
      " [9.63166822e-03 9.90368366e-01]\n",
      " [3.43391392e-03 9.96566117e-01]\n",
      " [3.85952834e-03 9.96140420e-01]\n",
      " [3.16989655e-03 9.96830046e-01]\n",
      " [3.34446342e-03 9.96655464e-01]\n",
      " [5.11502894e-03 9.94884908e-01]\n",
      " [3.90363112e-03 9.96096432e-01]\n",
      " [3.02919396e-03 9.96970773e-01]\n",
      " [1.03581056e-01 8.96418989e-01]\n",
      " [9.87644568e-02 9.01235580e-01]\n",
      " [9.58579659e-01 4.14203927e-02]\n",
      " [9.90747094e-01 9.25294496e-03]\n",
      " [9.80281293e-01 1.97187066e-02]\n",
      " [9.11808908e-01 8.81910697e-02]\n",
      " [6.06923223e-01 3.93076718e-01]\n",
      " [4.49290693e-01 5.50709307e-01]\n",
      " [9.92859244e-01 7.14077381e-03]\n",
      " [9.88826156e-01 1.11738369e-02]\n",
      " [9.96283233e-01 3.71674006e-03]\n",
      " [9.97009456e-01 2.99052894e-03]\n",
      " [9.91269112e-01 8.73087347e-03]\n",
      " [9.93305147e-01 6.69480907e-03]\n",
      " [9.93688941e-01 6.31107390e-03]\n",
      " [9.94901419e-01 5.09860832e-03]\n",
      " [9.93772566e-01 6.22742670e-03]\n",
      " [9.95973408e-01 4.02658759e-03]\n",
      " [9.02338624e-01 9.76614207e-02]\n",
      " [1.92522600e-01 8.07477415e-01]\n",
      " [9.36935902e-01 6.30640835e-02]\n",
      " [1.21800918e-02 9.87819910e-01]\n",
      " [9.51160967e-01 4.88390513e-02]\n",
      " [9.69555199e-01 3.04447971e-02]\n",
      " [9.89017367e-01 1.09826317e-02]\n",
      " [9.93605614e-01 6.39440957e-03]\n",
      " [9.95323598e-01 4.67640301e-03]\n",
      " [9.92379189e-01 7.62086082e-03]\n",
      " [9.93620932e-01 6.37900410e-03]\n",
      " [9.95057225e-01 4.94280970e-03]\n",
      " [9.95083869e-01 4.91607236e-03]\n",
      " [9.94220495e-01 5.77946519e-03]\n",
      " [9.93679643e-01 6.32036244e-03]\n",
      " [9.92374361e-01 7.62565108e-03]\n",
      " [9.94607031e-01 5.39296260e-03]\n",
      " [9.92655396e-01 7.34461704e-03]\n",
      " [9.77377117e-01 2.26228405e-02]\n",
      " [9.90377367e-01 9.62267537e-03]\n",
      " [9.49273467e-01 5.07264994e-02]\n",
      " [9.95048225e-01 4.95179510e-03]\n",
      " [9.94224131e-01 5.77588892e-03]\n",
      " [9.93655682e-01 6.34432957e-03]\n",
      " [9.89684463e-01 1.03155654e-02]\n",
      " [9.89146709e-01 1.08532840e-02]\n",
      " [9.82716799e-01 1.72832161e-02]\n",
      " [9.78106380e-01 2.18935981e-02]\n",
      " [9.89701867e-01 1.02981227e-02]\n",
      " [9.87024903e-01 1.29750790e-02]\n",
      " [9.95734513e-01 4.26545646e-03]\n",
      " [9.90911067e-01 9.08895675e-03]\n",
      " [9.93945062e-01 6.05495321e-03]\n",
      " [9.96904075e-01 3.09593114e-03]\n",
      " [9.97894466e-01 2.10551778e-03]\n",
      " [9.97615337e-01 2.38470989e-03]\n",
      " [9.97656107e-01 2.34381855e-03]\n",
      " [9.97189462e-01 2.81051523e-03]\n",
      " [9.95424449e-01 4.57557477e-03]\n",
      " [9.94042695e-01 5.95730077e-03]\n",
      " [9.94655132e-01 5.34481695e-03]\n",
      " [9.94584739e-01 5.41533297e-03]\n",
      " [9.93421972e-01 6.57801935e-03]\n",
      " [9.93714988e-01 6.28497964e-03]\n",
      " [9.96486425e-01 3.51363956e-03]\n",
      " [9.96115923e-01 3.88410036e-03]\n",
      " [9.98018265e-01 1.98170845e-03]\n",
      " [9.92825389e-01 7.17456453e-03]\n",
      " [9.90887284e-01 9.11267567e-03]\n",
      " [9.91119444e-01 8.88052024e-03]\n",
      " [9.93424654e-01 6.57537160e-03]\n",
      " [9.93881702e-01 6.11833436e-03]\n",
      " [9.92833853e-01 7.16615980e-03]\n",
      " [9.92368400e-01 7.63163622e-03]\n",
      " [9.94963050e-01 5.03700972e-03]\n",
      " [9.96536255e-01 3.46371601e-03]\n",
      " [9.95628119e-01 4.37190896e-03]\n",
      " [9.96539474e-01 3.46049317e-03]\n",
      " [9.94583666e-01 5.41637046e-03]\n",
      " [9.95288014e-01 4.71202284e-03]\n",
      " [9.93066669e-01 6.93326397e-03]\n",
      " [9.94301319e-01 5.69862360e-03]\n",
      " [9.93068457e-01 6.93159038e-03]\n",
      " [9.90886688e-01 9.11334716e-03]\n",
      " [9.84940231e-01 1.50597282e-02]\n",
      " [4.53943526e-03 9.95460570e-01]\n",
      " [3.32048745e-03 9.96679544e-01]\n",
      " [1.79451087e-03 9.98205423e-01]\n",
      " [1.66068273e-03 9.98339295e-01]\n",
      " [1.61867053e-03 9.98381376e-01]\n",
      " [3.97303794e-03 9.96026993e-01]\n",
      " [9.96354461e-01 3.64558864e-03]\n",
      " [9.96614158e-01 3.38584813e-03]\n",
      " [9.97062385e-01 2.93757766e-03]\n",
      " [9.96394694e-01 3.60529660e-03]\n",
      " [9.96846735e-01 3.15323984e-03]\n",
      " [9.97039199e-01 2.96085910e-03]\n",
      " [9.95295942e-01 4.70401207e-03]\n",
      " [9.95184600e-01 4.81536286e-03]\n",
      " [9.90437567e-01 9.56245977e-03]\n",
      " [9.95193899e-01 4.80611762e-03]\n",
      " [9.95652556e-01 4.34739375e-03]\n",
      " [9.93201792e-01 6.79827202e-03]\n",
      " [9.94829595e-01 5.17044822e-03]\n",
      " [9.90353167e-01 9.64689068e-03]\n",
      " [9.90774214e-01 9.22572054e-03]\n",
      " [9.96920466e-01 3.07953451e-03]\n",
      " [9.96603727e-01 3.39630689e-03]\n",
      " [9.95886147e-01 4.11383249e-03]\n",
      " [9.96430218e-01 3.56984255e-03]\n",
      " [9.88143623e-01 1.18563883e-02]\n",
      " [9.85097647e-01 1.49023235e-02]\n",
      " [9.90721703e-01 9.27835610e-03]\n",
      " [9.94172513e-01 5.82748558e-03]\n",
      " [9.91527379e-01 8.47264752e-03]\n",
      " [3.27444337e-02 9.67255592e-01]\n",
      " [1.01989498e-02 9.89800990e-01]\n",
      " [2.53445259e-03 9.97465491e-01]\n",
      " [2.65820837e-03 9.97341812e-01]\n",
      " [2.03022850e-03 9.97969806e-01]\n",
      " [1.31624867e-03 9.98683751e-01]\n",
      " [1.36963453e-03 9.98630404e-01]\n",
      " [2.71400157e-03 9.97285962e-01]\n",
      " [6.86239591e-03 9.93137598e-01]\n",
      " [1.14916847e-03 9.98850822e-01]\n",
      " [2.57957121e-03 9.97420430e-01]\n",
      " [1.27403985e-03 9.98725951e-01]\n",
      " [2.01005791e-03 9.97989893e-01]\n",
      " [2.26362376e-03 9.97736335e-01]\n",
      " [3.43115209e-03 9.96568799e-01]\n",
      " [3.36594670e-03 9.96634066e-01]\n",
      " [9.03728187e-01 9.62718949e-02]\n",
      " [9.74647164e-01 2.53528617e-02]\n",
      " [9.86099780e-01 1.39001738e-02]\n",
      " [9.90732551e-01 9.26739909e-03]\n",
      " [9.86917675e-01 1.30823152e-02]\n",
      " [9.88504708e-01 1.14952736e-02]\n",
      " [9.85422492e-01 1.45775294e-02]\n",
      " [9.59209025e-01 4.07910347e-02]\n",
      " [9.92089391e-01 7.91064464e-03]\n",
      " [9.93125439e-01 6.87453803e-03]\n",
      " [9.95860279e-01 4.13968600e-03]\n",
      " [9.96010900e-01 3.98908276e-03]\n",
      " [9.97397900e-01 2.60214554e-03]\n",
      " [9.97081101e-01 2.91883736e-03]\n",
      " [9.89418209e-01 1.05818156e-02]\n",
      " [9.79914069e-01 2.00859364e-02]\n",
      " [9.26882684e-01 7.31172934e-02]\n",
      " [6.74163938e-01 3.25836062e-01]\n",
      " [3.03348270e-03 9.96966541e-01]\n",
      " [1.94400852e-03 9.98056054e-01]\n",
      " [1.94367371e-03 9.98056412e-01]\n",
      " [8.39530607e-04 9.99160528e-01]\n",
      " [3.35616339e-03 9.96643901e-01]\n",
      " [2.06503551e-03 9.97934937e-01]\n",
      " [2.16317596e-03 9.97836888e-01]\n",
      " [3.08017619e-03 9.96919870e-01]\n",
      " [2.85072019e-03 9.97149289e-01]\n",
      " [2.80727376e-03 9.97192800e-01]\n",
      " [9.38323677e-01 6.16763197e-02]\n",
      " [9.95946229e-01 4.05373145e-03]\n",
      " [9.85447407e-01 1.45526379e-02]\n",
      " [9.81742561e-01 1.82574987e-02]\n",
      " [8.42827320e-01 1.57172710e-01]\n",
      " [9.75588679e-01 2.44113654e-02]\n",
      " [8.92458558e-01 1.07541434e-01]\n",
      " [9.87427831e-01 1.25721972e-02]\n",
      " [9.93535876e-01 6.46418426e-03]\n",
      " [9.90717590e-01 9.28243715e-03]\n",
      " [9.83912885e-01 1.60870999e-02]\n",
      " [9.34239149e-01 6.57608137e-02]\n",
      " [2.95889005e-03 9.97041047e-01]\n",
      " [2.51757144e-03 9.97482479e-01]\n",
      " [2.07850547e-03 9.97921526e-01]\n",
      " [3.01000709e-03 9.96989965e-01]\n",
      " [1.50728961e-02 9.84927058e-01]\n",
      " [9.75628614e-01 2.43713316e-02]\n",
      " [9.98089492e-01 1.91045948e-03]\n",
      " [9.97288346e-01 2.71163182e-03]\n",
      " [9.95885670e-01 4.11434472e-03]\n",
      " [9.95434344e-01 4.56560357e-03]\n",
      " [9.90441084e-01 9.55896452e-03]\n",
      " [9.93091345e-01 6.90865796e-03]\n",
      " [9.88217175e-01 1.17828278e-02]\n",
      " [9.85150635e-01 1.48493629e-02]\n",
      " [1.23281656e-02 9.87671852e-01]\n",
      " [9.94850099e-01 5.14992001e-03]\n",
      " [9.96292412e-01 3.70757328e-03]\n",
      " [9.91766274e-01 8.23375396e-03]\n",
      " [9.97918546e-01 2.08149850e-03]\n",
      " [9.96587515e-01 3.41247022e-03]\n",
      " [9.93313134e-01 6.68690586e-03]\n",
      " [9.95467782e-01 4.53226129e-03]\n",
      " [9.95118260e-01 4.88173449e-03]\n",
      " [9.95711565e-01 4.28839074e-03]\n",
      " [9.95019436e-01 4.98058647e-03]\n",
      " [9.95147884e-01 4.85215569e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc13bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decision(test,pred):\n",
    "    h = np.array(pred)\n",
    "    action = []\n",
    "    status = \"N\"\n",
    "    for i in range(len(h)):\n",
    "        if h[i][0] == max(h[i]):\n",
    "            h[i] = [1,0]\n",
    "            if status == \"N\":\n",
    "                action.append(\"Buy\")\n",
    "                status = \"Buy\"\n",
    "            else:\n",
    "                action.append(\"Hold\")\n",
    "        else:\n",
    "            h[i] = [0,1]\n",
    "            if status == \"Buy\":\n",
    "                action.append(\"Sell\")\n",
    "                status = \"N\"\n",
    "            else:\n",
    "                action.append(\"Hold\")\n",
    "                \n",
    "    backtest = test[[\"open\"]][10:]\n",
    "    backtest.columns = [\"Open\"]\n",
    "    return (backtest,pd.DataFrame(action,index=test[10:].index,columns=[\"Action\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "420df5e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (178, 1), indices imply (495, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (178, 1), indices imply (495, 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_23895/2870724268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbacktestdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_decision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfinal_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_23895/1062739970.py\u001b[0m in \u001b[0;36mconvert_decision\u001b[0;34m(test, pred)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbacktest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"open\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbacktest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Open\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                     \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1685\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (178, 1), indices imply (495, 1)"
     ]
    }
   ],
   "source": [
    "backtestdata,final_pred = convert_decision(test_X,predictions)\n",
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8975744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20190116</th>\n",
       "      <td>37.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190117</th>\n",
       "      <td>37.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190118</th>\n",
       "      <td>38.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190122</th>\n",
       "      <td>38.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190123</th>\n",
       "      <td>37.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201224</th>\n",
       "      <td>130.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201228</th>\n",
       "      <td>133.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201229</th>\n",
       "      <td>137.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201230</th>\n",
       "      <td>134.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201231</th>\n",
       "      <td>133.450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open\n",
       "DATE             \n",
       "20190116   37.209\n",
       "20190117   37.477\n",
       "20190118   38.280\n",
       "20190122   38.013\n",
       "20190123   37.464\n",
       "...           ...\n",
       "20201224  130.700\n",
       "20201228  133.360\n",
       "20201229  137.400\n",
       "20201230  134.910\n",
       "20201231  133.450\n",
       "\n",
       "[495 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4ef890",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3faf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Input ##########################\n",
    "# For hist_price_data: index=[\"date\"], columns = [\"Open\"]\n",
    "# For pred_action: index=[\"date\"], columns = [\"Action\"] (Buy/Sell)\n",
    "################### Output #########################\n",
    "# 1. trading record\n",
    "# 2. total profit\n",
    "class backtest:\n",
    "    hpd = \"\"\n",
    "    pred_action=\"\"\n",
    "    trade_record=pd.DataFrame(index=[],\n",
    "                              columns=[\"Action\",\"Price\",\"Position\",\"Cash\",\"Pos_Bal\",\"Cash_Bal\",\"Cum_Profit\"],\n",
    "                             )\n",
    "    capital = 0\n",
    "    cash_balance = 0\n",
    "    profit = 0\n",
    "    handle_fee = 0\n",
    "    position = 0\n",
    "    last_price = 0\n",
    "    \n",
    "    def __init__(self,hist_price_data,pred_action,capital,handling_fee):\n",
    "        self.hpd = hist_price_data\n",
    "        self.pred_action = pred_action\n",
    "        self.capital = capital\n",
    "        self.cash_balance = capital\n",
    "        self.handle_fee = handling_fee\n",
    "        trade_record=pd.DataFrame(index=[],\n",
    "                                  columns=[\"Action\",\"Price\",\"Position\",\"Cash\",\"Pos_Bal\",\"Cash_Bal\",\"Cum_Profit\"],\n",
    "                                 )\n",
    "        \n",
    "    def start_test(self):        \n",
    "        # For loop to iterate the data\n",
    "        for ind in self.pred_action.index:\n",
    "            # Update latest price\n",
    "            self.last_price = self.hpd.loc[ind,\"Open\"]\n",
    "            \n",
    "            if self.pred_action.loc[ind,\"Action\"].lower() == \"buy\":\n",
    "                self.buy(ind,self.hpd.loc[ind,\"Open\"])\n",
    "            elif self.pred_action.loc[ind,\"Action\"].lower() == \"sell\":\n",
    "                self.sell(ind,self.hpd.loc[ind,\"Open\"])\n",
    "            else:\n",
    "                print(\"Did not buy at \" + str(ind))\n",
    "            \n",
    "                \n",
    "        \n",
    "    def mark_down_record(self,date,action,price,pos_delta,cash_delta):\n",
    "        self.trade_record.loc[date,\"Action\"] = action\n",
    "        self.trade_record.loc[date,\"Price\"] = price\n",
    "        self.trade_record.loc[date,\"Position\"] = pos_delta\n",
    "        self.trade_record.loc[date,\"Cash\"] = cash_delta\n",
    "        \n",
    "        self.trade_record.loc[date,\"Pos_Bal\"] = round(self.position,4)\n",
    "        self.trade_record.loc[date,\"Cash_Bal\"] = round(self.cash_balance,3)\n",
    "        self.trade_record.loc[date,\"Cum_Profit\"] = round(self.get_profit(),3)\n",
    "        \n",
    "    def buy(self,date,price):\n",
    "        # Assume use all money to buy all\n",
    "        buy_flag = False\n",
    "        buy_pos = floor(self.cash_balance / price)\n",
    "        for i in range(buy_pos):\n",
    "            act_buy_pos = buy_pos - i\n",
    "            total_amt = act_buy_pos*price*(1+self.handle_fee)\n",
    "            if self.cash_balance > total_amt:\n",
    "                self.position += act_buy_pos\n",
    "                self.cash_balance -= total_amt\n",
    "                self.mark_down_record(date,\n",
    "                                 \"Buy\",\n",
    "                                 price,\n",
    "                                 act_buy_pos,\n",
    "                                 -total_amt)\n",
    "                print(\"Bought at\",date,\"with price =\", price)\n",
    "                buy_flag = True\n",
    "                break\n",
    "        if not buy_flag:\n",
    "            print(\"You do not have enough money to buy!\")\n",
    "    \n",
    "    def sell(self,date,price):\n",
    "        # Assume sell all position\n",
    "        sell_pos = self.position\n",
    "        total_amt = sell_pos*price*(1-self.handle_fee)\n",
    "        if self.position >= 1:\n",
    "            self.position -= sell_pos\n",
    "            self.cash_balance += total_amt\n",
    "            self.mark_down_record(date,\n",
    "                             \"Sell\",\n",
    "                             price,\n",
    "                             -sell_pos,\n",
    "                             total_amt)\n",
    "            print(\"Sold at\",date,\"with price =\", price)\n",
    "        else:\n",
    "            print(\"You do not have enough position to sell!\")\n",
    "    \n",
    "    def get_profit(self):\n",
    "        return self.get_cash_balance()+self.get_last_price()*self.get_position()-self.get_capital()\n",
    "    \n",
    "    def get_capital(self):\n",
    "        return self.capital\n",
    "    \n",
    "    def get_last_price(self):\n",
    "        return self.last_price\n",
    "    \n",
    "    def get_cash_balance(self):\n",
    "        return self.cash_balance\n",
    "    \n",
    "    def get_position(self):\n",
    "        return self.position\n",
    "    \n",
    "    def get_amounnt(self):\n",
    "        return self.capital+self.profit\n",
    "    \n",
    "    def print_trade_record(self):\n",
    "        print(self.trade_record)\n",
    "    \n",
    "    def print_profit(self):\n",
    "        print(self.get_profit())\n",
    "    \n",
    "    def export_trade_record(self,stock):\n",
    "        # Save the trade record to the path\n",
    "        self.trade_record.to_csv(\"trade_record/\"+stock+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc8dc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_1 = backtest(backtestdata,final_pred,10000,0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12273bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bought at 20190116 with price = 37.209\n",
      "Did not buy at 20190117\n",
      "Did not buy at 20190118\n",
      "Did not buy at 20190122\n",
      "Did not buy at 20190123\n",
      "Did not buy at 20190124\n",
      "Did not buy at 20190125\n",
      "Did not buy at 20190128\n",
      "Did not buy at 20190129\n",
      "Did not buy at 20190130\n",
      "Did not buy at 20190131\n",
      "Did not buy at 20190201\n",
      "Did not buy at 20190204\n",
      "Did not buy at 20190205\n",
      "Did not buy at 20190206\n",
      "Did not buy at 20190207\n",
      "Did not buy at 20190208\n",
      "Did not buy at 20190211\n",
      "Did not buy at 20190212\n",
      "Did not buy at 20190213\n",
      "Did not buy at 20190214\n",
      "Did not buy at 20190215\n",
      "Did not buy at 20190219\n",
      "Did not buy at 20190220\n",
      "Did not buy at 20190221\n",
      "Did not buy at 20190222\n",
      "Did not buy at 20190225\n",
      "Did not buy at 20190226\n",
      "Did not buy at 20190227\n",
      "Did not buy at 20190228\n",
      "Did not buy at 20190301\n",
      "Did not buy at 20190304\n",
      "Did not buy at 20190305\n",
      "Did not buy at 20190306\n",
      "Did not buy at 20190307\n",
      "Did not buy at 20190308\n",
      "Sold at 20190311 with price = 42.837\n",
      "Bought at 20190312 with price = 43.937\n",
      "Did not buy at 20190313\n",
      "Did not buy at 20190314\n",
      "Did not buy at 20190315\n",
      "Did not buy at 20190318\n",
      "Did not buy at 20190319\n",
      "Did not buy at 20190320\n",
      "Did not buy at 20190321\n",
      "Did not buy at 20190322\n",
      "Did not buy at 20190325\n",
      "Did not buy at 20190326\n",
      "Did not buy at 20190327\n",
      "Did not buy at 20190328\n",
      "Did not buy at 20190329\n",
      "Did not buy at 20190401\n",
      "Did not buy at 20190402\n",
      "Did not buy at 20190403\n",
      "Did not buy at 20190404\n",
      "Did not buy at 20190405\n",
      "Did not buy at 20190408\n",
      "Did not buy at 20190409\n",
      "Did not buy at 20190410\n",
      "Did not buy at 20190411\n",
      "Did not buy at 20190412\n",
      "Did not buy at 20190415\n",
      "Did not buy at 20190416\n",
      "Did not buy at 20190417\n",
      "Did not buy at 20190418\n",
      "Did not buy at 20190422\n",
      "Did not buy at 20190423\n",
      "Did not buy at 20190424\n",
      "Did not buy at 20190425\n",
      "Did not buy at 20190426\n",
      "Did not buy at 20190429\n",
      "Did not buy at 20190430\n",
      "Sold at 20190501 with price = 51.23\n",
      "Bought at 20190502 with price = 51.222\n",
      "Did not buy at 20190503\n",
      "Did not buy at 20190506\n",
      "Did not buy at 20190507\n",
      "Did not buy at 20190508\n",
      "Sold at 20190509 with price = 48.914\n",
      "Did not buy at 20190510\n",
      "Did not buy at 20190513\n",
      "Did not buy at 20190514\n",
      "Did not buy at 20190515\n",
      "Did not buy at 20190516\n",
      "Did not buy at 20190517\n",
      "Did not buy at 20190520\n",
      "Did not buy at 20190521\n",
      "Did not buy at 20190522\n",
      "Did not buy at 20190523\n",
      "Did not buy at 20190524\n",
      "Did not buy at 20190528\n",
      "Did not buy at 20190529\n",
      "Did not buy at 20190530\n",
      "Did not buy at 20190531\n",
      "Did not buy at 20190603\n",
      "Did not buy at 20190604\n",
      "Did not buy at 20190605\n",
      "Bought at 20190606 with price = 44.86\n",
      "Did not buy at 20190607\n",
      "Did not buy at 20190610\n",
      "Did not buy at 20190611\n",
      "Did not buy at 20190612\n",
      "Did not buy at 20190613\n",
      "Did not buy at 20190614\n",
      "Did not buy at 20190617\n",
      "Did not buy at 20190618\n",
      "Did not buy at 20190619\n",
      "Did not buy at 20190620\n",
      "Did not buy at 20190621\n",
      "Did not buy at 20190624\n",
      "Did not buy at 20190625\n",
      "Did not buy at 20190626\n",
      "Did not buy at 20190627\n",
      "Did not buy at 20190628\n",
      "Did not buy at 20190701\n",
      "Did not buy at 20190702\n",
      "Did not buy at 20190703\n",
      "Did not buy at 20190705\n",
      "Did not buy at 20190708\n",
      "Did not buy at 20190709\n",
      "Did not buy at 20190710\n",
      "Did not buy at 20190711\n",
      "Sold at 20190712 with price = 49.607\n",
      "Bought at 20190715 with price = 50.008\n",
      "Did not buy at 20190716\n",
      "Did not buy at 20190717\n",
      "Did not buy at 20190718\n",
      "Did not buy at 20190719\n",
      "Did not buy at 20190722\n",
      "Did not buy at 20190723\n",
      "Did not buy at 20190724\n",
      "Did not buy at 20190725\n",
      "Did not buy at 20190726\n",
      "Did not buy at 20190729\n",
      "Did not buy at 20190730\n",
      "Did not buy at 20190731\n",
      "Did not buy at 20190801\n",
      "Did not buy at 20190802\n",
      "Sold at 20190805 with price = 48.512\n",
      "Did not buy at 20190806\n",
      "Did not buy at 20190807\n",
      "Did not buy at 20190808\n",
      "Did not buy at 20190809\n",
      "Did not buy at 20190812\n",
      "Did not buy at 20190813\n",
      "Bought at 20190814 with price = 49.968\n",
      "Did not buy at 20190815\n",
      "Did not buy at 20190816\n",
      "Did not buy at 20190819\n",
      "Did not buy at 20190820\n",
      "Did not buy at 20190821\n",
      "Did not buy at 20190822\n",
      "Did not buy at 20190823\n",
      "Did not buy at 20190826\n",
      "Sold at 20190827 with price = 51.125\n",
      "Did not buy at 20190828\n",
      "Did not buy at 20190829\n",
      "Did not buy at 20190830\n",
      "Did not buy at 20190903\n",
      "Did not buy at 20190904\n",
      "Did not buy at 20190905\n",
      "Bought at 20190906 with price = 52.649\n",
      "Did not buy at 20190909\n",
      "Did not buy at 20190910\n",
      "Did not buy at 20190911\n",
      "Did not buy at 20190912\n",
      "Did not buy at 20190913\n",
      "Did not buy at 20190916\n",
      "Did not buy at 20190917\n",
      "Did not buy at 20190918\n",
      "Did not buy at 20190919\n",
      "Did not buy at 20190920\n",
      "Did not buy at 20190923\n",
      "Sold at 20190924 with price = 54.369\n",
      "Did not buy at 20190925\n",
      "Did not buy at 20190926\n",
      "Did not buy at 20190927\n",
      "Did not buy at 20190930\n",
      "Bought at 20191001 with price = 55.362\n",
      "Did not buy at 20191002\n",
      "Did not buy at 20191003\n",
      "Did not buy at 20191004\n",
      "Did not buy at 20191007\n",
      "Did not buy at 20191008\n",
      "Did not buy at 20191009\n",
      "Did not buy at 20191010\n",
      "Did not buy at 20191011\n",
      "Did not buy at 20191014\n",
      "Did not buy at 20191015\n",
      "Did not buy at 20191016\n",
      "Did not buy at 20191017\n",
      "Did not buy at 20191018\n",
      "Did not buy at 20191021\n",
      "Did not buy at 20191022\n",
      "Did not buy at 20191023\n",
      "Did not buy at 20191024\n",
      "Did not buy at 20191025\n",
      "Did not buy at 20191028\n",
      "Did not buy at 20191029\n",
      "Did not buy at 20191030\n",
      "Did not buy at 20191031\n",
      "Did not buy at 20191101\n",
      "Did not buy at 20191104\n",
      "Did not buy at 20191105\n",
      "Did not buy at 20191106\n",
      "Did not buy at 20191107\n",
      "Did not buy at 20191108\n",
      "Did not buy at 20191111\n",
      "Did not buy at 20191112\n",
      "Did not buy at 20191113\n",
      "Did not buy at 20191114\n",
      "Did not buy at 20191115\n",
      "Did not buy at 20191118\n",
      "Did not buy at 20191119\n",
      "Did not buy at 20191120\n",
      "Did not buy at 20191121\n",
      "Did not buy at 20191122\n",
      "Sold at 20191125 with price = 64.812\n",
      "Did not buy at 20191126\n",
      "Did not buy at 20191127\n",
      "Bought at 20191129 with price = 65.768\n",
      "Did not buy at 20191202\n",
      "Did not buy at 20191203\n",
      "Sold at 20191204 with price = 64.405\n",
      "Did not buy at 20191205\n",
      "Did not buy at 20191206\n",
      "Bought at 20191209 with price = 66.61\n",
      "Did not buy at 20191210\n",
      "Did not buy at 20191211\n",
      "Did not buy at 20191212\n",
      "Did not buy at 20191213\n",
      "Did not buy at 20191216\n",
      "Did not buy at 20191217\n",
      "Did not buy at 20191218\n",
      "Did not buy at 20191219\n",
      "Did not buy at 20191220\n",
      "Did not buy at 20191223\n",
      "Did not buy at 20191224\n",
      "Did not buy at 20191226\n",
      "Did not buy at 20191227\n",
      "Did not buy at 20191230\n",
      "Did not buy at 20191231\n",
      "Did not buy at 20200102\n",
      "Did not buy at 20200103\n",
      "Did not buy at 20200106\n",
      "Did not buy at 20200107\n",
      "Did not buy at 20200108\n",
      "Did not buy at 20200109\n",
      "Did not buy at 20200110\n",
      "Did not buy at 20200113\n",
      "Did not buy at 20200114\n",
      "Did not buy at 20200115\n",
      "Did not buy at 20200116\n",
      "Did not buy at 20200117\n",
      "Did not buy at 20200121\n",
      "Did not buy at 20200122\n",
      "Did not buy at 20200123\n",
      "Did not buy at 20200124\n",
      "Did not buy at 20200127\n",
      "Sold at 20200128 with price = 77.118\n",
      "Did not buy at 20200129\n",
      "Bought at 20200130 with price = 79.081\n",
      "Did not buy at 20200131\n",
      "Did not buy at 20200203\n",
      "Did not buy at 20200204\n",
      "Sold at 20200205 with price = 79.811\n",
      "Did not buy at 20200206\n",
      "Bought at 20200207 with price = 79.717\n",
      "Did not buy at 20200210\n",
      "Did not buy at 20200211\n",
      "Did not buy at 20200212\n",
      "Did not buy at 20200213\n",
      "Did not buy at 20200214\n",
      "Did not buy at 20200218\n",
      "Did not buy at 20200219\n",
      "Sold at 20200220 with price = 79.781\n",
      "Did not buy at 20200221\n",
      "Did not buy at 20200224\n",
      "Did not buy at 20200225\n",
      "Did not buy at 20200226\n",
      "Did not buy at 20200227\n",
      "Did not buy at 20200228\n",
      "Did not buy at 20200302\n",
      "Did not buy at 20200303\n",
      "Did not buy at 20200304\n",
      "Bought at 20200305 with price = 73.078\n",
      "Sold at 20200306 with price = 69.733\n",
      "Bought at 20200309 with price = 65.222\n",
      "Sold at 20200310 with price = 68.53\n",
      "Did not buy at 20200311\n",
      "Did not buy at 20200312\n",
      "Did not buy at 20200313\n",
      "Did not buy at 20200316\n",
      "Did not buy at 20200317\n",
      "Did not buy at 20200318\n",
      "Did not buy at 20200319\n",
      "Did not buy at 20200320\n",
      "Did not buy at 20200323\n",
      "Did not buy at 20200324\n",
      "Did not buy at 20200325\n",
      "Did not buy at 20200326\n",
      "Did not buy at 20200327\n",
      "Bought at 20200330 with price = 62.004\n",
      "Did not buy at 20200331\n",
      "Did not buy at 20200401\n",
      "Did not buy at 20200402\n",
      "Did not buy at 20200403\n",
      "Sold at 20200406 with price = 62.042\n",
      "Bought at 20200407 with price = 66.965\n",
      "Did not buy at 20200408\n",
      "Did not buy at 20200409\n",
      "Did not buy at 20200413\n",
      "Did not buy at 20200414\n",
      "Did not buy at 20200415\n",
      "Did not buy at 20200416\n",
      "Did not buy at 20200417\n",
      "Did not buy at 20200420\n",
      "Did not buy at 20200421\n",
      "Did not buy at 20200422\n",
      "Sold at 20200423 with price = 68.22\n",
      "Bought at 20200424 with price = 68.546\n",
      "Sold at 20200427 with price = 69.684\n",
      "Bought at 20200428 with price = 70.496\n",
      "Did not buy at 20200429\n",
      "Did not buy at 20200430\n",
      "Did not buy at 20200501\n",
      "Did not buy at 20200504\n",
      "Did not buy at 20200505\n",
      "Did not buy at 20200506\n",
      "Did not buy at 20200507\n",
      "Did not buy at 20200508\n",
      "Did not buy at 20200511\n",
      "Did not buy at 20200512\n",
      "Did not buy at 20200513\n",
      "Did not buy at 20200514\n",
      "Did not buy at 20200515\n",
      "Did not buy at 20200518\n",
      "Did not buy at 20200519\n",
      "Did not buy at 20200520\n",
      "Did not buy at 20200521\n",
      "Did not buy at 20200522\n",
      "Did not buy at 20200526\n",
      "Did not buy at 20200527\n",
      "Did not buy at 20200528\n",
      "Did not buy at 20200529\n",
      "Did not buy at 20200601\n",
      "Did not buy at 20200602\n",
      "Did not buy at 20200603\n",
      "Did not buy at 20200604\n",
      "Did not buy at 20200605\n",
      "Did not buy at 20200608\n",
      "Did not buy at 20200609\n",
      "Did not buy at 20200610\n",
      "Did not buy at 20200611\n",
      "Did not buy at 20200612\n",
      "Did not buy at 20200615\n",
      "Did not buy at 20200616\n",
      "Did not buy at 20200617\n",
      "Did not buy at 20200618\n",
      "Did not buy at 20200619\n",
      "Did not buy at 20200622\n",
      "Did not buy at 20200623\n",
      "Did not buy at 20200624\n",
      "Did not buy at 20200625\n",
      "Did not buy at 20200626\n",
      "Did not buy at 20200629\n",
      "Did not buy at 20200630\n",
      "Did not buy at 20200701\n",
      "Did not buy at 20200702\n",
      "Did not buy at 20200706\n",
      "Did not buy at 20200707\n",
      "Did not buy at 20200708\n",
      "Did not buy at 20200709\n",
      "Did not buy at 20200710\n",
      "Did not buy at 20200713\n",
      "Did not buy at 20200714\n",
      "Did not buy at 20200715\n",
      "Did not buy at 20200716\n",
      "Did not buy at 20200717\n",
      "Did not buy at 20200720\n",
      "Did not buy at 20200721\n",
      "Did not buy at 20200722\n",
      "Did not buy at 20200723\n",
      "Sold at 20200724 with price = 90.242\n",
      "Did not buy at 20200727\n",
      "Did not buy at 20200728\n",
      "Did not buy at 20200729\n",
      "Did not buy at 20200730\n",
      "Did not buy at 20200731\n",
      "Bought at 20200803 with price = 107.31\n",
      "Did not buy at 20200804\n",
      "Did not buy at 20200805\n",
      "Did not buy at 20200806\n",
      "Did not buy at 20200807\n",
      "Did not buy at 20200810\n",
      "Did not buy at 20200811\n",
      "Did not buy at 20200812\n",
      "Did not buy at 20200813\n",
      "Did not buy at 20200814\n",
      "Did not buy at 20200817\n",
      "Did not buy at 20200818\n",
      "Did not buy at 20200819\n",
      "Did not buy at 20200820\n",
      "Did not buy at 20200821\n",
      "Did not buy at 20200824\n",
      "Did not buy at 20200825\n",
      "Did not buy at 20200826\n",
      "Did not buy at 20200827\n",
      "Did not buy at 20200828\n",
      "Did not buy at 20200831\n",
      "Did not buy at 20200901\n",
      "Did not buy at 20200902\n",
      "Did not buy at 20200903\n",
      "Sold at 20200904 with price = 119.3\n",
      "Did not buy at 20200908\n",
      "Did not buy at 20200909\n",
      "Did not buy at 20200910\n",
      "Did not buy at 20200911\n",
      "Did not buy at 20200914\n",
      "Did not buy at 20200915\n",
      "Did not buy at 20200916\n",
      "Did not buy at 20200917\n",
      "Did not buy at 20200918\n",
      "Did not buy at 20200921\n",
      "Did not buy at 20200922\n",
      "Did not buy at 20200923\n",
      "Did not buy at 20200924\n",
      "Did not buy at 20200925\n",
      "Did not buy at 20200928\n",
      "Bought at 20200929 with price = 113.82\n",
      "Did not buy at 20200930\n",
      "Did not buy at 20201001\n",
      "Did not buy at 20201002\n",
      "Did not buy at 20201005\n",
      "Did not buy at 20201006\n",
      "Did not buy at 20201007\n",
      "Did not buy at 20201008\n",
      "Did not buy at 20201009\n",
      "Did not buy at 20201012\n",
      "Did not buy at 20201013\n",
      "Did not buy at 20201014\n",
      "Did not buy at 20201015\n",
      "Did not buy at 20201016\n",
      "Did not buy at 20201019\n",
      "Did not buy at 20201020\n",
      "Did not buy at 20201021\n",
      "Did not buy at 20201022\n",
      "Sold at 20201023 with price = 115.65\n",
      "Did not buy at 20201026\n",
      "Did not buy at 20201027\n",
      "Did not buy at 20201028\n",
      "Did not buy at 20201029\n",
      "Did not buy at 20201030\n",
      "Did not buy at 20201102\n",
      "Did not buy at 20201103\n",
      "Did not buy at 20201104\n",
      "Did not buy at 20201105\n",
      "Bought at 20201106 with price = 117.76\n",
      "Did not buy at 20201109\n",
      "Did not buy at 20201110\n",
      "Did not buy at 20201111\n",
      "Did not buy at 20201112\n",
      "Did not buy at 20201113\n",
      "Did not buy at 20201116\n",
      "Did not buy at 20201117\n",
      "Did not buy at 20201118\n",
      "Did not buy at 20201119\n",
      "Did not buy at 20201120\n",
      "Did not buy at 20201123\n",
      "Sold at 20201124 with price = 113.38\n",
      "Did not buy at 20201125\n",
      "Did not buy at 20201127\n",
      "Did not buy at 20201130\n",
      "Did not buy at 20201201\n",
      "Bought at 20201202 with price = 121.45\n",
      "Did not buy at 20201203\n",
      "Did not buy at 20201204\n",
      "Did not buy at 20201207\n",
      "Did not buy at 20201208\n",
      "Did not buy at 20201209\n",
      "Did not buy at 20201210\n",
      "Did not buy at 20201211\n",
      "Did not buy at 20201214\n",
      "Sold at 20201215 with price = 123.75\n",
      "Bought at 20201216 with price = 126.81\n",
      "Did not buy at 20201217\n",
      "Did not buy at 20201218\n",
      "Did not buy at 20201221\n",
      "Did not buy at 20201222\n",
      "Did not buy at 20201223\n",
      "Did not buy at 20201224\n",
      "Did not buy at 20201228\n",
      "Did not buy at 20201229\n",
      "Did not buy at 20201230\n",
      "Did not buy at 20201231\n"
     ]
    }
   ],
   "source": [
    "backtest_1.start_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e252944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Action   Price Position          Cash Pos_Bal   Cash_Bal Cum_Profit\n",
      "20190116    Buy  37.209      268  -9991.956024     268      8.044    -19.944\n",
      "20190311   Sell  42.837     -268  11457.355368       0  11465.399   1465.399\n",
      "20190312    Buy  43.937      260  -11446.46724     260     18.932   1442.552\n",
      "20190501   Sell   51.23     -260    13293.1604       0  13312.093   3312.093\n",
      "20190502    Buy  51.222      259 -13293.030996     259     19.062    3285.56\n",
      "20190509   Sell  48.914     -259  12643.388548       0   12662.45    2662.45\n",
      "20190606    Buy   44.86      281  -12630.87132     281     31.579   2637.239\n",
      "20190712   Sell  49.607     -281  13911.687866       0  13943.267   3943.267\n",
      "20190715    Buy  50.008      278 -13930.028448     278     13.238   3915.462\n",
      "20190805   Sell  48.512     -278  13459.363328       0  13472.601   3472.601\n",
      "20190814    Buy  49.968      269 -13468.274784     269      4.327   3445.719\n",
      "20190827   Sell  51.125     -269   13725.11975       0  13729.446   3729.446\n",
      "20190906    Buy  52.649      260  -13716.11748     260     13.329   3702.069\n",
      "20190924   Sell  54.369     -260   14107.66812       0  14120.997   4120.997\n",
      "20191001    Buy  55.362      254 -14090.071896     254     30.925   4092.873\n",
      "20191125   Sell  64.812     -254  16429.323504       0  16460.249   6460.249\n",
      "20191129    Buy  65.768      249 -16408.984464     249     51.264   6427.496\n",
      "20191204   Sell  64.405     -249   16004.77131       0  16056.036   6056.036\n",
      "20191209    Buy   66.61      240   -16018.3728     240     37.663   6024.063\n",
      "20200128   Sell  77.118     -240   18471.30336       0  18508.966   8508.966\n",
      "20200130    Buy  79.081      233 -18462.724746     233     46.241   8472.114\n",
      "20200205   Sell  79.811     -233  18558.771074       0  18605.012   8605.012\n",
      "20200207    Buy  79.717      232 -18531.332688     232      73.68   8568.024\n",
      "20200220   Sell  79.781     -232  18472.173616       0  18545.853   8545.853\n",
      "20200305    Buy  73.078      253 -18525.711468     253     20.142   8508.876\n",
      "20200306   Sell  69.733     -253  17607.164102       0  17627.306   7627.306\n",
      "20200309    Buy  65.222      269 -17579.807436     269     47.499   7592.217\n",
      "20200310   Sell   68.53     -269   18397.70086       0  18445.199   8445.199\n",
      "20200330    Buy  62.004      296 -18389.890368     296     55.309   8408.493\n",
      "20200406   Sell  62.042     -296  18327.703136       0  18383.012   8383.012\n",
      "20200407    Buy  66.965      273  -18318.00789     273     65.004   8346.449\n",
      "20200423   Sell   68.22     -273   18586.81188       0  18651.816   8651.816\n",
      "20200424    Buy  68.546      271 -18613.117932     271     38.698   8614.664\n",
      "20200427   Sell  69.684     -271  18846.595272       0  18885.294   8885.294\n",
      "20200428    Buy  70.496      267 -18860.076864     267     25.217   8847.649\n",
      "20200724   Sell  90.242     -267  24046.424772       0  24071.641  14071.641\n",
      "20200803    Buy  107.31      223  -23977.99026     223     93.651  14023.781\n",
      "20200904   Sell   119.3     -223    26550.6922       0  26644.343  16644.343\n",
      "20200929    Buy  113.82      233  -26573.10012     233     71.243  16591.303\n",
      "20201023   Sell  115.65     -233    26892.5571       0    26963.8    16963.8\n",
      "20201106    Buy  117.76      228  -26902.97856     228     60.822  16910.102\n",
      "20201124   Sell  113.38     -228   25798.93872       0  25859.761  15859.761\n",
      "20201202    Buy  121.45      212   -25798.8948     212     60.866  15808.266\n",
      "20201215   Sell  123.75     -212      26182.53       0  26243.396  16243.396\n",
      "20201216    Buy  126.81      206  -26175.10572     206      68.29   16191.15\n",
      "==========================\n",
      "17558.989981999996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "backtest_1.print_trade_record()\n",
    "print('==========================')\n",
    "backtest_1.export_trade_record(\"AAPL\")\n",
    "backtest_1.print_profit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "455084d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bought at 20200116 with price = 77.364\n",
      "Did not buy at 20200117\n",
      "Did not buy at 20200121\n",
      "Did not buy at 20200122\n",
      "Did not buy at 20200123\n",
      "Did not buy at 20200124\n",
      "Did not buy at 20200127\n",
      "Sold at 20200128 with price = 77.118\n",
      "Did not buy at 20200129\n",
      "Bought at 20200130 with price = 79.081\n",
      "Did not buy at 20200131\n",
      "Did not buy at 20200203\n",
      "Did not buy at 20200204\n",
      "Sold at 20200205 with price = 79.811\n",
      "Did not buy at 20200206\n",
      "Bought at 20200207 with price = 79.717\n",
      "Did not buy at 20200210\n",
      "Did not buy at 20200211\n",
      "Did not buy at 20200212\n",
      "Did not buy at 20200213\n",
      "Did not buy at 20200214\n",
      "Did not buy at 20200218\n",
      "Did not buy at 20200219\n",
      "Sold at 20200220 with price = 79.781\n",
      "Did not buy at 20200221\n",
      "Did not buy at 20200224\n",
      "Did not buy at 20200225\n",
      "Did not buy at 20200226\n",
      "Did not buy at 20200227\n",
      "Did not buy at 20200228\n",
      "Did not buy at 20200302\n",
      "Did not buy at 20200303\n",
      "Did not buy at 20200304\n",
      "Bought at 20200305 with price = 73.078\n",
      "Sold at 20200306 with price = 69.733\n",
      "Bought at 20200309 with price = 65.222\n",
      "Sold at 20200310 with price = 68.53\n",
      "Did not buy at 20200311\n",
      "Did not buy at 20200312\n",
      "Did not buy at 20200313\n",
      "Did not buy at 20200316\n",
      "Did not buy at 20200317\n",
      "Did not buy at 20200318\n",
      "Did not buy at 20200319\n",
      "Did not buy at 20200320\n",
      "Did not buy at 20200323\n",
      "Did not buy at 20200324\n",
      "Did not buy at 20200325\n",
      "Did not buy at 20200326\n",
      "Did not buy at 20200327\n",
      "Bought at 20200330 with price = 62.004\n",
      "Did not buy at 20200331\n",
      "Did not buy at 20200401\n",
      "Did not buy at 20200402\n",
      "Did not buy at 20200403\n",
      "Sold at 20200406 with price = 62.042\n",
      "Bought at 20200407 with price = 66.965\n",
      "Did not buy at 20200408\n",
      "Did not buy at 20200409\n",
      "Did not buy at 20200413\n",
      "Did not buy at 20200414\n",
      "Did not buy at 20200415\n",
      "Did not buy at 20200416\n",
      "Did not buy at 20200417\n",
      "Did not buy at 20200420\n",
      "Did not buy at 20200421\n",
      "Did not buy at 20200422\n",
      "Sold at 20200423 with price = 68.22\n",
      "Bought at 20200424 with price = 68.546\n",
      "Sold at 20200427 with price = 69.684\n",
      "Bought at 20200428 with price = 70.496\n",
      "Did not buy at 20200429\n",
      "Did not buy at 20200430\n",
      "Did not buy at 20200501\n",
      "Did not buy at 20200504\n",
      "Did not buy at 20200505\n",
      "Did not buy at 20200506\n",
      "Did not buy at 20200507\n",
      "Did not buy at 20200508\n",
      "Did not buy at 20200511\n",
      "Did not buy at 20200512\n",
      "Did not buy at 20200513\n",
      "Did not buy at 20200514\n",
      "Did not buy at 20200515\n",
      "Did not buy at 20200518\n",
      "Did not buy at 20200519\n",
      "Did not buy at 20200520\n",
      "Did not buy at 20200521\n",
      "Did not buy at 20200522\n",
      "Did not buy at 20200526\n",
      "Did not buy at 20200527\n",
      "Did not buy at 20200528\n",
      "Did not buy at 20200529\n",
      "Did not buy at 20200601\n",
      "Did not buy at 20200602\n",
      "Did not buy at 20200603\n",
      "Did not buy at 20200604\n",
      "Did not buy at 20200605\n",
      "Did not buy at 20200608\n",
      "Did not buy at 20200609\n",
      "Did not buy at 20200610\n",
      "Did not buy at 20200611\n",
      "Did not buy at 20200612\n",
      "Did not buy at 20200615\n",
      "Did not buy at 20200616\n",
      "Did not buy at 20200617\n",
      "Did not buy at 20200618\n",
      "Did not buy at 20200619\n",
      "Did not buy at 20200622\n",
      "Did not buy at 20200623\n",
      "Did not buy at 20200624\n",
      "Did not buy at 20200625\n",
      "Did not buy at 20200626\n",
      "Did not buy at 20200629\n",
      "Did not buy at 20200630\n",
      "Did not buy at 20200701\n",
      "Did not buy at 20200702\n",
      "Did not buy at 20200706\n",
      "Did not buy at 20200707\n",
      "Did not buy at 20200708\n",
      "Did not buy at 20200709\n",
      "Did not buy at 20200710\n",
      "Did not buy at 20200713\n",
      "Did not buy at 20200714\n",
      "Did not buy at 20200715\n",
      "Did not buy at 20200716\n",
      "Did not buy at 20200717\n",
      "Did not buy at 20200720\n",
      "Did not buy at 20200721\n",
      "Did not buy at 20200722\n",
      "Did not buy at 20200723\n",
      "Sold at 20200724 with price = 90.242\n",
      "Did not buy at 20200727\n",
      "Did not buy at 20200728\n",
      "Did not buy at 20200729\n",
      "Did not buy at 20200730\n",
      "Did not buy at 20200731\n",
      "Bought at 20200803 with price = 107.31\n",
      "Did not buy at 20200804\n",
      "Did not buy at 20200805\n",
      "Did not buy at 20200806\n",
      "Did not buy at 20200807\n",
      "Did not buy at 20200810\n",
      "Did not buy at 20200811\n",
      "Did not buy at 20200812\n",
      "Did not buy at 20200813\n",
      "Did not buy at 20200814\n",
      "Did not buy at 20200817\n",
      "Did not buy at 20200818\n",
      "Did not buy at 20200819\n",
      "Did not buy at 20200820\n",
      "Did not buy at 20200821\n",
      "Did not buy at 20200824\n",
      "Did not buy at 20200825\n",
      "Did not buy at 20200826\n",
      "Did not buy at 20200827\n",
      "Did not buy at 20200828\n",
      "Did not buy at 20200831\n",
      "Did not buy at 20200901\n",
      "Did not buy at 20200902\n",
      "Did not buy at 20200903\n",
      "Sold at 20200904 with price = 119.3\n",
      "Did not buy at 20200908\n",
      "Did not buy at 20200909\n",
      "Did not buy at 20200910\n",
      "Did not buy at 20200911\n",
      "Did not buy at 20200914\n",
      "Did not buy at 20200915\n",
      "Did not buy at 20200916\n",
      "Did not buy at 20200917\n",
      "Did not buy at 20200918\n",
      "Did not buy at 20200921\n",
      "Did not buy at 20200922\n",
      "Did not buy at 20200923\n",
      "Did not buy at 20200924\n",
      "Did not buy at 20200925\n",
      "Did not buy at 20200928\n",
      "Bought at 20200929 with price = 113.82\n",
      "Did not buy at 20200930\n",
      "Did not buy at 20201001\n",
      "Did not buy at 20201002\n",
      "Did not buy at 20201005\n",
      "Did not buy at 20201006\n",
      "Did not buy at 20201007\n",
      "Did not buy at 20201008\n",
      "Did not buy at 20201009\n",
      "Did not buy at 20201012\n",
      "Did not buy at 20201013\n",
      "Did not buy at 20201014\n",
      "Did not buy at 20201015\n",
      "Did not buy at 20201016\n",
      "Did not buy at 20201019\n",
      "Did not buy at 20201020\n",
      "Did not buy at 20201021\n",
      "Did not buy at 20201022\n",
      "Sold at 20201023 with price = 115.65\n",
      "Did not buy at 20201026\n",
      "Did not buy at 20201027\n",
      "Did not buy at 20201028\n",
      "Did not buy at 20201029\n",
      "Did not buy at 20201030\n",
      "Did not buy at 20201102\n",
      "Did not buy at 20201103\n",
      "Did not buy at 20201104\n",
      "Did not buy at 20201105\n",
      "Bought at 20201106 with price = 117.76\n",
      "Did not buy at 20201109\n",
      "Did not buy at 20201110\n",
      "Did not buy at 20201111\n",
      "Did not buy at 20201112\n",
      "Did not buy at 20201113\n",
      "Did not buy at 20201116\n",
      "Did not buy at 20201117\n",
      "Did not buy at 20201118\n",
      "Did not buy at 20201119\n",
      "Did not buy at 20201120\n",
      "Did not buy at 20201123\n",
      "Sold at 20201124 with price = 113.38\n",
      "Did not buy at 20201125\n",
      "Did not buy at 20201127\n",
      "Did not buy at 20201130\n",
      "Did not buy at 20201201\n",
      "Bought at 20201202 with price = 121.45\n",
      "Did not buy at 20201203\n",
      "Did not buy at 20201204\n",
      "Did not buy at 20201207\n",
      "Did not buy at 20201208\n",
      "Did not buy at 20201209\n",
      "Did not buy at 20201210\n",
      "Did not buy at 20201211\n",
      "Did not buy at 20201214\n",
      "Sold at 20201215 with price = 123.75\n",
      "Bought at 20201216 with price = 126.81\n",
      "Did not buy at 20201217\n",
      "Did not buy at 20201218\n",
      "Did not buy at 20201221\n",
      "Did not buy at 20201222\n",
      "Did not buy at 20201223\n",
      "Did not buy at 20201224\n",
      "Did not buy at 20201228\n",
      "Did not buy at 20201229\n",
      "Did not buy at 20201230\n",
      "Did not buy at 20201231\n",
      "Did not buy at 20210104\n",
      "Did not buy at 20210105\n",
      "Sold at 20210106 with price = 127.12\n",
      "Did not buy at 20210107\n",
      "Did not buy at 20210108\n",
      "Did not buy at 20210111\n",
      "Did not buy at 20210112\n",
      "Did not buy at 20210113\n",
      "Did not buy at 20210114\n",
      "Did not buy at 20210115\n",
      "Did not buy at 20210119\n",
      "Did not buy at 20210120\n",
      "Bought at 20210121 with price = 133.17\n",
      "Did not buy at 20210122\n",
      "Did not buy at 20210125\n",
      "Did not buy at 20210126\n",
      "Did not buy at 20210127\n",
      "Did not buy at 20210128\n",
      "Did not buy at 20210129\n",
      "Did not buy at 20210201\n",
      "Sold at 20210202 with price = 135.1\n",
      "Did not buy at 20210203\n",
      "Did not buy at 20210204\n",
      "Did not buy at 20210205\n",
      "Did not buy at 20210208\n",
      "Bought at 20210209 with price = 136.19\n",
      "Did not buy at 20210210\n",
      "Did not buy at 20210211\n",
      "Did not buy at 20210212\n",
      "Did not buy at 20210216\n",
      "Sold at 20210217 with price = 130.83\n",
      "Did not buy at 20210218\n",
      "Did not buy at 20210219\n",
      "Did not buy at 20210222\n",
      "Did not buy at 20210223\n",
      "Did not buy at 20210224\n",
      "Did not buy at 20210225\n",
      "Did not buy at 20210226\n",
      "Did not buy at 20210301\n",
      "Did not buy at 20210302\n",
      "Did not buy at 20210303\n",
      "Did not buy at 20210304\n",
      "Did not buy at 20210305\n",
      "Did not buy at 20210308\n",
      "Did not buy at 20210309\n",
      "Did not buy at 20210310\n",
      "Did not buy at 20210311\n",
      "Did not buy at 20210312\n",
      "Did not buy at 20210315\n",
      "Did not buy at 20210316\n",
      "Bought at 20210317 with price = 123.65\n",
      "Did not buy at 20210318\n",
      "Did not buy at 20210319\n",
      "Did not buy at 20210322\n",
      "Did not buy at 20210323\n",
      "Sold at 20210324 with price = 122.43\n",
      "Bought at 20210325 with price = 119.16\n",
      "Sold at 20210326 with price = 119.97\n",
      "Did not buy at 20210329\n",
      "Did not buy at 20210330\n",
      "Did not buy at 20210331\n",
      "Bought at 20210401 with price = 123.27\n",
      "Did not buy at 20210405\n",
      "Did not buy at 20210406\n",
      "Did not buy at 20210407\n",
      "Did not buy at 20210408\n",
      "Did not buy at 20210409\n",
      "Did not buy at 20210412\n",
      "Did not buy at 20210413\n",
      "Did not buy at 20210414\n",
      "Did not buy at 20210415\n",
      "Did not buy at 20210416\n",
      "Did not buy at 20210419\n",
      "Did not buy at 20210420\n",
      "Did not buy at 20210421\n",
      "Did not buy at 20210422\n",
      "Sold at 20210423 with price = 131.74\n",
      "Bought at 20210426 with price = 134.4\n",
      "Sold at 20210427 with price = 134.58\n",
      "Bought at 20210428 with price = 133.88\n",
      "Did not buy at 20210429\n",
      "Did not buy at 20210430\n",
      "Sold at 20210503 with price = 131.62\n",
      "Did not buy at 20210504\n",
      "Did not buy at 20210505\n",
      "Did not buy at 20210506\n",
      "Did not buy at 20210507\n",
      "Did not buy at 20210510\n",
      "Did not buy at 20210511\n",
      "Did not buy at 20210512\n",
      "Did not buy at 20210513\n",
      "Did not buy at 20210514\n",
      "Did not buy at 20210517\n",
      "Did not buy at 20210518\n",
      "Did not buy at 20210519\n",
      "Did not buy at 20210520\n",
      "Did not buy at 20210521\n",
      "Bought at 20210524 with price = 125.82\n",
      "Did not buy at 20210525\n",
      "Did not buy at 20210526\n",
      "Did not buy at 20210527\n",
      "Did not buy at 20210528\n",
      "Sold at 20210601 with price = 124.89\n",
      "Did not buy at 20210602\n",
      "Did not buy at 20210603\n",
      "Did not buy at 20210604\n",
      "Did not buy at 20210607\n",
      "Did not buy at 20210608\n",
      "Bought at 20210609 with price = 127.02\n",
      "Did not buy at 20210610\n",
      "Did not buy at 20210611\n",
      "Did not buy at 20210614\n",
      "Did not buy at 20210615\n",
      "Did not buy at 20210616\n",
      "Did not buy at 20210617\n",
      "Did not buy at 20210618\n",
      "Did not buy at 20210621\n",
      "Did not buy at 20210622\n",
      "Did not buy at 20210623\n",
      "Did not buy at 20210624\n",
      "Did not buy at 20210625\n",
      "Did not buy at 20210628\n",
      "Did not buy at 20210629\n",
      "Did not buy at 20210630\n",
      "Did not buy at 20210701\n",
      "Did not buy at 20210702\n",
      "Did not buy at 20210706\n",
      "Did not buy at 20210707\n",
      "Did not buy at 20210708\n",
      "Did not buy at 20210709\n",
      "Did not buy at 20210712\n",
      "Did not buy at 20210713\n",
      "Did not buy at 20210714\n",
      "Did not buy at 20210715\n",
      "Did not buy at 20210716\n",
      "Did not buy at 20210719\n",
      "Did not buy at 20210720\n",
      "Did not buy at 20210721\n",
      "Sold at 20210722 with price = 145.72\n",
      "Did not buy at 20210723\n",
      "Bought at 20210726 with price = 148.05\n",
      "Did not buy at 20210727\n",
      "Did not buy at 20210728\n",
      "Sold at 20210729 with price = 144.47\n",
      "Did not buy at 20210730\n",
      "Did not buy at 20210802\n",
      "Did not buy at 20210803\n",
      "Did not buy at 20210804\n",
      "Did not buy at 20210805\n",
      "Did not buy at 20210806\n",
      "Bought at 20210809 with price = 146.2\n",
      "Did not buy at 20210810\n",
      "Did not buy at 20210811\n",
      "Did not buy at 20210812\n",
      "Did not buy at 20210813\n",
      "Did not buy at 20210816\n",
      "Did not buy at 20210817\n",
      "Did not buy at 20210818\n",
      "Did not buy at 20210819\n",
      "Sold at 20210820 with price = 147.44\n",
      "Did not buy at 20210823\n",
      "Did not buy at 20210824\n",
      "Bought at 20210825 with price = 149.81\n",
      "Did not buy at 20210826\n",
      "Did not buy at 20210827\n",
      "Did not buy at 20210830\n",
      "Did not buy at 20210831\n",
      "Did not buy at 20210901\n",
      "Did not buy at 20210902\n",
      "Did not buy at 20210903\n",
      "Did not buy at 20210907\n",
      "Did not buy at 20210908\n",
      "Did not buy at 20210909\n",
      "Did not buy at 20210910\n",
      "Sold at 20210913 with price = 150.63\n",
      "Did not buy at 20210914\n",
      "Did not buy at 20210915\n",
      "Did not buy at 20210916\n",
      "Did not buy at 20210917\n",
      "Did not buy at 20210920\n",
      "Did not buy at 20210921\n",
      "Did not buy at 20210922\n",
      "Did not buy at 20210923\n",
      "Did not buy at 20210924\n",
      "Did not buy at 20210927\n",
      "Did not buy at 20210928\n",
      "Did not buy at 20210929\n",
      "Did not buy at 20210930\n",
      "         Action   Price Position          Cash Pos_Bal   Cash_Bal Cum_Profit\n",
      "20190116    Buy  37.209      268  -9991.956024     268      8.044    -19.944\n",
      "20190311   Sell  42.837     -268  11457.355368       0  11465.399   1465.399\n",
      "20190312    Buy  43.937      260  -11446.46724     260     18.932   1442.552\n",
      "20190501   Sell   51.23     -260    13293.1604       0  13312.093   3312.093\n",
      "20190502    Buy  51.222      259 -13293.030996     259     19.062    3285.56\n",
      "...         ...     ...      ...           ...     ...        ...        ...\n",
      "20210820   Sell  147.44     -104   15303.09248       0  15397.935   5397.935\n",
      "20210825    Buy  149.81      102  -15311.18124     102     86.754   5367.374\n",
      "20210913   Sell  150.63     -102   15333.53148       0  15420.285   5420.285\n",
      "20200116    Buy  77.364      129  -9999.915912     129      0.084     -19.96\n",
      "20210106   Sell  127.12     -110    13955.2336       0  14058.564   4058.564\n",
      "\n",
      "[71 rows x 7 columns]\n",
      "==========================\n",
      "5420.2854179999995\n"
     ]
    }
   ],
   "source": [
    "abc_test = custom_split(data,start = 20200101,end = 20210930)\n",
    "tf_abc_test= transform_X_data_to_tensor(abc_test,num_day_to_predict)\n",
    "predictions = loaded_model.predict(tf_abc_test)\n",
    "backtest_data,pred = convert_decision(abc_test,predictions)\n",
    "capital = 10000\n",
    "handling_fee = 0.002 \n",
    "backtest_abc = backtest(backtest_data,pred,capital,handling_fee)\n",
    "backtest_abc.start_test()\n",
    "backtest_abc.print_trade_record()\n",
    "print('==========================')\n",
    "backtest_abc.export_trade_record(\"AAPL (abc)\")\n",
    "backtest_abc.print_profit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77a11b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20200116</th>\n",
       "      <td>77.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200117</th>\n",
       "      <td>78.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200121</th>\n",
       "      <td>78.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200122</th>\n",
       "      <td>78.594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200123</th>\n",
       "      <td>78.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210924</th>\n",
       "      <td>145.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210927</th>\n",
       "      <td>145.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210928</th>\n",
       "      <td>143.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210929</th>\n",
       "      <td>142.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210930</th>\n",
       "      <td>143.660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>431 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open\n",
       "DATE             \n",
       "20200116   77.364\n",
       "20200117   78.023\n",
       "20200121   78.252\n",
       "20200122   78.594\n",
       "20200123   78.430\n",
       "...           ...\n",
       "20210924  145.660\n",
       "20210927  145.470\n",
       "20210928  143.250\n",
       "20210929  142.470\n",
       "20210930  143.660\n",
       "\n",
       "[431 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23368941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c089d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bd05a26",
   "metadata": {},
   "source": [
    "# Support Vectore Rgression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce479c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
