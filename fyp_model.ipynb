{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24859653",
   "metadata": {},
   "source": [
    "### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ad005eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from stockstats import StockDataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03bed7",
   "metadata": {},
   "source": [
    "### Set the data source path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2436b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data source path\n",
    "interval = \"daily\"\n",
    "region = \"us\"\n",
    "ex_product = \"nasdaq stocks\"\n",
    "section = \"1\"\n",
    "stock = \"aapl\"\n",
    "data_path = \"test_data/\"+interval+\"/\"+region+\"/\"+ex_product+\"/\"+section+\"/\"+stock+\".\"+region+\".txt\"\n",
    "\n",
    "# Use Apple .Inc stock for training\n",
    "\n",
    "# Extract only the OLHC\n",
    "column_to_use = [\"OPEN\",\"LOW\",\"HIGH\",\"CLOSE\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f91fe",
   "metadata": {},
   "source": [
    "### Load the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92c24ac-3419-4a42-973b-e21ae036c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ori_data = pd.read_csv(data_path, sep=\",\")\n",
    "\n",
    "# Rename the column names\n",
    "ori_data.columns = [colname[1:-1] for colname in ori_data.columns]\n",
    "\n",
    "# Drop the unnecessary\n",
    "ori_data.index = ori_data[\"DATE\"]\n",
    "ori_data = ori_data.drop(columns=['DATE','PER','TIME', 'TICKER', 'OPENINT'])\n",
    "ori_data.columns = [\"open\",\"high\",\"low\",\"close\",\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966120a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use online package to generate additional features\n",
    "x = StockDataFrame(ori_data)\n",
    "data = x[['open','high','low','close','volume',\n",
    "          'boll', 'boll_ub', 'boll_lb',\n",
    "          'macd', 'macdh', 'macds',\n",
    "          'rsi_11', 'rsi_14', 'rsi_21']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f7671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>boll</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdh</th>\n",
       "      <th>macds</th>\n",
       "      <th>rsi_11</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19840907</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10028</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>96970899</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840910</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.09905</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>75265237</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.102049</td>\n",
       "      <td>0.100351</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840911</th>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10456</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>177479896</td>\n",
       "      <td>0.101713</td>\n",
       "      <td>0.103590</td>\n",
       "      <td>0.099837</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>77.134146</td>\n",
       "      <td>76.758045</td>\n",
       "      <td>76.303318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840912</th>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10334</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>155043826</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.103762</td>\n",
       "      <td>0.098638</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>32.201239</td>\n",
       "      <td>32.592743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840913</th>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10548</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>241475025</td>\n",
       "      <td>0.101996</td>\n",
       "      <td>0.106191</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>68.412723</td>\n",
       "      <td>68.025100</td>\n",
       "      <td>67.561551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211021</th>\n",
       "      <td>148.81000</td>\n",
       "      <td>149.64000</td>\n",
       "      <td>147.87000</td>\n",
       "      <td>149.48000</td>\n",
       "      <td>61420990</td>\n",
       "      <td>143.875000</td>\n",
       "      <td>149.793245</td>\n",
       "      <td>137.956755</td>\n",
       "      <td>0.375617</td>\n",
       "      <td>1.069014</td>\n",
       "      <td>-0.693396</td>\n",
       "      <td>65.673205</td>\n",
       "      <td>61.532287</td>\n",
       "      <td>57.199864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022</th>\n",
       "      <td>149.69000</td>\n",
       "      <td>150.18000</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>148.69000</td>\n",
       "      <td>58883443</td>\n",
       "      <td>143.963500</td>\n",
       "      <td>150.121546</td>\n",
       "      <td>137.805454</td>\n",
       "      <td>0.577001</td>\n",
       "      <td>1.016318</td>\n",
       "      <td>-0.439317</td>\n",
       "      <td>61.924694</td>\n",
       "      <td>58.867114</td>\n",
       "      <td>55.611436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025</th>\n",
       "      <td>148.68000</td>\n",
       "      <td>149.37000</td>\n",
       "      <td>147.62110</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>50720556</td>\n",
       "      <td>144.127000</td>\n",
       "      <td>150.607481</td>\n",
       "      <td>137.646519</td>\n",
       "      <td>0.724216</td>\n",
       "      <td>0.930826</td>\n",
       "      <td>-0.206610</td>\n",
       "      <td>61.679591</td>\n",
       "      <td>58.693837</td>\n",
       "      <td>55.508996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211026</th>\n",
       "      <td>149.33000</td>\n",
       "      <td>150.84000</td>\n",
       "      <td>149.01010</td>\n",
       "      <td>149.32000</td>\n",
       "      <td>60893395</td>\n",
       "      <td>144.497500</td>\n",
       "      <td>151.284341</td>\n",
       "      <td>137.710659</td>\n",
       "      <td>0.885547</td>\n",
       "      <td>0.873726</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>63.821803</td>\n",
       "      <td>60.401009</td>\n",
       "      <td>56.649320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211027</th>\n",
       "      <td>149.36000</td>\n",
       "      <td>149.73000</td>\n",
       "      <td>148.49000</td>\n",
       "      <td>148.85000</td>\n",
       "      <td>55925403</td>\n",
       "      <td>144.798500</td>\n",
       "      <td>151.804399</td>\n",
       "      <td>137.792601</td>\n",
       "      <td>0.964362</td>\n",
       "      <td>0.762033</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>61.219811</td>\n",
       "      <td>58.598318</td>\n",
       "      <td>55.614834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9362 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               open       high        low      close     volume        boll  \\\n",
       "DATE                                                                          \n",
       "19840907    0.10150    0.10274    0.10028    0.10150   96970899    0.101500   \n",
       "19840910    0.10150    0.10181    0.09905    0.10090   75265237    0.101200   \n",
       "19840911    0.10181    0.10456    0.10181    0.10274  177479896    0.101713   \n",
       "19840912    0.10274    0.10334    0.09966    0.09966  155043826    0.101200   \n",
       "19840913    0.10518    0.10548    0.10518    0.10518  241475025    0.101996   \n",
       "...             ...        ...        ...        ...        ...         ...   \n",
       "20211021  148.81000  149.64000  147.87000  149.48000   61420990  143.875000   \n",
       "20211022  149.69000  150.18000  148.64000  148.69000   58883443  143.963500   \n",
       "20211025  148.68000  149.37000  147.62110  148.64000   50720556  144.127000   \n",
       "20211026  149.33000  150.84000  149.01010  149.32000   60893395  144.497500   \n",
       "20211027  149.36000  149.73000  148.49000  148.85000   55925403  144.798500   \n",
       "\n",
       "             boll_ub     boll_lb      macd     macdh     macds     rsi_11  \\\n",
       "DATE                                                                        \n",
       "19840907         NaN         NaN  0.000000  0.000000  0.000000        NaN   \n",
       "19840910    0.102049    0.100351 -0.000013 -0.000006 -0.000007   0.000000   \n",
       "19840911    0.103590    0.099837  0.000040  0.000028  0.000012  77.134146   \n",
       "19840912    0.103762    0.098638 -0.000048 -0.000040 -0.000008  31.870001   \n",
       "19840913    0.106191    0.097801  0.000125  0.000094  0.000031  68.412723   \n",
       "...              ...         ...       ...       ...       ...        ...   \n",
       "20211021  149.793245  137.956755  0.375617  1.069014 -0.693396  65.673205   \n",
       "20211022  150.121546  137.805454  0.577001  1.016318 -0.439317  61.924694   \n",
       "20211025  150.607481  137.646519  0.724216  0.930826 -0.206610  61.679591   \n",
       "20211026  151.284341  137.710659  0.885547  0.873726  0.011821  63.821803   \n",
       "20211027  151.804399  137.792601  0.964362  0.762033  0.202329  61.219811   \n",
       "\n",
       "             rsi_14     rsi_21  \n",
       "DATE                            \n",
       "19840907        NaN        NaN  \n",
       "19840910   0.000000   0.000000  \n",
       "19840911  76.758045  76.303318  \n",
       "19840912  32.201239  32.592743  \n",
       "19840913  68.025100  67.561551  \n",
       "...             ...        ...  \n",
       "20211021  61.532287  57.199864  \n",
       "20211022  58.867114  55.611436  \n",
       "20211025  58.693837  55.508996  \n",
       "20211026  60.401009  56.649320  \n",
       "20211027  58.598318  55.614834  \n",
       "\n",
       "[9362 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e19a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test data\n",
    "\n",
    "def custom_split(data,start,end):\n",
    "    train = (data.index >= start) & (data.index <= end)\n",
    "    train_X = data[train]\n",
    "    \n",
    "    return train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca219b",
   "metadata": {},
   "source": [
    "# CNN_LSTM (Price Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cd6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = custom_split(data,start = 20130101,end = 20171031)\n",
    "valid_X = custom_split(data,start = 20171101,end = 20181231)\n",
    "test_X = custom_split(data,start = 20190101,end = 20201231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce609a",
   "metadata": {},
   "source": [
    "### Label the target result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e4e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we use 10 days price data to predict opening price of the 11th day\n",
    "num_day_to_predict = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f127afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_result_target_price(X,num_day,result_col_name = \"result_price\"):\n",
    "    y = pd.DataFrame(np.nan, index=X.index, columns=[result_col_name])\n",
    "    for i in range(len(X)-num_day):\n",
    "        y.iloc[i+num_day_to_predict,0] = X.iloc[i+num_day,0]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7fcaaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = produce_result_target_price(train_X,num_day_to_predict)\n",
    "valid_y = produce_result_target_price(valid_X,num_day_to_predict)\n",
    "test_y = produce_result_target_price(test_X,num_day_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1064e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20171101</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171102</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171103</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171106</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171107</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181224</th>\n",
       "      <td>36.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181226</th>\n",
       "      <td>36.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181227</th>\n",
       "      <td>37.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181228</th>\n",
       "      <td>38.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181231</th>\n",
       "      <td>38.526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          result_price\n",
       "DATE                  \n",
       "20171101           NaN\n",
       "20171102           NaN\n",
       "20171103           NaN\n",
       "20171106           NaN\n",
       "20171107           NaN\n",
       "...                ...\n",
       "20181224        36.007\n",
       "20181226        36.044\n",
       "20181227        37.876\n",
       "20181228        38.280\n",
       "20181231        38.526\n",
       "\n",
       "[292 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a8ca1",
   "metadata": {},
   "source": [
    "### Transform the X, y data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b8773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_to_tensor(X,y,num_day):\n",
    "    # Initiate tensor for X\n",
    "    x_first = X.iloc[0:num_day,:]\n",
    "    x_mean = x_first.mean(axis=0) # Get the mean of the 10-day frame\n",
    "    x_std = x_first.std(axis=0) # Get the std of the 10-day frame\n",
    "    x_first = x_first.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "    \n",
    "    # Initiate tensor for y\n",
    "    x_open = X.iloc[0:num_day,0]\n",
    "    y_val = y.iloc[num_day,:] # Get the corresponding y\n",
    "    y_val = y_val.sub(x_open.mean(axis=0)).div(x_open.std(axis=0)) # Normalize the y\n",
    "    \n",
    "    x_tf_data = [tf.convert_to_tensor(np.array(x_first),dtype = tf.float32)]\n",
    "    y_tf_data = [tf.convert_to_tensor(np.array(y_val),dtype = tf.float32)]\n",
    "    \n",
    "    for i in range(1,len(X)-num_day):   \n",
    "        x_window = X.iloc[i:i+num_day,:] # Set the window as a 10-day frame \n",
    "        x_mean = x_window.mean(axis=0) # Get the mean of the 10-day frame\n",
    "        x_std = x_window.std(axis=0) # Get the std of the 10-day frame\n",
    "        x_window = x_window.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "        \n",
    "        x_open = X.iloc[i:i+num_day,0] # Get the opening price of the 10-day frame\n",
    "        y_val = y.iloc[i+num_day,:] # Get the corresponding y\n",
    "        y_val = y_val.sub(x_open.mean(axis=0)).div(x_open.std(axis=0)) # Normalize the y\n",
    "        \n",
    "        x_next_tf = tf.convert_to_tensor(np.array(x_window),dtype = tf.float32)\n",
    "        x_tf_data = tf.concat([x_tf_data, [x_next_tf]], 0)\n",
    "        \n",
    "        y_next_tf = tf.convert_to_tensor(np.array(y_val),dtype = tf.float32)\n",
    "        y_tf_data = tf.concat([y_tf_data, [y_next_tf]], 0)\n",
    "    return (tf.reshape(x_tf_data,(-1,10,14,1)),y_tf_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b66168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:10:12.920175: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-12 16:10:12.920707: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "tf_train_X,tf_train_y = transform_data_to_tensor(train_X,train_y,num_day_to_predict)\n",
    "tf_valid_X,tf_valid_y = transform_data_to_tensor(valid_X,valid_y,num_day_to_predict)\n",
    "tf_test_X,tf_test_y = transform_data_to_tensor(test_X,test_y,num_day_to_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f49a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1208, 10, 14, 1)\n",
      "(1208, 1)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n",
      "(282, 10, 14, 1)\n",
      "(282, 1)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n",
      "(495, 10, 14, 1)\n",
      "(495, 1)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(tf_train_X.shape)\n",
    "print(tf_train_y.shape)\n",
    "print(tf_train_X.dtype)\n",
    "print(tf_train_y.dtype)\n",
    "\n",
    "print(tf_valid_X.shape)\n",
    "print(tf_valid_y.shape)\n",
    "print(tf_valid_X.dtype)\n",
    "print(tf_valid_y.dtype)\n",
    "\n",
    "print(tf_test_X.shape)\n",
    "print(tf_test_y.shape)\n",
    "print(tf_test_X.dtype)\n",
    "print(tf_test_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e64c36",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def myModel(input_shape,\n",
    "            encoder_unit = 100,\n",
    "            repeat_vector_n = 10):\n",
    "    \n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    print(\"Input: \",inputs.shape)\n",
    "    \n",
    "    # First Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 64,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"1 Cov: \",x.shape)\n",
    "    \n",
    "    # Second Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 16,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"2 Cov: \",x.shape)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = layers.Flatten()(x)\n",
    "    print(\"Flatten: \",x.shape)\n",
    "    \n",
    "    # Repeat Vector Layer\n",
    "    x = layers.RepeatVector(n = repeat_vector_n)(x)\n",
    "    print(\"RepeatVector: \",x.shape)\n",
    "    \n",
    "    # Connect to LSTM\n",
    "    x = layers.LSTM(units = encoder_unit, input_shape=(5,1))(x)\n",
    "    print(\"LSTM: \",x.shape)\n",
    "    \n",
    "    # Add the Dense Layer with relu activation\n",
    "    x = layers.Dense(units = 50,activation = \"relu\")(x)\n",
    "    print(\"1 Dense: \",x.shape)\n",
    "    \n",
    "    # Add the last Dense Layer with sigmoid activation\n",
    "    outputs = layers.Dense(units = 1,activation = \"sigmoid\")(x)\n",
    "    print(\"Output: \",outputs.shape)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fc4f2",
   "metadata": {},
   "source": [
    "### Model Training and Fitting and Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72215124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:10:51.276742: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_18606_18788' and '__inference___backward_standard_lstm_18912_19397_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_19570' both implement 'lstm_de1b7df4-cf8a-448b-bc07-e0e3230875cf' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 18s 15ms/sample - loss: 1.2080 - mean_squared_error: 2.6683\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1435 - mean_squared_error: 2.4437\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.1031 - mean_squared_error: 2.3636\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0714 - mean_squared_error: 2.2311\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0908 - mean_squared_error: 2.2741\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0545 - mean_squared_error: 2.1642\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0304 - mean_squared_error: 2.0970\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0464 - mean_squared_error: 2.1867\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0144 - mean_squared_error: 2.1571\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0458 - mean_squared_error: 2.2095\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.9829 - mean_squared_error: 1.9308\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0258 - mean_squared_error: 2.1814\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9830 - mean_squared_error: 1.9898\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0069 - mean_squared_error: 2.0349\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0262 - mean_squared_error: 2.1668\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0236 - mean_squared_error: 2.1446\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9640 - mean_squared_error: 1.9436\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0226 - mean_squared_error: 2.2288\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9610 - mean_squared_error: 1.8974\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0461 - mean_squared_error: 2.2522\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9541 - mean_squared_error: 1.9077\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0002 - mean_squared_error: 2.0878\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9999 - mean_squared_error: 2.0339\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9675 - mean_squared_error: 1.9471\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0209 - mean_squared_error: 2.3095\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 0.9656 - mean_squared_error: 1.9095\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9850 - mean_squared_error: 2.0778\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9743 - mean_squared_error: 2.0345\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9689 - mean_squared_error: 1.9845\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0139 - mean_squared_error: 2.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:15:18.738011: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_24508' and '__inference_standard_lstm_24397_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_24748' both implement 'lstm_903eee1f-fcd3-41b6-be3a-632ce4c5c846' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 1.2395 - mean_squared_error: 1.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0452159355718194, 1.8809282]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0452159355718194, 1.8809282]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:15:23.625665: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_27785_28270' and '__inference___backward_standard_lstm_27785_28270_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_28443' both implement 'lstm_944ea858-5a41-4ef5-a973-bfe2dda40ffc' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 15s 12ms/sample - loss: 1.1378 - mean_squared_error: 2.3878\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.2572 - mean_squared_error: 2.7414\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.3630 - mean_squared_error: 3.0532\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.3521 - mean_squared_error: 3.0044\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3552 - mean_squared_error: 3.0118\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.3606 - mean_squared_error: 3.0503\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.4003 - mean_squared_error: 3.2458\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3146 - mean_squared_error: 2.8238\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3619 - mean_squared_error: 2.9964\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3696 - mean_squared_error: 3.0902\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3456 - mean_squared_error: 3.0314\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0969 - mean_squared_error: 2.3115\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0427 - mean_squared_error: 2.1240\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0624 - mean_squared_error: 2.2614\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0437 - mean_squared_error: 2.1201\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0521 - mean_squared_error: 2.2678\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0880 - mean_squared_error: 2.1369\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0676 - mean_squared_error: 2.2931\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0649 - mean_squared_error: 2.1470\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0454 - mean_squared_error: 2.2020\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0188 - mean_squared_error: 2.1416\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0772 - mean_squared_error: 2.3557\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0139 - mean_squared_error: 1.9749\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0746 - mean_squared_error: 2.3465\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0328 - mean_squared_error: 1.9743\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0565 - mean_squared_error: 2.2442\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0642 - mean_squared_error: 2.2262\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9998 - mean_squared_error: 2.0601\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0441 - mean_squared_error: 2.1510\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0031 - mean_squared_error: 2.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:18:52.073476: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_33381' and '__inference_standard_lstm_33270_specialized_for_model_1_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_33621' both implement 'lstm_c270ec5e-8b70-4218-9f6c-ea09c904fbe7' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2396 - mean_squared_error: 1.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.052262969896303, 1.8932962]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.052262969896303, 1.8932962]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:18:55.022257: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_36352_36534' and '__inference___backward_standard_lstm_36658_37143_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_37316' both implement 'lstm_7b5e2f54-3c27-4cde-9af4-5009b0effe01' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.1765 - mean_squared_error: 2.5422\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1004 - mean_squared_error: 2.3625\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0705 - mean_squared_error: 2.2289\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0419 - mean_squared_error: 2.1247\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0343 - mean_squared_error: 2.1979\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0316 - mean_squared_error: 2.0994\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0209 - mean_squared_error: 2.0619\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0728 - mean_squared_error: 2.2918\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0449 - mean_squared_error: 2.2291\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0705 - mean_squared_error: 2.2267\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0276 - mean_squared_error: 1.9535\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0502 - mean_squared_error: 2.2495\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0946 - mean_squared_error: 2.2579\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0681 - mean_squared_error: 2.2630\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0689 - mean_squared_error: 2.1843\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0957 - mean_squared_error: 2.3985\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0296 - mean_squared_error: 2.1871\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1161 - mean_squared_error: 2.3147\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1369 - mean_squared_error: 2.3502\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0732 - mean_squared_error: 2.1708\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0697 - mean_squared_error: 2.1890\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1059 - mean_squared_error: 2.4329\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0533 - mean_squared_error: 2.0984\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0550 - mean_squared_error: 2.1984\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0473 - mean_squared_error: 2.0553\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1053 - mean_squared_error: 2.3953\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0524 - mean_squared_error: 2.1154\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0278 - mean_squared_error: 2.0249\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0387 - mean_squared_error: 2.1049\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1061 - mean_squared_error: 2.4132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:22:03.575298: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_42143' and '__inference_standard_lstm_42143_specialized_for_model_2_lstm_2_StatefulPartitionedCall_at___inference_distributed_function_42494' both implement 'lstm_4e434bdf-98b3-4b24-a06d-7c554b74396d' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2824 - mean_squared_error: 1.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0715465038380725, 1.9652798]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0715465038380725, 1.9652798]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:22:07.149239: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_45531_46016' and '__inference___backward_standard_lstm_45531_46016_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_46189' both implement 'lstm_41df7be8-0af2-452e-a77d-3caf197f50e1' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1552 - mean_squared_error: 2.4699\n",
      "Epoch 2/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1211 - mean_squared_error: 2.4261\n",
      "Epoch 3/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0842 - mean_squared_error: 2.3545\n",
      "Epoch 4/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0354 - mean_squared_error: 2.0506\n",
      "Epoch 5/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0596 - mean_squared_error: 2.1810\n",
      "Epoch 6/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0441 - mean_squared_error: 2.2779\n",
      "Epoch 7/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0253 - mean_squared_error: 2.0260\n",
      "Epoch 8/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0277 - mean_squared_error: 2.0909\n",
      "Epoch 9/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0140 - mean_squared_error: 2.1501\n",
      "Epoch 10/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0235 - mean_squared_error: 2.0271\n",
      "Epoch 11/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0427 - mean_squared_error: 2.1102\n",
      "Epoch 12/30\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0357 - mean_squared_error: 2.1529\n",
      "Epoch 13/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1329 - mean_squared_error: 2.5613\n",
      "Epoch 14/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0525 - mean_squared_error: 2.2104\n",
      "Epoch 15/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0292 - mean_squared_error: 2.0946\n",
      "Epoch 16/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0641 - mean_squared_error: 2.2981\n",
      "Epoch 17/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9875 - mean_squared_error: 1.8722\n",
      "Epoch 18/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0459 - mean_squared_error: 2.1722\n",
      "Epoch 19/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0297 - mean_squared_error: 2.2379\n",
      "Epoch 20/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0362 - mean_squared_error: 1.9961\n",
      "Epoch 21/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0454 - mean_squared_error: 2.3038\n",
      "Epoch 22/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0219 - mean_squared_error: 2.1183\n",
      "Epoch 23/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0269 - mean_squared_error: 2.1105\n",
      "Epoch 24/30\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0024 - mean_squared_error: 1.9486\n",
      "Epoch 25/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0475 - mean_squared_error: 2.2702\n",
      "Epoch 26/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0542 - mean_squared_error: 2.1564\n",
      "Epoch 27/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0588 - mean_squared_error: 2.3054\n",
      "Epoch 28/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0197 - mean_squared_error: 2.0540\n",
      "Epoch 29/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0446 - mean_squared_error: 2.1361\n",
      "Epoch 30/30\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0732 - mean_squared_error: 2.3403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:25:39.819050: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_51016' and '__inference_standard_lstm_51016_specialized_for_model_3_lstm_3_StatefulPartitionedCall_at___inference_distributed_function_51367' both implement 'lstm_9c9c8712-a52f-4abf-8074-c1f7d6a3abfa' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 1.2552 - mean_squared_error: 1.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0779119193131197, 1.934911]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0779119193131197, 1.934911]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:25:43.325351: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_54404_54889' and '__inference___backward_standard_lstm_54404_54889_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_55062' both implement 'lstm_b47c8c97-60b6-4dc4-b66f-65156bfcb62f' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.1920 - mean_squared_error: 2.5645\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0804 - mean_squared_error: 2.2378\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0509 - mean_squared_error: 2.1651\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0865 - mean_squared_error: 2.3298\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0309 - mean_squared_error: 2.0043\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0709 - mean_squared_error: 2.2982\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9981 - mean_squared_error: 1.9407\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0363 - mean_squared_error: 2.2148\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0615 - mean_squared_error: 2.2069\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0057 - mean_squared_error: 2.0097\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0349 - mean_squared_error: 2.2381\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9809 - mean_squared_error: 1.8109\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0227 - mean_squared_error: 2.1933\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0030 - mean_squared_error: 2.0486\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9934 - mean_squared_error: 2.0422\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0153 - mean_squared_error: 2.1188\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9697 - mean_squared_error: 1.9675\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9959 - mean_squared_error: 2.1067\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0103 - mean_squared_error: 2.0592\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0265 - mean_squared_error: 2.3334\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9728 - mean_squared_error: 1.8541\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9916 - mean_squared_error: 2.0089\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 13s 11ms/sample - loss: 1.0288 - mean_squared_error: 2.2119\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 16s 13ms/sample - loss: 0.9607 - mean_squared_error: 2.1138\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9878 - mean_squared_error: 1.9256\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9803 - mean_squared_error: 2.1044\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9638 - mean_squared_error: 1.8847\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0039 - mean_squared_error: 2.1696\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9565 - mean_squared_error: 2.0107\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0037 - mean_squared_error: 2.0126\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9765 - mean_squared_error: 2.0854\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9942 - mean_squared_error: 2.1619\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9149 - mean_squared_error: 1.8328\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9798 - mean_squared_error: 2.0932\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9501 - mean_squared_error: 1.9844\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9797 - mean_squared_error: 2.1311\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9614 - mean_squared_error: 2.0621\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9754 - mean_squared_error: 2.0935\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9248 - mean_squared_error: 1.9126\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9891 - mean_squared_error: 2.0675\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9523 - mean_squared_error: 2.0027\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9513 - mean_squared_error: 1.9638\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9606 - mean_squared_error: 2.0250\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9701 - mean_squared_error: 2.0894\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9513 - mean_squared_error: 1.9585\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9512 - mean_squared_error: 1.9394\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9732 - mean_squared_error: 2.1526\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9420 - mean_squared_error: 1.9797\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 0.9540 - mean_squared_error: 2.0220\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 0.9574 - mean_squared_error: 2.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:31:05.389125: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_63040' and '__inference_standard_lstm_62929_specialized_for_model_4_lstm_4_StatefulPartitionedCall_at___inference_distributed_function_63280' both implement 'lstm_c119b11f-a367-435c-97bd-40c736795289' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2399 - mean_squared_error: 1.8220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0216271382697084, 1.8219554]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0216271382697084, 1.8219554]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:31:08.495262: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_66317_66802_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_66975' and '__inference___backward_cudnn_lstm_with_fallback_66011_66193' both implement 'lstm_8aabda42-578b-44ae-a9f6-9b9ca0aaf801' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.1744 - mean_squared_error: 2.5390\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0743 - mean_squared_error: 2.2381\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0821 - mean_squared_error: 2.3211\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0443 - mean_squared_error: 2.0995\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0261 - mean_squared_error: 2.1586\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0467 - mean_squared_error: 2.2216\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.2305 - mean_squared_error: 2.7607\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3077 - mean_squared_error: 3.1827\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1418 - mean_squared_error: 2.4099\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1100 - mean_squared_error: 2.2227\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1133 - mean_squared_error: 2.3535\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0711 - mean_squared_error: 2.3766\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0887 - mean_squared_error: 2.2934\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0916 - mean_squared_error: 2.2741\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0580 - mean_squared_error: 2.3212\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0753 - mean_squared_error: 2.1302\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0581 - mean_squared_error: 2.1828\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0690 - mean_squared_error: 2.2203\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0574 - mean_squared_error: 2.3146\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0644 - mean_squared_error: 2.2341\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0134 - mean_squared_error: 1.8793\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0751 - mean_squared_error: 2.3635\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0094 - mean_squared_error: 2.1669\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0384 - mean_squared_error: 2.0553\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0928 - mean_squared_error: 2.2555\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0550 - mean_squared_error: 2.2884\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0273 - mean_squared_error: 1.9794\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0715 - mean_squared_error: 2.2429\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0101 - mean_squared_error: 2.0437\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0619 - mean_squared_error: 2.3140\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0114 - mean_squared_error: 2.0344\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0176 - mean_squared_error: 2.0793\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0614 - mean_squared_error: 2.2450\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0372 - mean_squared_error: 2.1931\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0040 - mean_squared_error: 2.0314\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0136 - mean_squared_error: 2.0627\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0356 - mean_squared_error: 2.2448\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0006 - mean_squared_error: 2.1113\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0281 - mean_squared_error: 2.1359\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9913 - mean_squared_error: 1.8346\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0144 - mean_squared_error: 2.3047\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0208 - mean_squared_error: 2.0858\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0065 - mean_squared_error: 2.0880\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0397 - mean_squared_error: 2.2289\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0270 - mean_squared_error: 2.1152\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0363 - mean_squared_error: 2.0398\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0198 - mean_squared_error: 2.1901\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0184 - mean_squared_error: 2.1087\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0330 - mean_squared_error: 2.1253\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0171 - mean_squared_error: 2.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:36:22.834132: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_74953' and '__inference_standard_lstm_74842_specialized_for_model_5_lstm_5_StatefulPartitionedCall_at___inference_distributed_function_75193' both implement 'lstm_ff0d7fec-6e75-4388-8522-9ae330742285' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2277 - mean_squared_error: 1.8623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0325432677640982, 1.8623041]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0325432677640982, 1.8623041]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:36:25.840425: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_77924_78106' and '__inference___backward_standard_lstm_78230_78715_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_78888' both implement 'lstm_ba9c2e2e-9ea1-4c22-b413-715ea5de7765' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.1376 - mean_squared_error: 2.4261\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0924 - mean_squared_error: 2.3180\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0618 - mean_squared_error: 2.2146\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0686 - mean_squared_error: 2.2097\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0598 - mean_squared_error: 2.2061\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0874 - mean_squared_error: 2.3548\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0437 - mean_squared_error: 2.0159\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1080 - mean_squared_error: 2.4391\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0782 - mean_squared_error: 2.1814\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0262 - mean_squared_error: 2.1038\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0539 - mean_squared_error: 2.2151\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0800 - mean_squared_error: 2.2602\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0556 - mean_squared_error: 2.1630\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0592 - mean_squared_error: 2.2477\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0649 - mean_squared_error: 2.1294\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0644 - mean_squared_error: 2.2643\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0748 - mean_squared_error: 2.2804\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0871 - mean_squared_error: 2.2478\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0118 - mean_squared_error: 1.9557\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0960 - mean_squared_error: 2.4729\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0486 - mean_squared_error: 2.1094\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0418 - mean_squared_error: 2.1362\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0665 - mean_squared_error: 2.1480\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0543 - mean_squared_error: 2.1792\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0175 - mean_squared_error: 2.0308\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0395 - mean_squared_error: 2.1592\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0558 - mean_squared_error: 2.3977\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0189 - mean_squared_error: 2.1430\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0327 - mean_squared_error: 1.9416\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.2310 - mean_squared_error: 2.8728\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1057 - mean_squared_error: 2.3380\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0963 - mean_squared_error: 2.2488\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0605 - mean_squared_error: 2.2543\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0530 - mean_squared_error: 2.1377\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0507 - mean_squared_error: 2.2352\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0596 - mean_squared_error: 2.2077\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0541 - mean_squared_error: 2.3513\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0410 - mean_squared_error: 2.0951\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0304 - mean_squared_error: 2.1188\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0335 - mean_squared_error: 1.9613\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0658 - mean_squared_error: 2.3794\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0489 - mean_squared_error: 2.1334\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0722 - mean_squared_error: 2.3236\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0517 - mean_squared_error: 2.1664\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0574 - mean_squared_error: 2.2154\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0743 - mean_squared_error: 2.2453\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0805 - mean_squared_error: 2.2338\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0702 - mean_squared_error: 2.2478\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0752 - mean_squared_error: 2.2124\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0815 - mean_squared_error: 2.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:41:18.014921: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_86755' and '__inference_standard_lstm_86755_specialized_for_model_6_lstm_6_StatefulPartitionedCall_at___inference_distributed_function_87106' both implement 'lstm_de9d6bb2-b828-4d71-888b-9d0e11672778' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.2638 - mean_squared_error: 1.9938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0826821665391855, 1.9937755]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0826821665391855, 1.9937755]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:41:20.691029: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_90143_90628' and '__inference___backward_standard_lstm_90143_90628_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_90801' both implement 'lstm_ba274c1b-e72f-4712-be2b-aaeeac866a22' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.1423 - mean_squared_error: 2.4555\n",
      "Epoch 2/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0809 - mean_squared_error: 2.2349\n",
      "Epoch 3/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0717 - mean_squared_error: 2.2282\n",
      "Epoch 4/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0350 - mean_squared_error: 2.1483\n",
      "Epoch 5/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0720 - mean_squared_error: 2.2203\n",
      "Epoch 6/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0310 - mean_squared_error: 2.1980\n",
      "Epoch 7/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0324 - mean_squared_error: 2.0911\n",
      "Epoch 8/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0286 - mean_squared_error: 2.0925\n",
      "Epoch 9/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0096 - mean_squared_error: 2.0972\n",
      "Epoch 10/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0709 - mean_squared_error: 2.2990\n",
      "Epoch 11/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0385 - mean_squared_error: 2.1861\n",
      "Epoch 12/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0247 - mean_squared_error: 2.0405\n",
      "Epoch 13/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0432 - mean_squared_error: 2.2284\n",
      "Epoch 14/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0811 - mean_squared_error: 2.2677\n",
      "Epoch 15/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9709 - mean_squared_error: 1.8868\n",
      "Epoch 16/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0920 - mean_squared_error: 2.3866\n",
      "Epoch 17/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0293 - mean_squared_error: 2.1542\n",
      "Epoch 18/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0740 - mean_squared_error: 2.1631\n",
      "Epoch 19/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0292 - mean_squared_error: 2.2462\n",
      "Epoch 20/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0615 - mean_squared_error: 2.0387\n",
      "Epoch 21/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1191 - mean_squared_error: 2.4306\n",
      "Epoch 22/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0682 - mean_squared_error: 2.1742\n",
      "Epoch 23/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0790 - mean_squared_error: 2.2031\n",
      "Epoch 24/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0807 - mean_squared_error: 2.4170\n",
      "Epoch 25/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0866 - mean_squared_error: 2.3085\n",
      "Epoch 26/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0602 - mean_squared_error: 2.2086\n",
      "Epoch 27/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0378 - mean_squared_error: 2.1539\n",
      "Epoch 28/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0491 - mean_squared_error: 2.0753\n",
      "Epoch 29/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0835 - mean_squared_error: 2.3410\n",
      "Epoch 30/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.3732\n",
      "Epoch 31/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0526 - mean_squared_error: 2.1051\n",
      "Epoch 32/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0326 - mean_squared_error: 2.0093\n",
      "Epoch 33/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1329 - mean_squared_error: 2.6043\n",
      "Epoch 34/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1041 - mean_squared_error: 2.1798\n",
      "Epoch 35/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0881 - mean_squared_error: 2.3242\n",
      "Epoch 36/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1549 - mean_squared_error: 2.4375\n",
      "Epoch 37/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1489 - mean_squared_error: 2.3595\n",
      "Epoch 38/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1228 - mean_squared_error: 2.4678\n",
      "Epoch 39/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0899 - mean_squared_error: 2.2704\n",
      "Epoch 40/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1594 - mean_squared_error: 2.5484\n",
      "Epoch 41/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1209 - mean_squared_error: 2.4606\n",
      "Epoch 42/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1046 - mean_squared_error: 2.3059\n",
      "Epoch 43/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1410 - mean_squared_error: 2.3152\n",
      "Epoch 44/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1002 - mean_squared_error: 2.4619\n",
      "Epoch 45/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1881 - mean_squared_error: 2.7140\n",
      "Epoch 46/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.2327\n",
      "Epoch 47/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0969 - mean_squared_error: 2.3716\n",
      "Epoch 48/50\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0800 - mean_squared_error: 2.2880\n",
      "Epoch 49/50\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0731 - mean_squared_error: 2.2676\n",
      "Epoch 50/50\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0587 - mean_squared_error: 2.2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:46:29.306962: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_98668' and '__inference_standard_lstm_98668_specialized_for_model_7_lstm_7_StatefulPartitionedCall_at___inference_distributed_function_99019' both implement 'lstm_92041608-55fe-440c-870e-93fc48b35040' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.3026 - mean_squared_error: 1.9762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0859702288681734, 1.9761673]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0859702288681734, 1.9761673]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:46:32.859927: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_101750_101932' and '__inference___backward_standard_lstm_102056_102541_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_102714' both implement 'lstm_a0a49fa6-8737-439e-8a04-ed4d5a498aac' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3953 - mean_squared_error: 3.4142\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.4243 - mean_squared_error: 3.5722\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.4190 - mean_squared_error: 3.5444\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1108 - mean_squared_error: 2.3394\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0493 - mean_squared_error: 2.2261\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0149 - mean_squared_error: 2.0116\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0212 - mean_squared_error: 2.1172\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0277 - mean_squared_error: 2.2432\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0068 - mean_squared_error: 1.9564\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9975 - mean_squared_error: 2.0394\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0404 - mean_squared_error: 2.2080\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9956 - mean_squared_error: 2.0748\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0070 - mean_squared_error: 2.1421\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9944 - mean_squared_error: 2.0038\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0670 - mean_squared_error: 2.3270\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9867 - mean_squared_error: 1.9878\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0129 - mean_squared_error: 2.0850\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0113 - mean_squared_error: 2.0965\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9720 - mean_squared_error: 1.9463\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0368 - mean_squared_error: 2.1416\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0117 - mean_squared_error: 2.2217\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0264 - mean_squared_error: 2.1194\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0061 - mean_squared_error: 2.0871\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9846 - mean_squared_error: 1.9849\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0311 - mean_squared_error: 2.2243\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9479 - mean_squared_error: 1.8154\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0206 - mean_squared_error: 2.2372\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9917 - mean_squared_error: 2.0179\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9567 - mean_squared_error: 1.8848\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0325 - mean_squared_error: 2.1396\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9926 - mean_squared_error: 2.0730\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9775 - mean_squared_error: 2.1245\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9675 - mean_squared_error: 1.9887\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0089 - mean_squared_error: 2.1186\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9667 - mean_squared_error: 1.9342\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9998 - mean_squared_error: 2.2536\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9687 - mean_squared_error: 1.9113\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0057 - mean_squared_error: 2.1105\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9685 - mean_squared_error: 1.9118\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9774 - mean_squared_error: 2.1191\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9851 - mean_squared_error: 2.0677\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9692 - mean_squared_error: 2.0521\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9654 - mean_squared_error: 1.9805\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9796 - mean_squared_error: 1.9702\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9785 - mean_squared_error: 2.1499\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9717 - mean_squared_error: 2.0270\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9692 - mean_squared_error: 2.0083\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9652 - mean_squared_error: 2.0294\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 0.9541 - mean_squared_error: 2.0278\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9665 - mean_squared_error: 2.0122\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9543 - mean_squared_error: 2.0127\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9709 - mean_squared_error: 2.0572\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9530 - mean_squared_error: 1.9868\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9952 - mean_squared_error: 2.1008\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9611 - mean_squared_error: 1.9525\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9776 - mean_squared_error: 2.0034\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9845 - mean_squared_error: 2.1145\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9800 - mean_squared_error: 1.9914\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9549 - mean_squared_error: 2.0958\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9741 - mean_squared_error: 2.0765\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9707 - mean_squared_error: 2.0087\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9374 - mean_squared_error: 1.8136\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9638 - mean_squared_error: 2.0200\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9796 - mean_squared_error: 2.1098\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9498 - mean_squared_error: 1.9540\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9847 - mean_squared_error: 2.1848\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9559 - mean_squared_error: 1.9552\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 0.9448 - mean_squared_error: 1.8844\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9506 - mean_squared_error: 2.1077\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9336 - mean_squared_error: 1.8018\n",
      "Epoch 71/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9822 - mean_squared_error: 2.2750\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9462 - mean_squared_error: 2.0362\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9252 - mean_squared_error: 1.8957\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9592 - mean_squared_error: 2.0125\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9673 - mean_squared_error: 2.1065\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9386 - mean_squared_error: 1.9045\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9306 - mean_squared_error: 1.8559\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9591 - mean_squared_error: 2.0697\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9710 - mean_squared_error: 2.0962\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9138 - mean_squared_error: 1.7766\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9837 - mean_squared_error: 2.2769\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9514 - mean_squared_error: 2.0018\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9349 - mean_squared_error: 1.9110\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9488 - mean_squared_error: 2.0379\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9417 - mean_squared_error: 2.0723\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9470 - mean_squared_error: 2.0191\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 0.9045 - mean_squared_error: 1.7892\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9964 - mean_squared_error: 2.1983\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.9210 - mean_squared_error: 1.8977\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9407 - mean_squared_error: 2.0182\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9533 - mean_squared_error: 1.9828\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9621 - mean_squared_error: 2.1063\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9318 - mean_squared_error: 1.9797\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9567 - mean_squared_error: 2.0152\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9619 - mean_squared_error: 2.0284\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9612 - mean_squared_error: 1.9995\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9579 - mean_squared_error: 2.0157\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 9s 7ms/sample - loss: 0.9529 - mean_squared_error: 2.0106\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9561 - mean_squared_error: 2.0355\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 0.9422 - mean_squared_error: 2.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:57:24.872491: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_118292' and '__inference_standard_lstm_118181_specialized_for_model_8_lstm_8_StatefulPartitionedCall_at___inference_distributed_function_118532' both implement 'lstm_f4e1fdf5-c031-497f-b0f8-f63237e37e53' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 1.2306 - mean_squared_error: 1.8863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0403979792662545, 1.8863058]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0403979792662545, 1.8863058]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 16:57:31.759752: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_121569_122054_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_122227' and '__inference___backward_cudnn_lstm_with_fallback_121263_121445' both implement 'lstm_8039e148-f607-41a1-8369-115303c585e5' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 19s 16ms/sample - loss: 1.1464 - mean_squared_error: 2.5365\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0664 - mean_squared_error: 2.1787\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0549 - mean_squared_error: 2.1754\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1064 - mean_squared_error: 2.3500\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1187 - mean_squared_error: 2.3425\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0790 - mean_squared_error: 2.2836\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0522 - mean_squared_error: 2.1979\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0671 - mean_squared_error: 2.2623\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0750 - mean_squared_error: 2.2918\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0216 - mean_squared_error: 2.0472\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0743 - mean_squared_error: 2.2837\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0131 - mean_squared_error: 2.0715\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0540 - mean_squared_error: 2.2115\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0380 - mean_squared_error: 2.1143\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0555 - mean_squared_error: 2.2905\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9941 - mean_squared_error: 2.0135\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0061 - mean_squared_error: 2.0266\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0525 - mean_squared_error: 2.2949\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0361 - mean_squared_error: 2.1330\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0349 - mean_squared_error: 2.1784\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0182 - mean_squared_error: 1.9514\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9947 - mean_squared_error: 2.1322\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0145 - mean_squared_error: 2.0841\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0376 - mean_squared_error: 2.1806\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9998 - mean_squared_error: 1.9883\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0042 - mean_squared_error: 2.1648\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0309 - mean_squared_error: 2.0355\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0507 - mean_squared_error: 2.2796\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 0.9914 - mean_squared_error: 1.9324\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 11s 9ms/sample - loss: 1.0506 - mean_squared_error: 2.3442\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 0.9837 - mean_squared_error: 2.0082\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0032 - mean_squared_error: 1.9888\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9912 - mean_squared_error: 2.1009\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 0.9503 - mean_squared_error: 1.8050\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0542 - mean_squared_error: 2.2852\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 0.9798 - mean_squared_error: 1.9446\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 9s 7ms/sample - loss: 1.0079 - mean_squared_error: 2.1933\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 9s 8ms/sample - loss: 1.0031 - mean_squared_error: 2.1160\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0141 - mean_squared_error: 2.0289\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0406 - mean_squared_error: 2.1900\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.0292 - mean_squared_error: 2.0653\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0377 - mean_squared_error: 2.1985\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0523 - mean_squared_error: 2.0611\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0370 - mean_squared_error: 2.2442\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0312 - mean_squared_error: 2.1373\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0186 - mean_squared_error: 2.1018\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0103 - mean_squared_error: 2.0179\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0178 - mean_squared_error: 2.1588\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0006 - mean_squared_error: 2.1097\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0061 - mean_squared_error: 2.0932\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0025 - mean_squared_error: 2.0871\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9876 - mean_squared_error: 2.0564\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0748 - mean_squared_error: 2.3112\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1058 - mean_squared_error: 2.3601\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0982 - mean_squared_error: 2.3755\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0621 - mean_squared_error: 2.1497\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0511 - mean_squared_error: 2.2451\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0128 - mean_squared_error: 2.0649\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0526 - mean_squared_error: 2.1708\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0342 - mean_squared_error: 2.1808\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0014 - mean_squared_error: 1.9928\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0626 - mean_squared_error: 2.3413\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0286 - mean_squared_error: 2.1403\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0174 - mean_squared_error: 2.0319\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0143 - mean_squared_error: 2.1007\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0192 - mean_squared_error: 2.0813\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1402 - mean_squared_error: 2.6003\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0483 - mean_squared_error: 1.9950\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0284 - mean_squared_error: 2.2115\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0202 - mean_squared_error: 2.0745\n",
      "Epoch 71/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0354 - mean_squared_error: 2.0439\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0342 - mean_squared_error: 2.2447\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0455 - mean_squared_error: 2.2128\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0030 - mean_squared_error: 2.0434\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0648 - mean_squared_error: 2.2511\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0256 - mean_squared_error: 2.1910\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0029 - mean_squared_error: 2.0382\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0018 - mean_squared_error: 2.0075\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0353 - mean_squared_error: 2.1167\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9907 - mean_squared_error: 1.9587\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0475 - mean_squared_error: 2.2490\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0377 - mean_squared_error: 2.1422\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0222 - mean_squared_error: 1.9813\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0264 - mean_squared_error: 2.2730\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0168 - mean_squared_error: 2.1690\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9957 - mean_squared_error: 1.9696\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0392 - mean_squared_error: 2.2627\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0317 - mean_squared_error: 2.0369\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0101 - mean_squared_error: 2.1016\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0100 - mean_squared_error: 2.0671\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0449 - mean_squared_error: 2.2650\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0293 - mean_squared_error: 2.1416\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0336 - mean_squared_error: 2.0853\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0070 - mean_squared_error: 2.1429\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0201 - mean_squared_error: 2.0851\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0072 - mean_squared_error: 2.1084\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0058 - mean_squared_error: 2.0867\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0007 - mean_squared_error: 2.0820\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0026 - mean_squared_error: 2.0672\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 0.9993 - mean_squared_error: 2.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:10:22.977341: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_137694_specialized_for_model_9_lstm_9_StatefulPartitionedCall_at___inference_distributed_function_138045' and '__inference_cudnn_lstm_with_fallback_137805' both implement 'lstm_ba389048-02bc-409a-9953-c7a739579dbe' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 1.2334 - mean_squared_error: 1.8434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0343166961737558, 1.8434093]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0343166961737558, 1.8434093]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:10:26.129923: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_140776_140958' and '__inference___backward_standard_lstm_141082_141567_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_141740' both implement 'lstm_b2ac370b-fc9a-4987-8ca8-1e6419eca5a8' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.1757 - mean_squared_error: 2.5349\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0659 - mean_squared_error: 2.2342\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0538 - mean_squared_error: 2.1692\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0631 - mean_squared_error: 2.2460\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0207 - mean_squared_error: 2.0962\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0096 - mean_squared_error: 2.0112\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0558 - mean_squared_error: 2.2017\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0487 - mean_squared_error: 2.1878\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.1277 - mean_squared_error: 2.3805\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0587 - mean_squared_error: 2.1715\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0497 - mean_squared_error: 2.1714\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 5s 4ms/sample - loss: 1.0691 - mean_squared_error: 2.2447\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0795 - mean_squared_error: 2.3965\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0182 - mean_squared_error: 1.9524\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0405 - mean_squared_error: 2.2420\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0364 - mean_squared_error: 2.0768\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1116 - mean_squared_error: 2.3774\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0992 - mean_squared_error: 2.4099\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0459 - mean_squared_error: 2.1689\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0542 - mean_squared_error: 2.0668\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0856 - mean_squared_error: 2.2352\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1749 - mean_squared_error: 2.5857\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0850 - mean_squared_error: 2.3094\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0855 - mean_squared_error: 2.2682\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0713 - mean_squared_error: 2.1625\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1245 - mean_squared_error: 2.3069\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1336 - mean_squared_error: 2.5850\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0824 - mean_squared_error: 2.2084\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0818 - mean_squared_error: 2.1947\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0888 - mean_squared_error: 2.3252\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0631 - mean_squared_error: 2.1879\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0763 - mean_squared_error: 2.3123\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1868 - mean_squared_error: 2.4972\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.2773 - mean_squared_error: 2.5984\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1089 - mean_squared_error: 2.4132\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1304 - mean_squared_error: 2.3723\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1249 - mean_squared_error: 2.3719\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0845 - mean_squared_error: 2.2760\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.1149 - mean_squared_error: 2.3618\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0977 - mean_squared_error: 2.3151\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0735 - mean_squared_error: 2.1875\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0625 - mean_squared_error: 2.2281\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0572 - mean_squared_error: 2.1376\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0748 - mean_squared_error: 2.3392\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0681 - mean_squared_error: 2.2404\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1412 - mean_squared_error: 2.4842\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0939 - mean_squared_error: 2.2737\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0597 - mean_squared_error: 2.1656\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.0520 - mean_squared_error: 2.1824\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0533 - mean_squared_error: 2.2313\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0566 - mean_squared_error: 2.1674\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0543 - mean_squared_error: 2.1782\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0464 - mean_squared_error: 2.1135\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0524 - mean_squared_error: 2.2017\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0565 - mean_squared_error: 2.2231\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0418 - mean_squared_error: 2.1779\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0671 - mean_squared_error: 2.2103\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0809 - mean_squared_error: 2.2156\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0688 - mean_squared_error: 2.2327\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0646 - mean_squared_error: 2.1587\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0678 - mean_squared_error: 2.2121\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0649 - mean_squared_error: 2.2871\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0513 - mean_squared_error: 2.0349\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1657 - mean_squared_error: 2.5927\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0895 - mean_squared_error: 2.2925\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1138 - mean_squared_error: 2.3419\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0995 - mean_squared_error: 2.3331\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0562 - mean_squared_error: 2.1449\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0884 - mean_squared_error: 2.2764\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0634 - mean_squared_error: 2.2783\n",
      "Epoch 71/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0535 - mean_squared_error: 2.0757\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0815 - mean_squared_error: 2.3692\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1187 - mean_squared_error: 2.4683\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0264 - mean_squared_error: 1.9476\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0820 - mean_squared_error: 2.2717\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0628 - mean_squared_error: 2.1296\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0852 - mean_squared_error: 2.3410\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0809 - mean_squared_error: 2.2738\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.0486 - mean_squared_error: 2.1757\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1103 - mean_squared_error: 2.3176\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0825 - mean_squared_error: 2.1885\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0701 - mean_squared_error: 2.2148\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0866 - mean_squared_error: 2.2779\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0903 - mean_squared_error: 2.2816\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0765 - mean_squared_error: 2.3663\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.2651 - mean_squared_error: 2.6886\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1891 - mean_squared_error: 2.6297\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0533 - mean_squared_error: 2.1715\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0793 - mean_squared_error: 2.1982\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0530 - mean_squared_error: 2.2333\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0642 - mean_squared_error: 2.2027\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0450 - mean_squared_error: 2.1570\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.1618\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0712 - mean_squared_error: 2.2257\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0702 - mean_squared_error: 2.2152\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1098 - mean_squared_error: 2.4231\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.3006 - mean_squared_error: 3.1333\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 6s 5ms/sample - loss: 1.2928 - mean_squared_error: 2.9270\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.3637 - mean_squared_error: 3.0643\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 6s 4ms/sample - loss: 1.3537 - mean_squared_error: 3.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:20:17.138647: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_157207' and '__inference_standard_lstm_157207_specialized_for_model_10_lstm_10_StatefulPartitionedCall_at___inference_distributed_function_157558' both implement 'lstm_5909eb33-7209-4ef2-a426-b041f0c23ee9' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 4ms/sample - loss: 1.3647 - mean_squared_error: 2.6459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3403674930545455, 2.6459386]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.3403674930545455, 2.6459386]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 50, 128)\n",
      "LSTM:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 1)\n",
      "Train on 1208 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:20:22.224905: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_160595_161080' and '__inference___backward_standard_lstm_160595_161080_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_161253' both implement 'lstm_272e8383-0155-4fa0-9c07-6dc714f5d585' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1208 [==============================] - 10s 8ms/sample - loss: 1.1269 - mean_squared_error: 2.3878\n",
      "Epoch 2/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0933 - mean_squared_error: 2.2779\n",
      "Epoch 3/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0794 - mean_squared_error: 2.2517\n",
      "Epoch 4/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0599 - mean_squared_error: 2.2046\n",
      "Epoch 5/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0495 - mean_squared_error: 2.1546\n",
      "Epoch 6/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0485 - mean_squared_error: 2.1820\n",
      "Epoch 7/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0508 - mean_squared_error: 2.1778\n",
      "Epoch 8/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0324 - mean_squared_error: 2.1668\n",
      "Epoch 9/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0375 - mean_squared_error: 2.1081\n",
      "Epoch 10/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0222 - mean_squared_error: 2.1454\n",
      "Epoch 11/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0531 - mean_squared_error: 2.2229\n",
      "Epoch 12/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0195 - mean_squared_error: 2.0971\n",
      "Epoch 13/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 0.9662 - mean_squared_error: 1.8381\n",
      "Epoch 14/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0628 - mean_squared_error: 2.3868\n",
      "Epoch 15/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9861 - mean_squared_error: 1.9707\n",
      "Epoch 16/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0051 - mean_squared_error: 2.0772\n",
      "Epoch 17/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 0.9979 - mean_squared_error: 2.1259\n",
      "Epoch 18/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0199 - mean_squared_error: 2.0438\n",
      "Epoch 19/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0462 - mean_squared_error: 2.3392\n",
      "Epoch 20/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0353 - mean_squared_error: 2.1282\n",
      "Epoch 21/100\n",
      "1233/1208 [==============================] - 71s 57ms/sample - loss: 1.0419 - mean_squared_error: 2.0659\n",
      "Epoch 22/100\n",
      "1233/1208 [==============================] - 30s 24ms/sample - loss: 1.0455 - mean_squared_error: 2.2522\n",
      "Epoch 23/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0530 - mean_squared_error: 2.2388\n",
      "Epoch 24/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0259 - mean_squared_error: 1.9229\n",
      "Epoch 25/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0612 - mean_squared_error: 2.1623\n",
      "Epoch 26/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1482 - mean_squared_error: 2.6546\n",
      "Epoch 27/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1645 - mean_squared_error: 2.5342\n",
      "Epoch 28/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1719 - mean_squared_error: 2.4014\n",
      "Epoch 29/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1198 - mean_squared_error: 2.3112\n",
      "Epoch 30/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1044 - mean_squared_error: 2.4268\n",
      "Epoch 31/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1132 - mean_squared_error: 2.3729\n",
      "Epoch 32/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1293 - mean_squared_error: 2.4087\n",
      "Epoch 33/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0762 - mean_squared_error: 2.1248\n",
      "Epoch 34/100\n",
      "1233/1208 [==============================] - 8s 7ms/sample - loss: 1.0590 - mean_squared_error: 2.3027\n",
      "Epoch 35/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.0062 - mean_squared_error: 1.9785\n",
      "Epoch 36/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1019 - mean_squared_error: 2.2999\n",
      "Epoch 37/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0476 - mean_squared_error: 2.1515\n",
      "Epoch 38/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1266 - mean_squared_error: 2.4180\n",
      "Epoch 39/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0992 - mean_squared_error: 2.3051\n",
      "Epoch 40/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0637 - mean_squared_error: 2.1883\n",
      "Epoch 41/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1242 - mean_squared_error: 2.4275\n",
      "Epoch 42/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0865 - mean_squared_error: 2.2225\n",
      "Epoch 43/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0894 - mean_squared_error: 2.2624\n",
      "Epoch 44/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0813 - mean_squared_error: 2.2628\n",
      "Epoch 45/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0473 - mean_squared_error: 2.1392\n",
      "Epoch 46/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0501 - mean_squared_error: 2.1870\n",
      "Epoch 47/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0594 - mean_squared_error: 2.1832\n",
      "Epoch 48/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0509 - mean_squared_error: 2.1476\n",
      "Epoch 49/100\n",
      "1216/1208 [==============================] - 7s 5ms/sample - loss: 1.0576 - mean_squared_error: 2.2117\n",
      "Epoch 50/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0654 - mean_squared_error: 2.2065\n",
      "Epoch 51/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0714 - mean_squared_error: 2.2293\n",
      "Epoch 52/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0591 - mean_squared_error: 2.2572\n",
      "Epoch 53/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1129 - mean_squared_error: 2.2878\n",
      "Epoch 54/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0679 - mean_squared_error: 2.1887\n",
      "Epoch 55/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0825 - mean_squared_error: 2.2912\n",
      "Epoch 56/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0473 - mean_squared_error: 2.1242\n",
      "Epoch 57/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0522 - mean_squared_error: 2.1662\n",
      "Epoch 58/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0684 - mean_squared_error: 2.3440\n",
      "Epoch 59/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0882 - mean_squared_error: 2.3074\n",
      "Epoch 60/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0534 - mean_squared_error: 2.1052\n",
      "Epoch 61/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0412 - mean_squared_error: 2.1014\n",
      "Epoch 62/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0894 - mean_squared_error: 2.4105\n",
      "Epoch 63/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0524 - mean_squared_error: 2.1827\n",
      "Epoch 64/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0759 - mean_squared_error: 2.2213\n",
      "Epoch 65/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0625 - mean_squared_error: 2.1153\n",
      "Epoch 66/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0488 - mean_squared_error: 2.2656\n",
      "Epoch 67/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0364 - mean_squared_error: 2.0274\n",
      "Epoch 68/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1981 - mean_squared_error: 2.6320\n",
      "Epoch 69/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1484 - mean_squared_error: 2.3493\n",
      "Epoch 70/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3062 - mean_squared_error: 3.1107\n",
      "Epoch 71/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.3050 - mean_squared_error: 2.8130\n",
      "Epoch 72/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.4055 - mean_squared_error: 3.2624\n",
      "Epoch 73/100\n",
      "1233/1208 [==============================] - 8s 6ms/sample - loss: 1.3265 - mean_squared_error: 2.8813\n",
      "Epoch 74/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1876 - mean_squared_error: 2.6424\n",
      "Epoch 75/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0652 - mean_squared_error: 2.1083\n",
      "Epoch 76/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0867 - mean_squared_error: 2.2490\n",
      "Epoch 77/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0873 - mean_squared_error: 2.2867\n",
      "Epoch 78/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0716 - mean_squared_error: 2.1055\n",
      "Epoch 79/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0990 - mean_squared_error: 2.4624\n",
      "Epoch 80/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0568 - mean_squared_error: 2.0658\n",
      "Epoch 81/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0986 - mean_squared_error: 2.3940\n",
      "Epoch 82/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0853 - mean_squared_error: 2.3373\n",
      "Epoch 83/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1128 - mean_squared_error: 2.3913\n",
      "Epoch 84/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0388 - mean_squared_error: 2.0394\n",
      "Epoch 85/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.1033 - mean_squared_error: 2.3784\n",
      "Epoch 86/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1016 - mean_squared_error: 2.2333\n",
      "Epoch 87/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0684 - mean_squared_error: 2.2714\n",
      "Epoch 88/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1099 - mean_squared_error: 2.3303\n",
      "Epoch 89/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0824 - mean_squared_error: 2.2619\n",
      "Epoch 90/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0536 - mean_squared_error: 2.2070\n",
      "Epoch 91/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0408 - mean_squared_error: 2.1169\n",
      "Epoch 92/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0643 - mean_squared_error: 2.2156\n",
      "Epoch 93/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0937 - mean_squared_error: 2.2843\n",
      "Epoch 94/100\n",
      "1233/1208 [==============================] - 6s 5ms/sample - loss: 1.0810 - mean_squared_error: 2.1785\n",
      "Epoch 95/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.1877 - mean_squared_error: 2.6210\n",
      "Epoch 96/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.1083 - mean_squared_error: 2.3255\n",
      "Epoch 97/100\n",
      "1233/1208 [==============================] - 7s 6ms/sample - loss: 1.0906 - mean_squared_error: 2.2710\n",
      "Epoch 98/100\n",
      "1216/1208 [==============================] - 7s 6ms/sample - loss: 1.0723 - mean_squared_error: 2.2492\n",
      "Epoch 99/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0737 - mean_squared_error: 2.2275\n",
      "Epoch 100/100\n",
      "1233/1208 [==============================] - 7s 5ms/sample - loss: 1.0704 - mean_squared_error: 2.2341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:33:07.857121: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_176720_specialized_for_model_11_lstm_11_StatefulPartitionedCall_at___inference_distributed_function_177071' and '__inference_standard_lstm_176720' both implement 'lstm_474204ea-a89d-47cf-a8c8-7563773e6996' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "282/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 1.2583 - mean_squared_error: 1.9554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0799023783798758, 1.9554123]\n",
      "===== Summary =====\n",
      "Epoch:  100\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  MAE\n",
      "Metrics:  [<tensorflow.python.keras.metrics.MeanSquaredError object at 0x7fb4541f92d0>]\n",
      "Validation:  [1.0799023783798758, 1.9554123]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 17:33:12.079012: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model/cnn_lstm_best/assets\n"
     ]
    }
   ],
   "source": [
    "loss_list = [\"MAE\"]\n",
    "optimizer_list = [\"Adam\"]\n",
    "epoch_list = [30,50,100]\n",
    "batch_list = [50]\n",
    "encoder_list = [50,100]\n",
    "lr_list = [0.005,0.01]\n",
    "train_df = pd.DataFrame(columns = [\"Epoch\",\"Batch\",\"Optimizer\",\"LR\",\"Encoder Unit\",\"Loss\",\"Metrics\",\"Validation\"])\n",
    "best_model = \"\"\n",
    "best_valid = 99999\n",
    "metrics = [keras.metrics.MeanSquaredError()]\n",
    "\n",
    "for los in loss_list:\n",
    "    for opti in optimizer_list:\n",
    "        for epochs in epoch_list:\n",
    "            for batchs in batch_list:\n",
    "                for lr in lr_list:\n",
    "                    for encoder_u in encoder_list:\n",
    "\n",
    "                        model = myModel(input_shape=(num_day_to_predict,train_X.shape[1],1),\n",
    "                                        encoder_unit = encoder_u,\n",
    "                                        repeat_vector_n = 50\n",
    "                                       )\n",
    "\n",
    "                        if opti == \"Adam\":\n",
    "                            optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "                        if los == \"MAE\":\n",
    "                            loss = keras.losses.MeanAbsoluteError()\n",
    "                        elif los == \"MSE\":\n",
    "                            loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "                        model.compile(\n",
    "                            optimizer=optimizer,\n",
    "                            loss=loss,\n",
    "                            metrics=metrics,\n",
    "                        )\n",
    "\n",
    "                        history = model.fit(\n",
    "                                tf_train_X,\n",
    "                                tf_train_y,\n",
    "                                epochs = epochs,\n",
    "                                steps_per_epoch = batchs,\n",
    "                            )\n",
    "\n",
    "                        results = model.evaluate(tf_valid_X, tf_valid_y, batch_size=batchs)\n",
    "                        print(results)\n",
    "                        print(\"===== Summary =====\")\n",
    "                        print(\"Epoch: \",epochs)\n",
    "                        print(\"Batch Size: \",batchs)\n",
    "                        print(\"Optimizer: \",opti)\n",
    "                        print(\"Learning Rate: \",lr)\n",
    "                        print(\"Encoder Units: \",encoder_u)\n",
    "                        print(\"Loss Function: \", los)\n",
    "                        print(\"Metrics: \", metrics)\n",
    "                        print(\"Validation: \",results)\n",
    "                        if results[0] < best_valid:\n",
    "                            best_valid = results[0]\n",
    "                            best_model = model\n",
    "                        train_df = train_df.append({\"Epoch\": epochs,\n",
    "                                                    \"Batch\": batchs,\n",
    "                                                    \"Optimizer\": opti,\n",
    "                                                    \"LR\": lr,\n",
    "                                                    \"Encoder Unit\": encoder_u,\n",
    "                                                    \"Loss\": los,\n",
    "                                                    \"Metrics\": metrics,\n",
    "                                                    \"Validation\":results}, ignore_index=True)\n",
    "best_model.save(\"model/cnn_lstm_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1049860",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by=[\"Validation\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6201ca",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c93eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (495, 1)\n",
      "[[9.99935865e-01]\n",
      " [9.99999225e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999762e-01]\n",
      " [6.37081385e-01]\n",
      " [1.27184570e-01]\n",
      " [3.74007225e-03]\n",
      " [9.98438239e-01]\n",
      " [9.99805331e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998987e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99991655e-01]\n",
      " [9.06998873e-01]\n",
      " [9.24459994e-02]\n",
      " [4.23881710e-02]\n",
      " [3.89069319e-04]\n",
      " [8.59656632e-02]\n",
      " [7.81863928e-04]\n",
      " [2.86102295e-06]\n",
      " [2.23398209e-02]\n",
      " [9.99546766e-01]\n",
      " [9.99998808e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99998331e-01]\n",
      " [9.99999523e-01]\n",
      " [9.13837552e-01]\n",
      " [9.16434944e-01]\n",
      " [9.99710798e-01]\n",
      " [9.24031019e-01]\n",
      " [9.43064690e-04]\n",
      " [1.49011612e-07]\n",
      " [1.19209290e-07]\n",
      " [9.98399138e-01]\n",
      " [9.99970555e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999106e-01]\n",
      " [9.99998510e-01]\n",
      " [9.99938130e-01]\n",
      " [9.99997556e-01]\n",
      " [9.99997616e-01]\n",
      " [9.99999642e-01]\n",
      " [9.98965144e-01]\n",
      " [6.97687507e-01]\n",
      " [9.50902700e-04]\n",
      " [9.53674316e-07]\n",
      " [5.66244125e-07]\n",
      " [1.90448582e-01]\n",
      " [1.18739009e-02]\n",
      " [9.23145413e-01]\n",
      " [9.99983013e-01]\n",
      " [9.99992788e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999225e-01]\n",
      " [9.91631269e-01]\n",
      " [5.14423966e-01]\n",
      " [2.77360678e-02]\n",
      " [4.40082550e-01]\n",
      " [9.99997258e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998808e-01]\n",
      " [8.12213242e-01]\n",
      " [5.83559275e-04]\n",
      " [7.98702240e-06]\n",
      " [3.57627869e-07]\n",
      " [9.99999523e-01]\n",
      " [9.99993324e-01]\n",
      " [9.99998748e-01]\n",
      " [6.45449758e-03]\n",
      " [2.08616257e-07]\n",
      " [2.98023224e-08]\n",
      " [3.27825546e-07]\n",
      " [3.57627869e-07]\n",
      " [1.46031380e-06]\n",
      " [9.23871994e-07]\n",
      " [6.55651093e-07]\n",
      " [1.31130219e-06]\n",
      " [2.81035900e-05]\n",
      " [2.98023224e-08]\n",
      " [3.27825546e-07]\n",
      " [1.49011612e-07]\n",
      " [2.98023224e-07]\n",
      " [6.25848770e-07]\n",
      " [7.15255737e-07]\n",
      " [0.00000000e+00]\n",
      " [5.17010689e-04]\n",
      " [1.72853470e-06]\n",
      " [1.19209290e-07]\n",
      " [9.99975681e-01]\n",
      " [9.99999702e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99942124e-01]\n",
      " [9.99989033e-01]\n",
      " [9.95539784e-01]\n",
      " [9.99063969e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99608994e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99970734e-01]\n",
      " [9.83703613e-01]\n",
      " [9.99796629e-01]\n",
      " [9.99487102e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99979854e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99993145e-01]\n",
      " [2.48402357e-04]\n",
      " [4.98682261e-04]\n",
      " [7.30454922e-04]\n",
      " [2.68518925e-05]\n",
      " [6.20321631e-01]\n",
      " [9.99715328e-01]\n",
      " [9.99946952e-01]\n",
      " [1.06104612e-02]\n",
      " [9.99997973e-01]\n",
      " [6.81582987e-02]\n",
      " [9.97015059e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999404e-01]\n",
      " [9.75770473e-01]\n",
      " [7.38902867e-01]\n",
      " [9.98165905e-01]\n",
      " [5.53074479e-02]\n",
      " [9.99999642e-01]\n",
      " [9.99999285e-01]\n",
      " [2.37095356e-03]\n",
      " [2.98023224e-08]\n",
      " [1.49011612e-07]\n",
      " [1.49011612e-07]\n",
      " [1.67340040e-04]\n",
      " [1.21721625e-03]\n",
      " [1.78813934e-07]\n",
      " [9.99997973e-01]\n",
      " [6.87472165e-01]\n",
      " [2.57310510e-01]\n",
      " [9.99998927e-01]\n",
      " [9.91830587e-01]\n",
      " [9.99952376e-01]\n",
      " [9.99989986e-01]\n",
      " [9.99883771e-01]\n",
      " [5.66959381e-04]\n",
      " [5.51177800e-01]\n",
      " [4.34845686e-04]\n",
      " [1.69873238e-06]\n",
      " [4.42332834e-01]\n",
      " [3.55243683e-05]\n",
      " [3.74317169e-05]\n",
      " [5.43294728e-01]\n",
      " [9.99906898e-01]\n",
      " [9.99999225e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999404e-01]\n",
      " [9.98997688e-01]\n",
      " [1.77279711e-01]\n",
      " [1.24043226e-03]\n",
      " [6.50286674e-04]\n",
      " [3.17756414e-01]\n",
      " [5.81145287e-06]\n",
      " [1.55031681e-04]\n",
      " [2.08616257e-07]\n",
      " [3.69411707e-03]\n",
      " [2.98771650e-01]\n",
      " [4.09215689e-04]\n",
      " [9.99999046e-01]\n",
      " [9.99999642e-01]\n",
      " [5.41959107e-02]\n",
      " [8.43653083e-03]\n",
      " [9.99999583e-01]\n",
      " [8.32157493e-01]\n",
      " [9.99999523e-01]\n",
      " [4.68289703e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99158382e-01]\n",
      " [9.99978721e-01]\n",
      " [9.99445081e-01]\n",
      " [9.99998569e-01]\n",
      " [9.99990582e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99999523e-01]\n",
      " [9.40379202e-02]\n",
      " [3.96071970e-02]\n",
      " [9.98443127e-01]\n",
      " [9.99976397e-01]\n",
      " [9.99999225e-01]\n",
      " [9.99999523e-01]\n",
      " [7.20288277e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99971628e-01]\n",
      " [9.99997675e-01]\n",
      " [9.99796450e-01]\n",
      " [9.99995232e-01]\n",
      " [9.99960005e-01]\n",
      " [9.99993265e-01]\n",
      " [9.99999404e-01]\n",
      " [9.92240906e-01]\n",
      " [4.33146954e-03]\n",
      " [8.85128975e-06]\n",
      " [1.49011612e-07]\n",
      " [9.55969095e-04]\n",
      " [8.31189156e-01]\n",
      " [9.99866486e-01]\n",
      " [9.99998152e-01]\n",
      " [3.37362289e-04]\n",
      " [2.38418579e-07]\n",
      " [1.19209290e-07]\n",
      " [4.94718552e-06]\n",
      " [9.98902261e-01]\n",
      " [9.99970973e-01]\n",
      " [6.03388190e-01]\n",
      " [9.99989092e-01]\n",
      " [9.98977900e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99999464e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99785304e-01]\n",
      " [9.99968052e-01]\n",
      " [9.99944389e-01]\n",
      " [9.99317408e-01]\n",
      " [9.99997735e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99997497e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99996483e-01]\n",
      " [9.68820572e-01]\n",
      " [5.88398814e-01]\n",
      " [8.73096466e-01]\n",
      " [9.99987900e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99988616e-01]\n",
      " [3.55830848e-01]\n",
      " [8.74320805e-01]\n",
      " [5.73018253e-01]\n",
      " [7.54948854e-01]\n",
      " [4.35537696e-01]\n",
      " [9.97424245e-01]\n",
      " [9.99998629e-01]\n",
      " [5.96046448e-08]\n",
      " [6.02461934e-01]\n",
      " [9.86331642e-01]\n",
      " [9.98135567e-01]\n",
      " [5.14745712e-04]\n",
      " [2.02655792e-06]\n",
      " [1.69873238e-06]\n",
      " [5.42402267e-06]\n",
      " [9.99920607e-01]\n",
      " [9.79900360e-05]\n",
      " [8.10027122e-05]\n",
      " [2.57581472e-04]\n",
      " [8.19458067e-01]\n",
      " [9.99995708e-01]\n",
      " [2.47942537e-01]\n",
      " [3.21865082e-06]\n",
      " [2.97944129e-01]\n",
      " [2.05636024e-06]\n",
      " [5.03063202e-04]\n",
      " [1.19209290e-07]\n",
      " [8.94069672e-08]\n",
      " [8.94069672e-08]\n",
      " [0.00000000e+00]\n",
      " [8.94069672e-08]\n",
      " [4.56243753e-04]\n",
      " [1.15470290e-02]\n",
      " [1.74678564e-01]\n",
      " [5.44205308e-03]\n",
      " [4.21404839e-05]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [5.00679016e-06]\n",
      " [1.46031380e-06]\n",
      " [1.15092099e-01]\n",
      " [2.54631042e-03]\n",
      " [3.84449959e-06]\n",
      " [6.16610050e-04]\n",
      " [9.23871994e-07]\n",
      " [1.19209290e-07]\n",
      " [8.94069672e-08]\n",
      " [4.52092916e-01]\n",
      " [4.78000343e-02]\n",
      " [8.84959698e-01]\n",
      " [1.48397893e-01]\n",
      " [9.97308612e-01]\n",
      " [1.81452900e-01]\n",
      " [1.57952309e-06]\n",
      " [2.47776508e-04]\n",
      " [7.16149807e-05]\n",
      " [4.05087233e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99477506e-01]\n",
      " [9.99986410e-01]\n",
      " [9.99997854e-01]\n",
      " [9.99992847e-01]\n",
      " [9.99901533e-01]\n",
      " [9.98796105e-01]\n",
      " [6.41970336e-02]\n",
      " [2.86470056e-02]\n",
      " [1.48394644e-01]\n",
      " [5.71133792e-02]\n",
      " [3.03461134e-01]\n",
      " [3.97369623e-01]\n",
      " [6.85888529e-03]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99998689e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99997854e-01]\n",
      " [9.99996662e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99998927e-01]\n",
      " [9.99998450e-01]\n",
      " [5.36937773e-01]\n",
      " [2.90410250e-01]\n",
      " [2.71113336e-01]\n",
      " [8.16369534e-01]\n",
      " [6.70991898e-01]\n",
      " [9.99994040e-01]\n",
      " [9.95961189e-01]\n",
      " [9.96407986e-01]\n",
      " [9.73135591e-01]\n",
      " [7.25436330e-01]\n",
      " [8.56520891e-01]\n",
      " [9.99418020e-01]\n",
      " [9.99875665e-01]\n",
      " [9.99864697e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99856949e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99998331e-01]\n",
      " [9.97941494e-01]\n",
      " [9.99999225e-01]\n",
      " [7.64110684e-01]\n",
      " [9.99954700e-01]\n",
      " [9.98729825e-01]\n",
      " [4.91828799e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99999642e-01]\n",
      " [9.99999046e-01]\n",
      " [4.53310341e-01]\n",
      " [5.37095666e-02]\n",
      " [7.70688057e-05]\n",
      " [4.22984362e-04]\n",
      " [9.00804520e-01]\n",
      " [3.12084675e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99996603e-01]\n",
      " [9.99997139e-01]\n",
      " [9.99960542e-01]\n",
      " [9.99999285e-01]\n",
      " [9.99988317e-01]\n",
      " [7.63323903e-03]\n",
      " [9.97866273e-01]\n",
      " [6.60747290e-04]\n",
      " [4.51971591e-01]\n",
      " [5.96046448e-08]\n",
      " [8.94069672e-08]\n",
      " [8.64267349e-07]\n",
      " [1.34408474e-05]\n",
      " [1.13811493e-02]\n",
      " [6.04096830e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99935448e-01]\n",
      " [9.99943197e-01]\n",
      " [6.72018826e-01]\n",
      " [7.35868216e-01]\n",
      " [9.99786615e-01]\n",
      " [9.99997914e-01]\n",
      " [1.64497256e-01]\n",
      " [6.85726166e-01]\n",
      " [9.99857426e-01]\n",
      " [9.99964833e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999166e-01]\n",
      " [9.99999106e-01]\n",
      " [9.99990284e-01]\n",
      " [7.20635235e-01]\n",
      " [9.99996424e-01]\n",
      " [9.99999046e-01]\n",
      " [9.99999285e-01]\n",
      " [8.49366188e-06]\n",
      " [3.03983688e-06]\n",
      " [2.98023224e-08]\n",
      " [5.96046448e-08]\n",
      " [5.06639481e-07]\n",
      " [1.49011612e-06]\n",
      " [8.94069672e-08]\n",
      " [1.85549259e-04]\n",
      " [2.98023224e-07]\n",
      " [2.98023224e-08]\n",
      " [2.98023224e-08]\n",
      " [8.94069672e-08]\n",
      " [2.59768486e-01]\n",
      " [6.33099675e-03]\n",
      " [1.17421150e-05]\n",
      " [8.97111535e-01]\n",
      " [6.41469002e-01]\n",
      " [6.20875657e-01]\n",
      " [9.99999344e-01]\n",
      " [9.99999583e-01]\n",
      " [9.82311726e-01]\n",
      " [9.99944270e-01]\n",
      " [9.99974251e-01]\n",
      " [5.43374419e-01]\n",
      " [8.74484479e-01]\n",
      " [9.99405921e-01]\n",
      " [9.99999583e-01]\n",
      " [9.99999404e-01]\n",
      " [9.99998152e-01]\n",
      " [9.99857903e-01]\n",
      " [3.89617682e-03]\n",
      " [3.30209732e-05]\n",
      " [1.22487545e-05]\n",
      " [1.64273381e-02]\n",
      " [4.17232513e-07]\n",
      " [4.17232513e-07]\n",
      " [8.94069672e-08]\n",
      " [1.10268593e-06]\n",
      " [8.94069672e-08]\n",
      " [8.53859305e-01]\n",
      " [1.13248825e-06]\n",
      " [0.00000000e+00]\n",
      " [4.47034836e-07]\n",
      " [1.22767687e-03]\n",
      " [5.24982452e-01]\n",
      " [9.99997973e-01]\n",
      " [8.50013614e-01]\n",
      " [8.93590271e-01]\n",
      " [9.99899387e-01]\n",
      " [9.97734547e-01]\n",
      " [9.97021198e-01]\n",
      " [9.89935696e-01]\n",
      " [9.99994040e-01]\n",
      " [5.83072007e-02]\n",
      " [1.07255578e-03]\n",
      " [2.69711018e-04]\n",
      " [8.94069672e-08]\n",
      " [2.03520060e-04]\n",
      " [1.52356625e-02]\n",
      " [1.53729320e-03]\n",
      " [9.99981880e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998391e-01]\n",
      " [9.99998569e-01]\n",
      " [9.99990940e-01]\n",
      " [9.99992609e-01]\n",
      " [9.99843717e-01]\n",
      " [7.83081293e-01]\n",
      " [1.02639198e-04]\n",
      " [1.49634778e-02]\n",
      " [8.94069672e-08]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99116063e-01]\n",
      " [9.98905182e-01]\n",
      " [9.99998212e-01]\n",
      " [9.99998569e-01]\n",
      " [9.59208310e-01]\n",
      " [9.99999523e-01]\n",
      " [9.99998808e-01]\n",
      " [8.92563879e-01]]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('model/cnn_lstm_best')\n",
    "\n",
    "predictions = loaded_model.predict(tf_test_X)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5404bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanAndStd(X,y,num_day):\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    for i in range(0,len(X)-num_day): \n",
    "        x_open = X.iloc[i:i+num_day,0]\n",
    "        mean_list.append(x_open.mean(axis=0))\n",
    "        std_list.append(x_open.std(axis=0))\n",
    "    mean_df = pd.DataFrame(mean_list, columns = [\"mean\"])\n",
    "    std_df = pd.DataFrame(std_list, columns = [\"std\"])\n",
    "    return (mean_df,std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa6ca8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean, test_std = getMeanAndStd(test_X, test_y, num_day_to_predict)\n",
    "\n",
    "\n",
    "final_test_y = test_y.iloc[num_day_to_predict: , :]\n",
    "\n",
    "\n",
    "final_pred = np.array(predictions*np.array(test_std) + np.array(test_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71b54cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.29035142],\n",
       "       [ 37.18608539],\n",
       "       [ 37.32548867],\n",
       "       [ 37.58556612],\n",
       "       [ 37.55125726],\n",
       "       [ 37.33432626],\n",
       "       [ 37.33292246],\n",
       "       [ 37.9528955 ],\n",
       "       [ 38.03473699],\n",
       "       [ 38.10515598],\n",
       "       [ 38.61917405],\n",
       "       [ 39.23200851],\n",
       "       [ 39.74283334],\n",
       "       [ 40.15409194],\n",
       "       [ 40.85501918],\n",
       "       [ 41.51633103],\n",
       "       [ 41.73286913],\n",
       "       [ 40.62369368],\n",
       "       [ 40.92198723],\n",
       "       [ 41.22053677],\n",
       "       [ 41.49469509],\n",
       "       [ 41.54095493],\n",
       "       [ 41.66230136],\n",
       "       [ 41.74389926],\n",
       "       [ 42.0487353 ],\n",
       "       [ 41.89885457],\n",
       "       [ 41.89469633],\n",
       "       [ 42.09972826],\n",
       "       [ 42.21872919],\n",
       "       [ 42.29749707],\n",
       "       [ 42.37915286],\n",
       "       [ 42.4713048 ],\n",
       "       [ 42.6644042 ],\n",
       "       [ 42.7437438 ],\n",
       "       [ 42.4562321 ],\n",
       "       [ 42.50600004],\n",
       "       [ 42.47550005],\n",
       "       [ 42.90279959],\n",
       "       [ 43.25817253],\n",
       "       [ 43.69082445],\n",
       "       [ 44.12962487],\n",
       "       [ 44.54114504],\n",
       "       [ 44.92490634],\n",
       "       [ 45.39497021],\n",
       "       [ 45.67340156],\n",
       "       [ 46.06929676],\n",
       "       [ 46.5436151 ],\n",
       "       [ 46.3807105 ],\n",
       "       [ 45.88725301],\n",
       "       [ 46.04560083],\n",
       "       [ 46.16950044],\n",
       "       [ 46.42025963],\n",
       "       [ 46.44097498],\n",
       "       [ 47.04005536],\n",
       "       [ 47.16056241],\n",
       "       [ 47.33514794],\n",
       "       [ 47.41636406],\n",
       "       [ 47.63290763],\n",
       "       [ 48.07477599],\n",
       "       [ 48.3219779 ],\n",
       "       [ 48.61793162],\n",
       "       [ 48.34957088],\n",
       "       [ 48.09856054],\n",
       "       [ 48.54736931],\n",
       "       [ 48.90648243],\n",
       "       [ 49.12221905],\n",
       "       [ 49.28334151],\n",
       "       [ 49.49269798],\n",
       "       [ 49.87665748],\n",
       "       [ 49.99649244],\n",
       "       [ 49.45956599],\n",
       "       [ 49.586006  ],\n",
       "       [ 49.69540023],\n",
       "       [ 50.64874585],\n",
       "       [ 50.85438539],\n",
       "       [ 51.1152246 ],\n",
       "       [ 50.43101323],\n",
       "       [ 50.46210014],\n",
       "       [ 50.32920002],\n",
       "       [ 50.17200029],\n",
       "       [ 50.00810037],\n",
       "       [ 49.6183024 ],\n",
       "       [ 49.22940191],\n",
       "       [ 48.67030145],\n",
       "       [ 48.20110277],\n",
       "       [ 47.63375286],\n",
       "       [ 47.14400006],\n",
       "       [ 46.65670052],\n",
       "       [ 46.2533002 ],\n",
       "       [ 45.76760034],\n",
       "       [ 45.34580049],\n",
       "       [ 45.13010062],\n",
       "       [ 44.8855    ],\n",
       "       [ 44.68235102],\n",
       "       [ 44.34700162],\n",
       "       [ 44.0698001 ],\n",
       "       [ 44.73732089],\n",
       "       [ 44.67200173],\n",
       "       [ 44.56603449],\n",
       "       [ 44.93896224],\n",
       "       [ 45.62072877],\n",
       "       [ 46.38580746],\n",
       "       [ 46.95098533],\n",
       "       [ 47.44927174],\n",
       "       [ 47.68049956],\n",
       "       [ 47.85307792],\n",
       "       [ 47.93517525],\n",
       "       [ 48.33311889],\n",
       "       [ 48.57940503],\n",
       "       [ 48.67703597],\n",
       "       [ 48.79309107],\n",
       "       [ 48.90315258],\n",
       "       [ 48.96783435],\n",
       "       [ 49.12162588],\n",
       "       [ 49.09932725],\n",
       "       [ 49.26470746],\n",
       "       [ 49.33718549],\n",
       "       [ 49.51109452],\n",
       "       [ 49.64730446],\n",
       "       [ 49.14613261],\n",
       "       [ 49.16275888],\n",
       "       [ 49.24685722],\n",
       "       [ 49.38241157],\n",
       "       [ 49.69711502],\n",
       "       [ 49.93110545],\n",
       "       [ 50.00439238],\n",
       "       [ 49.67213139],\n",
       "       [ 50.10422294],\n",
       "       [ 49.77798002],\n",
       "       [ 50.25480222],\n",
       "       [ 50.49250147],\n",
       "       [ 50.65512212],\n",
       "       [ 50.85309327],\n",
       "       [ 50.81636072],\n",
       "       [ 51.06483732],\n",
       "       [ 50.68148689],\n",
       "       [ 51.82345568],\n",
       "       [ 52.10315957],\n",
       "       [ 51.19416004],\n",
       "       [ 51.05310004],\n",
       "       [ 50.75540023],\n",
       "       [ 50.45530026],\n",
       "       [ 50.24240007],\n",
       "       [ 50.11158272],\n",
       "       [ 49.91110032],\n",
       "       [ 51.47313713],\n",
       "       [ 50.33094505],\n",
       "       [ 49.41176984],\n",
       "       [ 50.0026048 ],\n",
       "       [ 50.62645789],\n",
       "       [ 51.11227964],\n",
       "       [ 51.57048708],\n",
       "       [ 51.97838271],\n",
       "       [ 50.88110966],\n",
       "       [ 51.63616556],\n",
       "       [ 51.20260833],\n",
       "       [ 51.22570154],\n",
       "       [ 51.70670153],\n",
       "       [ 51.49412526],\n",
       "       [ 51.39142753],\n",
       "       [ 51.72002306],\n",
       "       [ 51.9863687 ],\n",
       "       [ 52.04884714],\n",
       "       [ 52.32732849],\n",
       "       [ 52.53933746],\n",
       "       [ 52.96321084],\n",
       "       [ 53.74973365],\n",
       "       [ 54.06351367],\n",
       "       [ 53.12441035],\n",
       "       [ 53.22143329],\n",
       "       [ 53.53243311],\n",
       "       [ 54.06123771],\n",
       "       [ 53.95810475],\n",
       "       [ 54.05911172],\n",
       "       [ 54.23550011],\n",
       "       [ 54.24893256],\n",
       "       [ 54.22835935],\n",
       "       [ 54.14253703],\n",
       "       [ 54.48762737],\n",
       "       [ 54.79027587],\n",
       "       [ 54.4208414 ],\n",
       "       [ 54.3114034 ],\n",
       "       [ 55.04714447],\n",
       "       [ 55.1831429 ],\n",
       "       [ 55.47391696],\n",
       "       [ 55.27346473],\n",
       "       [ 55.89193933],\n",
       "       [ 56.39172868],\n",
       "       [ 56.904743  ],\n",
       "       [ 57.39390782],\n",
       "       [ 57.63886114],\n",
       "       [ 57.77766747],\n",
       "       [ 57.94740112],\n",
       "       [ 58.21745076],\n",
       "       [ 58.61124414],\n",
       "       [ 58.95428722],\n",
       "       [ 59.33801651],\n",
       "       [ 59.60893415],\n",
       "       [ 60.09199057],\n",
       "       [ 59.35211133],\n",
       "       [ 59.55377361],\n",
       "       [ 60.91330723],\n",
       "       [ 61.10231272],\n",
       "       [ 61.81790966],\n",
       "       [ 62.35451427],\n",
       "       [ 62.37658219],\n",
       "       [ 63.23537391],\n",
       "       [ 63.58434876],\n",
       "       [ 63.8633535 ],\n",
       "       [ 64.25547411],\n",
       "       [ 64.43375436],\n",
       "       [ 64.65177642],\n",
       "       [ 64.72964024],\n",
       "       [ 65.05832285],\n",
       "       [ 65.43933266],\n",
       "       [ 64.76408607],\n",
       "       [ 64.88270675],\n",
       "       [ 64.9791001 ],\n",
       "       [ 65.08848915],\n",
       "       [ 65.65450444],\n",
       "       [ 65.77445683],\n",
       "       [ 65.85355432],\n",
       "       [ 65.48935629],\n",
       "       [ 65.30480017],\n",
       "       [ 65.13600009],\n",
       "       [ 65.09310348],\n",
       "       [ 65.94270196],\n",
       "       [ 66.23120343],\n",
       "       [ 66.04534989],\n",
       "       [ 66.47178607],\n",
       "       [ 66.53879874],\n",
       "       [ 66.75523253],\n",
       "       [ 67.28745258],\n",
       "       [ 67.86060028],\n",
       "       [ 68.31733728],\n",
       "       [ 68.66011204],\n",
       "       [ 69.10589305],\n",
       "       [ 69.37789085],\n",
       "       [ 69.79984057],\n",
       "       [ 70.10713457],\n",
       "       [ 70.62845615],\n",
       "       [ 70.92025322],\n",
       "       [ 71.23464084],\n",
       "       [ 71.90555029],\n",
       "       [ 72.47636424],\n",
       "       [ 72.67712524],\n",
       "       [ 72.62371425],\n",
       "       [ 73.27585667],\n",
       "       [ 74.25453405],\n",
       "       [ 75.07427782],\n",
       "       [ 75.81213236],\n",
       "       [ 76.69801335],\n",
       "       [ 75.77341492],\n",
       "       [ 77.24645391],\n",
       "       [ 77.10147161],\n",
       "       [ 77.81632498],\n",
       "       [ 77.66817528],\n",
       "       [ 78.42503656],\n",
       "       [ 78.64139465],\n",
       "       [ 77.81050005],\n",
       "       [ 78.32453697],\n",
       "       [ 79.08511568],\n",
       "       [ 79.28434414],\n",
       "       [ 78.42183237],\n",
       "       [ 78.12620301],\n",
       "       [ 78.07970253],\n",
       "       [ 78.20140857],\n",
       "       [ 79.95565492],\n",
       "       [ 78.38736534],\n",
       "       [ 78.50772767],\n",
       "       [ 78.79850162],\n",
       "       [ 79.98849294],\n",
       "       [ 80.43571589],\n",
       "       [ 79.373419  ],\n",
       "       [ 79.25580329],\n",
       "       [ 79.65486779],\n",
       "       [ 79.38730182],\n",
       "       [ 79.30895396],\n",
       "       [ 78.68780024],\n",
       "       [ 78.36060022],\n",
       "       [ 77.4439003 ],\n",
       "       [ 76.4459    ],\n",
       "       [ 74.79120049],\n",
       "       [ 73.74342234],\n",
       "       [ 73.51101598],\n",
       "       [ 73.69480618],\n",
       "       [ 72.22103027],\n",
       "       [ 71.29324273],\n",
       "       [ 70.4643    ],\n",
       "       [ 69.8752    ],\n",
       "       [ 69.64891775],\n",
       "       [ 69.02650595],\n",
       "       [ 69.65620134],\n",
       "       [ 68.22990708],\n",
       "       [ 66.8291178 ],\n",
       "       [ 65.43031915],\n",
       "       [ 64.23730356],\n",
       "       [ 63.37640041],\n",
       "       [ 62.49410036],\n",
       "       [ 63.08686509],\n",
       "       [ 60.94903015],\n",
       "       [ 62.7150939 ],\n",
       "       [ 60.56712416],\n",
       "       [ 62.40957029],\n",
       "       [ 61.08911022],\n",
       "       [ 60.87820319],\n",
       "       [ 60.70441189],\n",
       "       [ 60.59554824],\n",
       "       [ 61.76144448],\n",
       "       [ 64.09192228],\n",
       "       [ 64.58930795],\n",
       "       [ 65.41809894],\n",
       "       [ 66.0248665 ],\n",
       "       [ 67.28047997],\n",
       "       [ 68.4046559 ],\n",
       "       [ 69.66722513],\n",
       "       [ 70.35541828],\n",
       "       [ 67.78297481],\n",
       "       [ 68.28880074],\n",
       "       [ 68.59068951],\n",
       "       [ 68.71568857],\n",
       "       [ 69.25170146],\n",
       "       [ 69.5944606 ],\n",
       "       [ 69.30328019],\n",
       "       [ 70.54560626],\n",
       "       [ 70.72391128],\n",
       "       [ 70.80032128],\n",
       "       [ 71.19342145],\n",
       "       [ 71.87993915],\n",
       "       [ 72.73132694],\n",
       "       [ 73.56254191],\n",
       "       [ 74.38895032],\n",
       "       [ 75.21023382],\n",
       "       [ 76.51973078],\n",
       "       [ 75.91095063],\n",
       "       [ 75.57720854],\n",
       "       [ 75.78188862],\n",
       "       [ 77.27175361],\n",
       "       [ 77.40441813],\n",
       "       [ 78.30528134],\n",
       "       [ 78.71198751],\n",
       "       [ 78.91809709],\n",
       "       [ 79.44492202],\n",
       "       [ 78.96771409],\n",
       "       [ 79.31090729],\n",
       "       [ 79.73158405],\n",
       "       [ 79.36551709],\n",
       "       [ 79.49988664],\n",
       "       [ 79.86311237],\n",
       "       [ 80.12976729],\n",
       "       [ 80.28364998],\n",
       "       [ 80.84943241],\n",
       "       [ 81.33862299],\n",
       "       [ 83.06119141],\n",
       "       [ 84.36501447],\n",
       "       [ 85.09691218],\n",
       "       [ 85.2273067 ],\n",
       "       [ 85.45503037],\n",
       "       [ 87.025279  ],\n",
       "       [ 87.51978217],\n",
       "       [ 86.71903534],\n",
       "       [ 88.10449229],\n",
       "       [ 88.82290519],\n",
       "       [ 89.53921737],\n",
       "       [ 88.62732392],\n",
       "       [ 88.18403862],\n",
       "       [ 88.55341001],\n",
       "       [ 88.76747163],\n",
       "       [ 90.30352935],\n",
       "       [ 89.86395747],\n",
       "       [ 91.28552127],\n",
       "       [ 91.88206444],\n",
       "       [ 92.47317644],\n",
       "       [ 93.52134454],\n",
       "       [ 94.1592347 ],\n",
       "       [ 95.12975187],\n",
       "       [ 95.25732163],\n",
       "       [ 96.275945  ],\n",
       "       [ 96.54887416],\n",
       "       [ 94.90896138],\n",
       "       [ 96.82142999],\n",
       "       [ 95.81206226],\n",
       "       [ 96.6791555 ],\n",
       "       [ 96.13310008],\n",
       "       [ 95.7020002 ],\n",
       "       [ 95.34960209],\n",
       "       [ 95.30283293],\n",
       "       [ 94.8095971 ],\n",
       "       [ 95.95365122],\n",
       "       [ 98.44828898],\n",
       "       [101.39279237],\n",
       "       [103.6324717 ],\n",
       "       [105.77310308],\n",
       "       [107.81413331],\n",
       "       [110.15166456],\n",
       "       [111.87679731],\n",
       "       [110.62981681],\n",
       "       [111.67064068],\n",
       "       [112.77467318],\n",
       "       [113.03455526],\n",
       "       [111.88029048],\n",
       "       [113.55023813],\n",
       "       [114.75881109],\n",
       "       [115.08913299],\n",
       "       [116.29385861],\n",
       "       [120.40527771],\n",
       "       [122.07860754],\n",
       "       [123.66787167],\n",
       "       [125.21431533],\n",
       "       [124.66733596],\n",
       "       [127.3358179 ],\n",
       "       [129.19056047],\n",
       "       [131.86804218],\n",
       "       [126.86604087],\n",
       "       [126.94701417],\n",
       "       [125.48100019],\n",
       "       [124.74300041],\n",
       "       [124.16500361],\n",
       "       [122.91601157],\n",
       "       [121.79500073],\n",
       "       [120.87750188],\n",
       "       [119.13500217],\n",
       "       [116.36500014],\n",
       "       [114.72500011],\n",
       "       [113.18100041],\n",
       "       [114.25469566],\n",
       "       [112.52142609],\n",
       "       [110.98305138],\n",
       "       [114.2828812 ],\n",
       "       [113.21543101],\n",
       "       [112.40497817],\n",
       "       [113.54976744],\n",
       "       [114.92526276],\n",
       "       [115.10599473],\n",
       "       [115.35015894],\n",
       "       [115.78674556],\n",
       "       [114.42908319],\n",
       "       [115.68463461],\n",
       "       [115.57486851],\n",
       "       [116.82300747],\n",
       "       [119.47514047],\n",
       "       [120.29703902],\n",
       "       [120.43148315],\n",
       "       [117.46817682],\n",
       "       [118.05511023],\n",
       "       [118.1050403 ],\n",
       "       [118.35917759],\n",
       "       [118.42900123],\n",
       "       [118.53900117],\n",
       "       [117.93800029],\n",
       "       [116.96700267],\n",
       "       [116.3760002 ],\n",
       "       [117.97694986],\n",
       "       [114.72800287],\n",
       "       [113.651     ],\n",
       "       [113.00100134],\n",
       "       [112.752458  ],\n",
       "       [114.31667091],\n",
       "       [116.19582736],\n",
       "       [116.96613295],\n",
       "       [117.16884945],\n",
       "       [117.92424536],\n",
       "       [118.86741973],\n",
       "       [119.60640252],\n",
       "       [119.79215851],\n",
       "       [119.55420368],\n",
       "       [118.07202697],\n",
       "       [117.97453889],\n",
       "       [118.00638653],\n",
       "       [117.67600012],\n",
       "       [117.51334991],\n",
       "       [117.37668469],\n",
       "       [117.04779687],\n",
       "       [118.5061995 ],\n",
       "       [119.02705889],\n",
       "       [119.65513618],\n",
       "       [120.74954873],\n",
       "       [121.52107997],\n",
       "       [122.06532136],\n",
       "       [123.05324938],\n",
       "       [122.99065988],\n",
       "       [120.87428386],\n",
       "       [121.48968743],\n",
       "       [122.01700011],\n",
       "       [123.60874466],\n",
       "       [124.72596804],\n",
       "       [125.93562467],\n",
       "       [126.94548539],\n",
       "       [127.09814589],\n",
       "       [128.51492992],\n",
       "       [129.79908952],\n",
       "       [130.40764544],\n",
       "       [131.76223598],\n",
       "       [133.69831538],\n",
       "       [134.15165408]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f381de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoluted Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6349001500594025"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean Absoluted Error\")\n",
    "np.mean(abs(np.array(final_test_y)-final_pred)) ## Mean Absolute Error of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1659f5e",
   "metadata": {},
   "source": [
    "### Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8d53847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_start_date = 20190101\n",
    "plot_end_date = 20201231\n",
    "keep = (final_test_y.index >= plot_start_date) & (final_test_y.index <= plot_end_date)\n",
    "final_pred = pd.DataFrame(data=final_pred,index = final_test_y.index, columns = [\"Predicted\"])\n",
    "plot_test_y = final_test_y[keep]\n",
    "plot_pred = final_pred[keep]\n",
    "\n",
    "string_index =  plot_test_y.index.map(str)\n",
    "\n",
    "plt.plot(string_index, plot_test_y[\"result_price\"], label = \"Actual\", color = 'Black')\n",
    "plt.plot(string_index, plot_pred[\"Predicted\"], label = \"Predicted\", color = 'Orange')\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.title(\"Prediction of \"+stock.upper()+\" using CNN-LSTM\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"plot/CNN_LSTM/\"+stock.upper()+\"-day(\"+str(num_day_to_predict)+\").jpg\",\n",
    "            dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617290e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_23895/3608706721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_test_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Actual\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Predicted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_test_y' is not defined"
     ]
    }
   ],
   "source": [
    "abc = pd.concat([plot_test_y,plot_pred], ignore_index=True, sort=False,axis=1)\n",
    "abc.columns = [\"Actual\",\"Predicted\"]\n",
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b42a6723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>boll</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdh</th>\n",
       "      <th>macds</th>\n",
       "      <th>rsi_11</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19840907</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10028</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>96970899</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840910</th>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.09905</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>75265237</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.102049</td>\n",
       "      <td>0.100351</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840911</th>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10456</td>\n",
       "      <td>0.10181</td>\n",
       "      <td>0.10274</td>\n",
       "      <td>177479896</td>\n",
       "      <td>0.101713</td>\n",
       "      <td>0.103590</td>\n",
       "      <td>0.099837</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>77.134146</td>\n",
       "      <td>76.758045</td>\n",
       "      <td>76.303318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840912</th>\n",
       "      <td>0.10274</td>\n",
       "      <td>0.10334</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>0.09966</td>\n",
       "      <td>155043826</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.103762</td>\n",
       "      <td>0.098638</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>32.201239</td>\n",
       "      <td>32.592743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840913</th>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10548</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>0.10518</td>\n",
       "      <td>241475025</td>\n",
       "      <td>0.101996</td>\n",
       "      <td>0.106191</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>68.412723</td>\n",
       "      <td>68.025100</td>\n",
       "      <td>67.561551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211021</th>\n",
       "      <td>148.81000</td>\n",
       "      <td>149.64000</td>\n",
       "      <td>147.87000</td>\n",
       "      <td>149.48000</td>\n",
       "      <td>61420990</td>\n",
       "      <td>143.875000</td>\n",
       "      <td>149.793245</td>\n",
       "      <td>137.956755</td>\n",
       "      <td>0.375617</td>\n",
       "      <td>1.069014</td>\n",
       "      <td>-0.693396</td>\n",
       "      <td>65.673205</td>\n",
       "      <td>61.532287</td>\n",
       "      <td>57.199864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211022</th>\n",
       "      <td>149.69000</td>\n",
       "      <td>150.18000</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>148.69000</td>\n",
       "      <td>58883443</td>\n",
       "      <td>143.963500</td>\n",
       "      <td>150.121546</td>\n",
       "      <td>137.805454</td>\n",
       "      <td>0.577001</td>\n",
       "      <td>1.016318</td>\n",
       "      <td>-0.439317</td>\n",
       "      <td>61.924694</td>\n",
       "      <td>58.867114</td>\n",
       "      <td>55.611436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211025</th>\n",
       "      <td>148.68000</td>\n",
       "      <td>149.37000</td>\n",
       "      <td>147.62110</td>\n",
       "      <td>148.64000</td>\n",
       "      <td>50720556</td>\n",
       "      <td>144.127000</td>\n",
       "      <td>150.607481</td>\n",
       "      <td>137.646519</td>\n",
       "      <td>0.724216</td>\n",
       "      <td>0.930826</td>\n",
       "      <td>-0.206610</td>\n",
       "      <td>61.679591</td>\n",
       "      <td>58.693837</td>\n",
       "      <td>55.508996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211026</th>\n",
       "      <td>149.33000</td>\n",
       "      <td>150.84000</td>\n",
       "      <td>149.01010</td>\n",
       "      <td>149.32000</td>\n",
       "      <td>60893395</td>\n",
       "      <td>144.497500</td>\n",
       "      <td>151.284341</td>\n",
       "      <td>137.710659</td>\n",
       "      <td>0.885547</td>\n",
       "      <td>0.873726</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>63.821803</td>\n",
       "      <td>60.401009</td>\n",
       "      <td>56.649320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20211027</th>\n",
       "      <td>149.36000</td>\n",
       "      <td>149.73000</td>\n",
       "      <td>148.49000</td>\n",
       "      <td>148.85000</td>\n",
       "      <td>55925403</td>\n",
       "      <td>144.798500</td>\n",
       "      <td>151.804399</td>\n",
       "      <td>137.792601</td>\n",
       "      <td>0.964362</td>\n",
       "      <td>0.762033</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>61.219811</td>\n",
       "      <td>58.598318</td>\n",
       "      <td>55.614834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9362 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               open       high        low      close     volume        boll  \\\n",
       "DATE                                                                          \n",
       "19840907    0.10150    0.10274    0.10028    0.10150   96970899    0.101500   \n",
       "19840910    0.10150    0.10181    0.09905    0.10090   75265237    0.101200   \n",
       "19840911    0.10181    0.10456    0.10181    0.10274  177479896    0.101713   \n",
       "19840912    0.10274    0.10334    0.09966    0.09966  155043826    0.101200   \n",
       "19840913    0.10518    0.10548    0.10518    0.10518  241475025    0.101996   \n",
       "...             ...        ...        ...        ...        ...         ...   \n",
       "20211021  148.81000  149.64000  147.87000  149.48000   61420990  143.875000   \n",
       "20211022  149.69000  150.18000  148.64000  148.69000   58883443  143.963500   \n",
       "20211025  148.68000  149.37000  147.62110  148.64000   50720556  144.127000   \n",
       "20211026  149.33000  150.84000  149.01010  149.32000   60893395  144.497500   \n",
       "20211027  149.36000  149.73000  148.49000  148.85000   55925403  144.798500   \n",
       "\n",
       "             boll_ub     boll_lb      macd     macdh     macds     rsi_11  \\\n",
       "DATE                                                                        \n",
       "19840907         NaN         NaN  0.000000  0.000000  0.000000        NaN   \n",
       "19840910    0.102049    0.100351 -0.000013 -0.000006 -0.000007   0.000000   \n",
       "19840911    0.103590    0.099837  0.000040  0.000028  0.000012  77.134146   \n",
       "19840912    0.103762    0.098638 -0.000048 -0.000040 -0.000008  31.870001   \n",
       "19840913    0.106191    0.097801  0.000125  0.000094  0.000031  68.412723   \n",
       "...              ...         ...       ...       ...       ...        ...   \n",
       "20211021  149.793245  137.956755  0.375617  1.069014 -0.693396  65.673205   \n",
       "20211022  150.121546  137.805454  0.577001  1.016318 -0.439317  61.924694   \n",
       "20211025  150.607481  137.646519  0.724216  0.930826 -0.206610  61.679591   \n",
       "20211026  151.284341  137.710659  0.885547  0.873726  0.011821  63.821803   \n",
       "20211027  151.804399  137.792601  0.964362  0.762033  0.202329  61.219811   \n",
       "\n",
       "             rsi_14     rsi_21  \n",
       "DATE                            \n",
       "19840907        NaN        NaN  \n",
       "19840910   0.000000   0.000000  \n",
       "19840911  76.758045  76.303318  \n",
       "19840912  32.201239  32.592743  \n",
       "19840913  68.025100  67.561551  \n",
       "...             ...        ...  \n",
       "20211021  61.532287  57.199864  \n",
       "20211022  58.867114  55.611436  \n",
       "20211025  58.693837  55.508996  \n",
       "20211026  60.401009  56.649320  \n",
       "20211027  58.598318  55.614834  \n",
       "\n",
       "[9362 rows x 14 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23013b2",
   "metadata": {},
   "source": [
    "# CNN_LSTM (Direction Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b4edf",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5591f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = custom_split(data,start = 20120101,end = 20161031)\n",
    "valid_X = custom_split(data,start = 20161101,end = 20171231)\n",
    "test_X = custom_split(data,start = 20180101,end = 20191231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccf495",
   "metadata": {},
   "source": [
    "### Label the target result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dd349a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we use 10 days price data to predict opening price of the 11th day\n",
    "num_day_to_predict = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2359d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_result_target_price(X,num_day,result_col_name = \"Action\"):\n",
    "    y = pd.DataFrame(np.nan, index=X.index, columns=[result_col_name])\n",
    "    status = \"Hold\"\n",
    "    for i in range(len(X)-num_day):\n",
    "        last_10_day_mean = np.mean(X.iloc[i:i+num_day,0])\n",
    "        if X.iloc[i+num_day,0]>last_10_day_mean*1.01:\n",
    "            y.iloc[i+num_day_to_predict,0] = 1\n",
    "            status = \"Buy\"\n",
    "        elif X.iloc[i+num_day,0]<last_10_day_mean/1.01:\n",
    "            y.iloc[i+num_day_to_predict,0] = 0\n",
    "            status = \"Sell\"\n",
    "        else:\n",
    "            if status == \"Hold\" or status == \"Sell\":\n",
    "                y.iloc[i+num_day_to_predict,0] = 0\n",
    "            elif status == \"Buy\":\n",
    "                y.iloc[i+num_day_to_predict,0] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c967a375",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_42478/1760813407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y value meaning {1: Buy, 0: Sell}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduce_result_target_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_day_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduce_result_target_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_day_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduce_result_target_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_day_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "# y value meaning {1: Buy, 0: Sell}\n",
    "train_y = produce_result_target_price(train_X,num_day_to_predict)\n",
    "valid_y = produce_result_target_price(valid_X,num_day_to_predict)\n",
    "test_y = produce_result_target_price(test_X,num_day_to_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ff912",
   "metadata": {},
   "source": [
    "### Transform the X, y data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6df60e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_X_data_to_tensor(X,num_day):\n",
    "    # Initiate tensor for X\n",
    "    x_first = X.iloc[0:num_day,:]\n",
    "    x_mean = x_first.mean(axis=0) # Get the mean of the 10-day frame\n",
    "    x_std = x_first.std(axis=0) # Get the std of the 10-day frame\n",
    "    x_first = x_first.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "    x_tf_data = [tf.convert_to_tensor(np.array(x_first),dtype = tf.float32)]\n",
    "    \n",
    "    for i in range(1,len(X)-num_day):   \n",
    "        x_window = X.iloc[i:i+num_day,:] # Set the window as a 10-day frame \n",
    "        x_mean = x_window.mean(axis=0) # Get the mean of the 10-day frame\n",
    "        x_std = x_window.std(axis=0) # Get the std of the 10-day frame\n",
    "        x_window = x_window.sub(x_mean, axis=1).div(x_std, axis=1) # Normalize the 10-day frame here\n",
    "        \n",
    "        x_next_tf = tf.convert_to_tensor(np.array(x_window),dtype = tf.float32)\n",
    "        x_tf_data = tf.concat([x_tf_data, [x_next_tf]], 0)\n",
    "        \n",
    "    return tf.reshape(x_tf_data,(-1,10,14,1))\n",
    "def transform_y_data_to_tensor(y,num_day):\n",
    "    temp_y = y.dropna()\n",
    "    y_tf_data = []\n",
    "    for ind in temp_y.index:\n",
    "        if temp_y.loc[ind,\"Action\"] == 1:\n",
    "            y_tf_data.append([1,0])\n",
    "        elif temp_y.loc[ind,\"Action\"] == 0:\n",
    "            y_tf_data.append([0,1])\n",
    "    y_tf_data = tf.convert_to_tensor(y_tf_data)\n",
    "        \n",
    "    return y_tf_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "88f70317",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_X = transform_X_data_to_tensor(train_X,num_day_to_predict)\n",
    "tf_train_y = transform_y_data_to_tensor(train_y,num_day_to_predict)\n",
    "tf_valid_X = transform_X_data_to_tensor(valid_X,num_day_to_predict)\n",
    "tf_valid_y = transform_y_data_to_tensor(valid_y,num_day_to_predict)\n",
    "tf_test_X = transform_X_data_to_tensor(test_X,num_day_to_predict)\n",
    "tf_test_y = transform_y_data_to_tensor(test_y,num_day_to_predict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "87b2e32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1206, 10, 14, 1)\n",
      "(1206, 2)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int32'>\n",
      "(283, 10, 14, 1)\n",
      "(283, 2)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int32'>\n",
      "(493, 10, 14, 1)\n",
      "(493, 2)\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print(tf_train_X.shape)\n",
    "print(tf_train_y.shape)\n",
    "print(tf_train_X.dtype)\n",
    "print(tf_train_y.dtype)\n",
    "\n",
    "print(tf_valid_X.shape)\n",
    "print(tf_valid_y.shape)\n",
    "print(tf_valid_X.dtype)\n",
    "print(tf_valid_y.dtype)\n",
    "\n",
    "print(tf_test_X.shape)\n",
    "print(tf_test_y.shape)\n",
    "print(tf_test_X.dtype)\n",
    "print(tf_test_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999db5fc",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0aaf34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def myModel(input_shape,\n",
    "            encoder_unit = 100,\n",
    "            repeat_vector_n = 10):\n",
    "    \n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    print(\"Input: \",inputs.shape)\n",
    "    \n",
    "    # First Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 64,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"1 Cov: \",x.shape)\n",
    "    \n",
    "    # Second Convolution + MaxPooling + Dropout\n",
    "    x = layers.Conv2D(filters = 16,kernel_size=(3,3), strides = (1,1), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2),strides=(2,1), padding='valid')(x)\n",
    "    x = layers.Dropout(rate = 0.01)(x)\n",
    "    print(\"2 Cov: \",x.shape)\n",
    "    \n",
    "    # Flatten Layer\n",
    "    x = layers.Flatten()(x)\n",
    "    print(\"Flatten: \",x.shape)\n",
    "    \n",
    "    # Repeat Vector Layer\n",
    "    x = layers.RepeatVector(n = repeat_vector_n)(x)\n",
    "    print(\"RepeatVector: \",x.shape)\n",
    "    \n",
    "    # Connect to LSTM\n",
    "    x = layers.LSTM(units = encoder_unit, input_shape=(5,1))(x)\n",
    "    print(\"LSTM: \",x.shape)\n",
    "    \n",
    "    # Second Flatten Layer\n",
    "    x = layers.Flatten()(x)\n",
    "    print(\"Flatten: \",x.shape)\n",
    "    \n",
    "    # Add the Dense Layer with relu activation\n",
    "    x = layers.Dense(units = 50,activation = \"relu\")(x)\n",
    "    print(\"1 Dense: \",x.shape)\n",
    "    \n",
    "    # Add the last Dense Layer with sigmoid activation\n",
    "    outputs = layers.Dense(units = 2,activation = \"softmax\")(x)\n",
    "    print(\"Output: \",outputs.shape)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b283d29",
   "metadata": {},
   "source": [
    "### Model Training and Fitting and Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c0e24de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 50)\n",
      "Flatten:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 18:56:14.040313: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_295784_296269_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_296442' and '__inference___backward_standard_lstm_295784_296269' both implement 'lstm_838bbf1c-f6c0-4327-9c7e-ad70855fa1fb' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.1300 - root_mean_squared_error: 0.3603\n",
      "Epoch 2/30\n",
      "1231/1206 [==============================] - 9s 7ms/sample - loss: 0.1111 - root_mean_squared_error: 0.3329\n",
      "Epoch 3/30\n",
      "1231/1206 [==============================] - 9s 8ms/sample - loss: 0.0782 - root_mean_squared_error: 0.2778\n",
      "Epoch 4/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0752 - root_mean_squared_error: 0.2689\n",
      "Epoch 5/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0834 - root_mean_squared_error: 0.2880\n",
      "Epoch 6/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0666 - root_mean_squared_error: 0.2598\n",
      "Epoch 7/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0625 - root_mean_squared_error: 0.2548\n",
      "Epoch 8/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0556 - root_mean_squared_error: 0.2301\n",
      "Epoch 9/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0625 - root_mean_squared_error: 0.2486\n",
      "Epoch 10/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0550 - root_mean_squared_error: 0.2347\n",
      "Epoch 11/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0499 - root_mean_squared_error: 0.2245\n",
      "Epoch 12/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0649 - root_mean_squared_error: 0.2534\n",
      "Epoch 13/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0454 - root_mean_squared_error: 0.2131\n",
      "Epoch 14/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0395 - root_mean_squared_error: 0.1959\n",
      "Epoch 15/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0378 - root_mean_squared_error: 0.1925\n",
      "Epoch 16/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0365 - root_mean_squared_error: 0.1883\n",
      "Epoch 17/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0257 - root_mean_squared_error: 0.1623\n",
      "Epoch 18/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0451 - root_mean_squared_error: 0.2118\n",
      "Epoch 19/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0283 - root_mean_squared_error: 0.1696\n",
      "Epoch 20/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0350 - root_mean_squared_error: 0.1873\n",
      "Epoch 21/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0247 - root_mean_squared_error: 0.1555\n",
      "Epoch 22/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0293 - root_mean_squared_error: 0.1679\n",
      "Epoch 23/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0624 - root_mean_squared_error: 0.2508\n",
      "Epoch 24/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0608 - root_mean_squared_error: 0.2434\n",
      "Epoch 25/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0436 - root_mean_squared_error: 0.2126\n",
      "Epoch 26/30\n",
      "1231/1206 [==============================] - 10s 9ms/sample - loss: 0.0349 - root_mean_squared_error: 0.1917\n",
      "Epoch 27/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0219 - root_mean_squared_error: 0.1483\n",
      "Epoch 28/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0242 - root_mean_squared_error: 0.1560\n",
      "Epoch 29/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0210 - root_mean_squared_error: 0.1431\n",
      "Epoch 30/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0195 - root_mean_squared_error: 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:01:21.079451: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_301380' and '__inference_standard_lstm_301269_specialized_for_model_15_lstm_15_StatefulPartitionedCall_at___inference_distributed_function_301622' both implement 'lstm_797c1cf5-a0e6-4267-842b-447c2606ddb6' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 0.1137 - root_mean_squared_error: 0.3577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12793228177636756, 0.3576762]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.12793228177636756, 0.3576762]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 100)\n",
      "Flatten:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:01:24.145790: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_304680_305165_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_305338' and '__inference___backward_cudnn_lstm_with_fallback_304364_304546' both implement 'lstm_221b1772-1964-4c26-9328-b87ac6bccfd9' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.1343 - root_mean_squared_error: 0.3663\n",
      "Epoch 2/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0895 - root_mean_squared_error: 0.2989\n",
      "Epoch 3/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0778 - root_mean_squared_error: 0.2792\n",
      "Epoch 4/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0613 - root_mean_squared_error: 0.2482\n",
      "Epoch 5/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0652 - root_mean_squared_error: 0.2574\n",
      "Epoch 6/30\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0560 - root_mean_squared_error: 0.2366\n",
      "Epoch 7/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0607 - root_mean_squared_error: 0.2468\n",
      "Epoch 8/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0598 - root_mean_squared_error: 0.2412\n",
      "Epoch 9/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0543 - root_mean_squared_error: 0.2317\n",
      "Epoch 10/30\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0512 - root_mean_squared_error: 0.2266\n",
      "Epoch 11/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0489 - root_mean_squared_error: 0.2178\n",
      "Epoch 12/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0410 - root_mean_squared_error: 0.2014\n",
      "Epoch 13/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0351 - root_mean_squared_error: 0.1892\n",
      "Epoch 14/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0321 - root_mean_squared_error: 0.1819\n",
      "Epoch 15/30\n",
      "1231/1206 [==============================] - 16s 13ms/sample - loss: 0.0385 - root_mean_squared_error: 0.2028\n",
      "Epoch 16/30\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0388 - root_mean_squared_error: 0.1985\n",
      "Epoch 17/30\n",
      "1231/1206 [==============================] - 15s 12ms/sample - loss: 0.0459 - root_mean_squared_error: 0.2106\n",
      "Epoch 18/30\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0320 - root_mean_squared_error: 0.1768\n",
      "Epoch 19/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0317 - root_mean_squared_error: 0.1803\n",
      "Epoch 20/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0249 - root_mean_squared_error: 0.1580\n",
      "Epoch 21/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0370 - root_mean_squared_error: 0.1968\n",
      "Epoch 22/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0270 - root_mean_squared_error: 0.1643\n",
      "Epoch 23/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0353 - root_mean_squared_error: 0.1899\n",
      "Epoch 24/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0326 - root_mean_squared_error: 0.1807\n",
      "Epoch 25/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0317 - root_mean_squared_error: 0.1780\n",
      "Epoch 26/30\n",
      "1231/1206 [==============================] - 18s 15ms/sample - loss: 0.0222 - root_mean_squared_error: 0.1519\n",
      "Epoch 27/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0247 - root_mean_squared_error: 0.1591\n",
      "Epoch 28/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0277 - root_mean_squared_error: 0.1663\n",
      "Epoch 29/30\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0241 - root_mean_squared_error: 0.1556\n",
      "Epoch 30/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0201 - root_mean_squared_error: 0.1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:07:41.590415: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_310165' and '__inference_standard_lstm_310165_specialized_for_model_16_lstm_16_StatefulPartitionedCall_at___inference_distributed_function_310518' both implement 'lstm_8feaa14f-c455-43f4-9df9-06ac546cab47' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 0.0980 - root_mean_squared_error: 0.3172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10063353690315473, 0.3172279]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.10063353690315473, 0.3172279]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 50)\n",
      "Flatten:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:07:44.903578: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_313576_314061' and '__inference___backward_standard_lstm_313576_314061_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_314234' both implement 'lstm_a0b85ac0-d434-497d-9fe1-0c27e99f8fd6' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.1265 - root_mean_squared_error: 0.3561\n",
      "Epoch 2/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.1213 - root_mean_squared_error: 0.3474\n",
      "Epoch 3/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0910 - root_mean_squared_error: 0.3006\n",
      "Epoch 4/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0768 - root_mean_squared_error: 0.2764\n",
      "Epoch 5/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0735 - root_mean_squared_error: 0.2713\n",
      "Epoch 6/30\n",
      "1231/1206 [==============================] - 10s 9ms/sample - loss: 0.0713 - root_mean_squared_error: 0.2694\n",
      "Epoch 7/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0735 - root_mean_squared_error: 0.2745\n",
      "Epoch 8/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0609 - root_mean_squared_error: 0.2452\n",
      "Epoch 9/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0562 - root_mean_squared_error: 0.2346\n",
      "Epoch 10/30\n",
      "1231/1206 [==============================] - 10s 9ms/sample - loss: 0.0582 - root_mean_squared_error: 0.2423\n",
      "Epoch 11/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0605 - root_mean_squared_error: 0.2427\n",
      "Epoch 12/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0498 - root_mean_squared_error: 0.2244\n",
      "Epoch 13/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0539 - root_mean_squared_error: 0.2292\n",
      "Epoch 14/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0564 - root_mean_squared_error: 0.2337\n",
      "Epoch 15/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0390 - root_mean_squared_error: 0.1987\n",
      "Epoch 16/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0478 - root_mean_squared_error: 0.2193\n",
      "Epoch 17/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0513 - root_mean_squared_error: 0.2239\n",
      "Epoch 18/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0418 - root_mean_squared_error: 0.2072\n",
      "Epoch 19/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0417 - root_mean_squared_error: 0.2037\n",
      "Epoch 20/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0478 - root_mean_squared_error: 0.2162\n",
      "Epoch 21/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0482 - root_mean_squared_error: 0.2178\n",
      "Epoch 22/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0403 - root_mean_squared_error: 0.2064\n",
      "Epoch 23/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0307 - root_mean_squared_error: 0.1748\n",
      "Epoch 24/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0446 - root_mean_squared_error: 0.2139\n",
      "Epoch 25/30\n",
      "1231/1206 [==============================] - 10s 8ms/sample - loss: 0.0537 - root_mean_squared_error: 0.2344\n",
      "Epoch 26/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0448 - root_mean_squared_error: 0.2117\n",
      "Epoch 27/30\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0391 - root_mean_squared_error: 0.1928\n",
      "Epoch 28/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0402 - root_mean_squared_error: 0.2018\n",
      "Epoch 29/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0413 - root_mean_squared_error: 0.2043\n",
      "Epoch 30/30\n",
      "1231/1206 [==============================] - 21s 17ms/sample - loss: 0.0410 - root_mean_squared_error: 0.2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:13:22.349147: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_319061_specialized_for_model_17_lstm_17_StatefulPartitionedCall_at___inference_distributed_function_319414' and '__inference_standard_lstm_319061' both implement 'lstm_79952b33-3f76-4299-9ed0-9188a0a55fbf' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 0.0841 - root_mean_squared_error: 0.2838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0805631507148591, 0.28383648]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.0805631507148591, 0.28383648]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 100)\n",
      "Flatten:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:13:29.446836: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_322156_322338' and '__inference___backward_standard_lstm_322472_322957_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_323130' both implement 'lstm_e0717f27-707a-4c71-a466-0421be2e0586' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 18s 14ms/sample - loss: 0.1477 - root_mean_squared_error: 0.3836\n",
      "Epoch 2/30\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0986 - root_mean_squared_error: 0.3174\n",
      "Epoch 3/30\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0800 - root_mean_squared_error: 0.2838\n",
      "Epoch 4/30\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0719 - root_mean_squared_error: 0.2725\n",
      "Epoch 5/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0639 - root_mean_squared_error: 0.2519\n",
      "Epoch 6/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0681 - root_mean_squared_error: 0.2595\n",
      "Epoch 7/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0562 - root_mean_squared_error: 0.2400\n",
      "Epoch 8/30\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0781 - root_mean_squared_error: 0.2771\n",
      "Epoch 9/30\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0799 - root_mean_squared_error: 0.2823\n",
      "Epoch 10/30\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0583 - root_mean_squared_error: 0.2383\n",
      "Epoch 11/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0584 - root_mean_squared_error: 0.2423\n",
      "Epoch 12/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0496 - root_mean_squared_error: 0.2227\n",
      "Epoch 13/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0516 - root_mean_squared_error: 0.2298\n",
      "Epoch 14/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0538 - root_mean_squared_error: 0.2296\n",
      "Epoch 15/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0605 - root_mean_squared_error: 0.2464\n",
      "Epoch 16/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0523 - root_mean_squared_error: 0.2305\n",
      "Epoch 17/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0477 - root_mean_squared_error: 0.2199\n",
      "Epoch 18/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0472 - root_mean_squared_error: 0.2178\n",
      "Epoch 19/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0447 - root_mean_squared_error: 0.2114\n",
      "Epoch 20/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0403 - root_mean_squared_error: 0.1987\n",
      "Epoch 21/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0446 - root_mean_squared_error: 0.2167\n",
      "Epoch 22/30\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0485 - root_mean_squared_error: 0.2227\n",
      "Epoch 23/30\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0405 - root_mean_squared_error: 0.2035\n",
      "Epoch 24/30\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0523 - root_mean_squared_error: 0.2259\n",
      "Epoch 25/30\n",
      "1231/1206 [==============================] - 15s 12ms/sample - loss: 0.0412 - root_mean_squared_error: 0.2007\n",
      "Epoch 26/30\n",
      "1231/1206 [==============================] - 16s 13ms/sample - loss: 0.0428 - root_mean_squared_error: 0.2082\n",
      "Epoch 27/30\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0659 - root_mean_squared_error: 0.2587\n",
      "Epoch 28/30\n",
      "1231/1206 [==============================] - 14s 12ms/sample - loss: 0.0375 - root_mean_squared_error: 0.1931\n",
      "Epoch 29/30\n",
      "1231/1206 [==============================] - 15s 12ms/sample - loss: 0.0435 - root_mean_squared_error: 0.2106\n",
      "Epoch 30/30\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0527 - root_mean_squared_error: 0.2294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:19:59.737921: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_327957' and '__inference_standard_lstm_327957_specialized_for_model_18_lstm_18_StatefulPartitionedCall_at___inference_distributed_function_328310' both implement 'lstm_c7570c8d-fdcc-4917-a654-839a9bf36db6' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 7ms/sample - loss: 0.0876 - root_mean_squared_error: 0.2610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06812245608060605, 0.26100278]\n",
      "===== Summary =====\n",
      "Epoch:  30\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.06812245608060605, 0.26100278]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 50)\n",
      "Flatten:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:20:03.507133: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_331368_331853_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_332026' and '__inference___backward_cudnn_lstm_with_fallback_331052_331234' both implement 'lstm_c85d9d1a-1ea8-4df8-a3f3-7a96fe17fdb5' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 14s 12ms/sample - loss: 0.1446 - root_mean_squared_error: 0.3798\n",
      "Epoch 2/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0900 - root_mean_squared_error: 0.3003\n",
      "Epoch 3/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0806 - root_mean_squared_error: 0.2843\n",
      "Epoch 4/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0722 - root_mean_squared_error: 0.2673\n",
      "Epoch 5/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0630 - root_mean_squared_error: 0.2513\n",
      "Epoch 6/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0604 - root_mean_squared_error: 0.2466\n",
      "Epoch 7/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0522 - root_mean_squared_error: 0.2292\n",
      "Epoch 8/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0621 - root_mean_squared_error: 0.2559\n",
      "Epoch 9/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0621 - root_mean_squared_error: 0.2483\n",
      "Epoch 10/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0449 - root_mean_squared_error: 0.2103\n",
      "Epoch 11/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0401 - root_mean_squared_error: 0.1976\n",
      "Epoch 12/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0405 - root_mean_squared_error: 0.1993\n",
      "Epoch 13/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0337 - root_mean_squared_error: 0.1816\n",
      "Epoch 14/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0345 - root_mean_squared_error: 0.1841\n",
      "Epoch 15/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0364 - root_mean_squared_error: 0.1909\n",
      "Epoch 16/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0314 - root_mean_squared_error: 0.1784\n",
      "Epoch 17/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0295 - root_mean_squared_error: 0.1718\n",
      "Epoch 18/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0266 - root_mean_squared_error: 0.1632\n",
      "Epoch 19/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0334 - root_mean_squared_error: 0.1829\n",
      "Epoch 20/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0268 - root_mean_squared_error: 0.1616\n",
      "Epoch 21/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0279 - root_mean_squared_error: 0.1674\n",
      "Epoch 22/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0226 - root_mean_squared_error: 0.1524\n",
      "Epoch 23/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0253 - root_mean_squared_error: 0.1548\n",
      "Epoch 24/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0239 - root_mean_squared_error: 0.1531\n",
      "Epoch 25/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0348 - root_mean_squared_error: 0.1863\n",
      "Epoch 26/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0214 - root_mean_squared_error: 0.1461\n",
      "Epoch 27/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
      "Epoch 28/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0171 - root_mean_squared_error: 0.1307\n",
      "Epoch 29/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0248 - root_mean_squared_error: 0.1571\n",
      "Epoch 30/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0224 - root_mean_squared_error: 0.1480\n",
      "Epoch 31/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0146 - root_mean_squared_error: 0.1226\n",
      "Epoch 32/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0164 - root_mean_squared_error: 0.1280\n",
      "Epoch 33/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0132 - root_mean_squared_error: 0.1150\n",
      "Epoch 34/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0159 - root_mean_squared_error: 0.1259\n",
      "Epoch 35/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0170 - root_mean_squared_error: 0.1288\n",
      "Epoch 36/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0096 - root_mean_squared_error: 0.1005\n",
      "Epoch 37/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0228 - root_mean_squared_error: 0.1536\n",
      "Epoch 38/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0190 - root_mean_squared_error: 0.1377\n",
      "Epoch 39/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0119 - root_mean_squared_error: 0.1097\n",
      "Epoch 40/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0183 - root_mean_squared_error: 0.1352\n",
      "Epoch 41/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0186 - root_mean_squared_error: 0.1334\n",
      "Epoch 42/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0191 - root_mean_squared_error: 0.1360\n",
      "Epoch 43/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0183 - root_mean_squared_error: 0.1332\n",
      "Epoch 44/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0126 - root_mean_squared_error: 0.1118\n",
      "Epoch 45/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0158 - root_mean_squared_error: 0.1283\n",
      "Epoch 46/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0181 - root_mean_squared_error: 0.1344\n",
      "Epoch 47/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0131 - root_mean_squared_error: 0.1151\n",
      "Epoch 48/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0209 - root_mean_squared_error: 0.1439\n",
      "Epoch 49/50\n",
      "1212/1206 [==============================] - 15s 13ms/sample - loss: 0.0190 - root_mean_squared_error: 0.1380\n",
      "Epoch 50/50\n",
      "1231/1206 [==============================] - 15s 12ms/sample - loss: 0.0148 - root_mean_squared_error: 0.1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:29:47.873891: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_340004' and '__inference_standard_lstm_339893_specialized_for_model_19_lstm_19_StatefulPartitionedCall_at___inference_distributed_function_340246' both implement 'lstm_928d7685-51af-442b-a4ff-b3236d4b4c07' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 7ms/sample - loss: 0.1074 - root_mean_squared_error: 0.3475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12077951965715354, 0.34753346]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  50\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.12077951965715354, 0.34753346]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 100)\n",
      "Flatten:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:29:52.164071: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_343304_343789_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_343962' and '__inference___backward_standard_lstm_343304_343789' both implement 'lstm_49025b24-22dc-46a0-9021-47d42d5c0807' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 17s 14ms/sample - loss: 0.1318 - root_mean_squared_error: 0.3635\n",
      "Epoch 2/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0832 - root_mean_squared_error: 0.2863\n",
      "Epoch 3/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0716 - root_mean_squared_error: 0.2659\n",
      "Epoch 4/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0815 - root_mean_squared_error: 0.2860\n",
      "Epoch 5/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0672 - root_mean_squared_error: 0.2609\n",
      "Epoch 6/50\n",
      "1231/1206 [==============================] - 14s 12ms/sample - loss: 0.0561 - root_mean_squared_error: 0.2352\n",
      "Epoch 7/50\n",
      "1231/1206 [==============================] - 14s 12ms/sample - loss: 0.1055 - root_mean_squared_error: 0.3254\n",
      "Epoch 8/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.1010 - root_mean_squared_error: 0.3160\n",
      "Epoch 9/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0790 - root_mean_squared_error: 0.2819\n",
      "Epoch 10/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0622 - root_mean_squared_error: 0.2485\n",
      "Epoch 11/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0615 - root_mean_squared_error: 0.2455\n",
      "Epoch 12/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0564 - root_mean_squared_error: 0.2354\n",
      "Epoch 13/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0505 - root_mean_squared_error: 0.2263\n",
      "Epoch 14/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0459 - root_mean_squared_error: 0.2151\n",
      "Epoch 15/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0486 - root_mean_squared_error: 0.2170\n",
      "Epoch 16/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0444 - root_mean_squared_error: 0.2075\n",
      "Epoch 17/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0489 - root_mean_squared_error: 0.2188\n",
      "Epoch 18/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0357 - root_mean_squared_error: 0.1906\n",
      "Epoch 19/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0283 - root_mean_squared_error: 0.1688\n",
      "Epoch 20/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0422 - root_mean_squared_error: 0.2039\n",
      "Epoch 21/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0290 - root_mean_squared_error: 0.1696\n",
      "Epoch 22/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0294 - root_mean_squared_error: 0.1771\n",
      "Epoch 23/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0417 - root_mean_squared_error: 0.2016\n",
      "Epoch 24/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0273 - root_mean_squared_error: 0.1647\n",
      "Epoch 25/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0305 - root_mean_squared_error: 0.1746\n",
      "Epoch 26/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0259 - root_mean_squared_error: 0.1656\n",
      "Epoch 27/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0235 - root_mean_squared_error: 0.1554\n",
      "Epoch 28/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0342 - root_mean_squared_error: 0.1834\n",
      "Epoch 29/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0261 - root_mean_squared_error: 0.1614\n",
      "Epoch 30/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0246 - root_mean_squared_error: 0.1580\n",
      "Epoch 31/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0296 - root_mean_squared_error: 0.1722\n",
      "Epoch 32/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0277 - root_mean_squared_error: 0.1626\n",
      "Epoch 33/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0325 - root_mean_squared_error: 0.1804\n",
      "Epoch 34/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0251 - root_mean_squared_error: 0.1594\n",
      "Epoch 35/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0238 - root_mean_squared_error: 0.1519\n",
      "Epoch 36/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0359 - root_mean_squared_error: 0.1889\n",
      "Epoch 37/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0276 - root_mean_squared_error: 0.1684\n",
      "Epoch 38/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0338 - root_mean_squared_error: 0.1860\n",
      "Epoch 39/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0286 - root_mean_squared_error: 0.1687\n",
      "Epoch 40/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0214 - root_mean_squared_error: 0.1459\n",
      "Epoch 41/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0179 - root_mean_squared_error: 0.1363\n",
      "Epoch 42/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0166 - root_mean_squared_error: 0.1257\n",
      "Epoch 43/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0199 - root_mean_squared_error: 0.1415\n",
      "Epoch 44/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0198 - root_mean_squared_error: 0.1408\n",
      "Epoch 45/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0153 - root_mean_squared_error: 0.1240\n",
      "Epoch 46/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0187 - root_mean_squared_error: 0.1366\n",
      "Epoch 47/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0159 - root_mean_squared_error: 0.1281\n",
      "Epoch 48/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0217 - root_mean_squared_error: 0.1495\n",
      "Epoch 49/50\n",
      "1212/1206 [==============================] - 13s 11ms/sample - loss: 0.0179 - root_mean_squared_error: 0.1351\n",
      "Epoch 50/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0165 - root_mean_squared_error: 0.1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:40:50.447234: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_351829' and '__inference_standard_lstm_351829_specialized_for_model_20_lstm_20_StatefulPartitionedCall_at___inference_distributed_function_352182' both implement 'lstm_b4a17b5c-96c4-434d-8a04-2112b1ad55d0' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 0.1149 - root_mean_squared_error: 0.3883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15075570950449144, 0.38827273]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.005\n",
      "Encoder Units:  100\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.15075570950449144, 0.38827273]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 50)\n",
      "Flatten:  (None, 50)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:40:54.505627: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_354924_355106' and '__inference___backward_standard_lstm_355240_355725_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_355898' both implement 'lstm_5ffad706-c78e-4431-bc7b-285e52f65c88' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 15s 12ms/sample - loss: 0.1172 - root_mean_squared_error: 0.3431\n",
      "Epoch 2/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.1104 - root_mean_squared_error: 0.3310\n",
      "Epoch 3/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0939 - root_mean_squared_error: 0.3081\n",
      "Epoch 4/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0788 - root_mean_squared_error: 0.2832\n",
      "Epoch 5/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0739 - root_mean_squared_error: 0.2716\n",
      "Epoch 6/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0745 - root_mean_squared_error: 0.2777\n",
      "Epoch 7/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0864 - root_mean_squared_error: 0.2929\n",
      "Epoch 8/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0855 - root_mean_squared_error: 0.2967\n",
      "Epoch 9/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0861 - root_mean_squared_error: 0.2936\n",
      "Epoch 10/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0850 - root_mean_squared_error: 0.2896\n",
      "Epoch 11/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0834 - root_mean_squared_error: 0.2880\n",
      "Epoch 12/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0823 - root_mean_squared_error: 0.2841\n",
      "Epoch 13/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0733 - root_mean_squared_error: 0.2711\n",
      "Epoch 14/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0609 - root_mean_squared_error: 0.2439\n",
      "Epoch 15/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0670 - root_mean_squared_error: 0.2556\n",
      "Epoch 16/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0750 - root_mean_squared_error: 0.2720\n",
      "Epoch 17/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0568 - root_mean_squared_error: 0.2420\n",
      "Epoch 18/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0657 - root_mean_squared_error: 0.2572\n",
      "Epoch 19/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0641 - root_mean_squared_error: 0.2529\n",
      "Epoch 20/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.1081 - root_mean_squared_error: 0.3273\n",
      "Epoch 21/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.1416 - root_mean_squared_error: 0.3767\n",
      "Epoch 22/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.1170 - root_mean_squared_error: 0.3423\n",
      "Epoch 23/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.1098 - root_mean_squared_error: 0.3311\n",
      "Epoch 24/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0928 - root_mean_squared_error: 0.3048\n",
      "Epoch 25/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0830 - root_mean_squared_error: 0.2847\n",
      "Epoch 26/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0879 - root_mean_squared_error: 0.2982\n",
      "Epoch 27/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0830 - root_mean_squared_error: 0.2885\n",
      "Epoch 28/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0871 - root_mean_squared_error: 0.2964\n",
      "Epoch 29/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.1027 - root_mean_squared_error: 0.3184\n",
      "Epoch 30/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0928 - root_mean_squared_error: 0.3009\n",
      "Epoch 31/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0883 - root_mean_squared_error: 0.2975\n",
      "Epoch 32/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.1033 - root_mean_squared_error: 0.3207\n",
      "Epoch 33/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0828 - root_mean_squared_error: 0.2848\n",
      "Epoch 34/50\n",
      "1231/1206 [==============================] - 12s 9ms/sample - loss: 0.0840 - root_mean_squared_error: 0.2903\n",
      "Epoch 35/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0839 - root_mean_squared_error: 0.2872\n",
      "Epoch 36/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0806 - root_mean_squared_error: 0.2834\n",
      "Epoch 37/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0791 - root_mean_squared_error: 0.2819\n",
      "Epoch 38/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0953 - root_mean_squared_error: 0.3131\n",
      "Epoch 39/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0930 - root_mean_squared_error: 0.3058\n",
      "Epoch 40/50\n",
      "1231/1206 [==============================] - 17s 14ms/sample - loss: 0.1036 - root_mean_squared_error: 0.3211\n",
      "Epoch 41/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0941 - root_mean_squared_error: 0.3073\n",
      "Epoch 42/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0895 - root_mean_squared_error: 0.2987\n",
      "Epoch 43/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0826 - root_mean_squared_error: 0.2902\n",
      "Epoch 44/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0894 - root_mean_squared_error: 0.2973\n",
      "Epoch 45/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0920 - root_mean_squared_error: 0.3013\n",
      "Epoch 46/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.1035 - root_mean_squared_error: 0.3187\n",
      "Epoch 47/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0877 - root_mean_squared_error: 0.2968\n",
      "Epoch 48/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0879 - root_mean_squared_error: 0.2953\n",
      "Epoch 49/50\n",
      "1212/1206 [==============================] - 13s 11ms/sample - loss: 0.0884 - root_mean_squared_error: 0.2965\n",
      "Epoch 50/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0904 - root_mean_squared_error: 0.3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:51:06.342497: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_363765' and '__inference_standard_lstm_363765_specialized_for_model_21_lstm_21_StatefulPartitionedCall_at___inference_distributed_function_364118' both implement 'lstm_5088a076-09f3-46fc-9969-a4e4d55c3c88' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 6ms/sample - loss: 0.1011 - root_mean_squared_error: 0.3076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0945959239934864, 0.3075645]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  50\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.0945959239934864, 0.3075645]\n",
      "Input:  (None, 10, 14, 1)\n",
      "1 Cov:  (None, 4, 11, 64)\n",
      "2 Cov:  (None, 1, 8, 16)\n",
      "Flatten:  (None, 128)\n",
      "RepeatVector:  (None, 100, 128)\n",
      "LSTM:  (None, 100)\n",
      "Flatten:  (None, 100)\n",
      "1 Dense:  (None, 50)\n",
      "Output:  (None, 2)\n",
      "Train on 1206 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:51:10.245110: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_367176_367661' and '__inference___backward_standard_lstm_367176_367661_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_367834' both implement 'lstm_155d743e-b541-4ffc-9766-ff4d7fb41860' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231/1206 [==============================] - 16s 13ms/sample - loss: 0.1560 - root_mean_squared_error: 0.3949\n",
      "Epoch 2/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.1380 - root_mean_squared_error: 0.3706\n",
      "Epoch 3/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0960 - root_mean_squared_error: 0.3111\n",
      "Epoch 4/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0993 - root_mean_squared_error: 0.3136\n",
      "Epoch 5/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0942 - root_mean_squared_error: 0.3088\n",
      "Epoch 6/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0748 - root_mean_squared_error: 0.2717\n",
      "Epoch 7/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0893 - root_mean_squared_error: 0.2962\n",
      "Epoch 8/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0704 - root_mean_squared_error: 0.2669\n",
      "Epoch 9/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0645 - root_mean_squared_error: 0.2540\n",
      "Epoch 10/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0583 - root_mean_squared_error: 0.2461\n",
      "Epoch 11/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0808 - root_mean_squared_error: 0.2839\n",
      "Epoch 12/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0614 - root_mean_squared_error: 0.2449\n",
      "Epoch 13/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0569 - root_mean_squared_error: 0.2372\n",
      "Epoch 14/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0602 - root_mean_squared_error: 0.2436\n",
      "Epoch 15/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0514 - root_mean_squared_error: 0.2259\n",
      "Epoch 16/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0593 - root_mean_squared_error: 0.2405\n",
      "Epoch 17/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0501 - root_mean_squared_error: 0.2253\n",
      "Epoch 18/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0635 - root_mean_squared_error: 0.2512\n",
      "Epoch 19/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0521 - root_mean_squared_error: 0.2271\n",
      "Epoch 20/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0508 - root_mean_squared_error: 0.2228\n",
      "Epoch 21/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0510 - root_mean_squared_error: 0.2261\n",
      "Epoch 22/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0523 - root_mean_squared_error: 0.2284\n",
      "Epoch 23/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0468 - root_mean_squared_error: 0.2189\n",
      "Epoch 24/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0536 - root_mean_squared_error: 0.2341\n",
      "Epoch 25/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0491 - root_mean_squared_error: 0.2223\n",
      "Epoch 26/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0512 - root_mean_squared_error: 0.2265\n",
      "Epoch 27/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0427 - root_mean_squared_error: 0.2069\n",
      "Epoch 28/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0688 - root_mean_squared_error: 0.2627\n",
      "Epoch 29/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0482 - root_mean_squared_error: 0.2184\n",
      "Epoch 30/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0341 - root_mean_squared_error: 0.1817\n",
      "Epoch 31/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0357 - root_mean_squared_error: 0.1888\n",
      "Epoch 32/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0326 - root_mean_squared_error: 0.1823\n",
      "Epoch 33/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0373 - root_mean_squared_error: 0.1955\n",
      "Epoch 34/50\n",
      "1231/1206 [==============================] - 11s 9ms/sample - loss: 0.0419 - root_mean_squared_error: 0.2043\n",
      "Epoch 35/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0431 - root_mean_squared_error: 0.2076\n",
      "Epoch 36/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0428 - root_mean_squared_error: 0.2090\n",
      "Epoch 37/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0451 - root_mean_squared_error: 0.2118\n",
      "Epoch 38/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0365 - root_mean_squared_error: 0.1916\n",
      "Epoch 39/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0372 - root_mean_squared_error: 0.1913\n",
      "Epoch 40/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0339 - root_mean_squared_error: 0.1846\n",
      "Epoch 41/50\n",
      "1231/1206 [==============================] - 12s 10ms/sample - loss: 0.0472 - root_mean_squared_error: 0.2151\n",
      "Epoch 42/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0289 - root_mean_squared_error: 0.1718\n",
      "Epoch 43/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0432 - root_mean_squared_error: 0.2080\n",
      "Epoch 44/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0287 - root_mean_squared_error: 0.1747\n",
      "Epoch 45/50\n",
      "1231/1206 [==============================] - 14s 11ms/sample - loss: 0.0343 - root_mean_squared_error: 0.1862\n",
      "Epoch 46/50\n",
      "1231/1206 [==============================] - 13s 11ms/sample - loss: 0.0381 - root_mean_squared_error: 0.1910\n",
      "Epoch 47/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0282 - root_mean_squared_error: 0.1679\n",
      "Epoch 48/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0373 - root_mean_squared_error: 0.1947\n",
      "Epoch 49/50\n",
      "1212/1206 [==============================] - 14s 11ms/sample - loss: 0.0344 - root_mean_squared_error: 0.1877\n",
      "Epoch 50/50\n",
      "1231/1206 [==============================] - 13s 10ms/sample - loss: 0.0295 - root_mean_squared_error: 0.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 20:01:53.084849: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_375701' and '__inference_standard_lstm_375701_specialized_for_model_22_lstm_22_StatefulPartitionedCall_at___inference_distributed_function_376054' both implement 'lstm_32c9c7c2-857c-464a-b710-09737151880e' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "283/1 [==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 7ms/sample - loss: 0.0762 - root_mean_squared_error: 0.3049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09299092046100343, 0.30494413]\n",
      "===== Summary =====\n",
      "Epoch:  50\n",
      "Batch Size:  50\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  0.01\n",
      "Encoder Units:  100\n",
      "Loss Function:  Categorical CrossEntropy\n",
      "Metrics:  [<tensorflow.python.keras.metrics.RootMeanSquaredError object at 0x7fae9f582c10>]\n",
      "Validation:  [0.09299092046100343, 0.30494413]\n",
      "INFO:tensorflow:Assets written to: model/cnn_lstm_classify_best/assets\n"
     ]
    }
   ],
   "source": [
    "optimizer_list = [\"Adam\"]\n",
    "epoch_list = [30,50]\n",
    "batch_list = [50]\n",
    "encoder_list = [50,100]\n",
    "lr_list = [0.005,0.01]\n",
    "train_df = pd.DataFrame(columns = [\"Epoch\",\"Batch\",\"Optimizer\",\"LR\",\"Encoder Unit\",\"Loss\",\"Metrics\",\"Validation\"])\n",
    "best_model = \"\"\n",
    "best_valid = 99999\n",
    "metrics = [keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "\n",
    "for opti in optimizer_list:\n",
    "    for epochs in epoch_list:\n",
    "        for batchs in batch_list:\n",
    "            for lr in lr_list:\n",
    "                for encoder_u in encoder_list:\n",
    "\n",
    "                    model = myModel(input_shape=(num_day_to_predict,train_X.shape[1],1),\n",
    "                                    encoder_unit = encoder_u,\n",
    "                                    repeat_vector_n = 100\n",
    "                                   )\n",
    "\n",
    "                    if opti == \"Adam\":\n",
    "                        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "                    model.compile(\n",
    "                        optimizer=optimizer,\n",
    "                        loss=keras.losses.MeanSquaredError(),\n",
    "                        metrics=metrics,\n",
    "                    )\n",
    "\n",
    "                    history = model.fit(\n",
    "                            tf_train_X,\n",
    "                            tf_train_y,\n",
    "                            epochs = epochs,\n",
    "                            steps_per_epoch = batchs,\n",
    "                        )\n",
    "\n",
    "                    results = model.evaluate(tf_valid_X, tf_valid_y, batch_size=batchs)\n",
    "                    print(results)\n",
    "                    print(\"===== Summary =====\")\n",
    "                    print(\"Epoch: \",epochs)\n",
    "                    print(\"Batch Size: \",batchs)\n",
    "                    print(\"Optimizer: \",opti)\n",
    "                    print(\"Learning Rate: \",lr)\n",
    "                    print(\"Encoder Units: \",encoder_u)\n",
    "                    print(\"Loss Function: \", \"Categorical CrossEntropy\")\n",
    "                    print(\"Metrics: \", metrics)\n",
    "                    print(\"Validation: \",results)\n",
    "                    if results[0] < best_valid:\n",
    "                        best_valid = results[0]\n",
    "                        best_model = model\n",
    "                    train_df = train_df.append({\"Epoch\": epochs,\n",
    "                                                \"Batch\": batchs,\n",
    "                                                \"Optimizer\": opti,\n",
    "                                                \"LR\": lr,\n",
    "                                                \"Encoder Unit\": encoder_u,\n",
    "                                                \"Loss\": \"Categorical CrossEntropy\",\n",
    "                                                \"Metrics\": metrics,\n",
    "                                                \"Validation\":results}, ignore_index=True)\n",
    "best_model.save(\"model/cnn_lstm_classify_best\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f0a1c",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "27d149fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('model/cnn_lstm_classify_best')\n",
    "predictions = loaded_model.predict(tf_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3b4d168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (493, 2)\n",
      "[[0.99592364 0.00407638]\n",
      " [0.9956215  0.00437855]\n",
      " [0.99581546 0.00418453]\n",
      " [0.9883577  0.0116423 ]\n",
      " [0.9934089  0.00659111]\n",
      " [0.99180263 0.00819742]\n",
      " [0.06043568 0.93956435]\n",
      " [0.03372977 0.9662702 ]\n",
      " [0.01021304 0.989787  ]\n",
      " [0.00971145 0.9902886 ]\n",
      " [0.01123999 0.98876   ]\n",
      " [0.00353817 0.9964618 ]\n",
      " [0.00359634 0.9964037 ]\n",
      " [0.00487208 0.9951279 ]\n",
      " [0.00307662 0.9969234 ]\n",
      " [0.00317898 0.99682105]\n",
      " [0.013501   0.98649895]\n",
      " [0.00551987 0.99448013]\n",
      " [0.01151958 0.98848045]\n",
      " [0.02495917 0.97504085]\n",
      " [0.8699542  0.13004583]\n",
      " [0.9415485  0.05845151]\n",
      " [0.99271977 0.00728025]\n",
      " [0.99270105 0.00729891]\n",
      " [0.9936953  0.00630464]\n",
      " [0.993415   0.00658504]\n",
      " [0.9936401  0.00635982]\n",
      " [0.994379   0.00562095]\n",
      " [0.9927942  0.0072058 ]\n",
      " [0.99342066 0.00657933]\n",
      " [0.9868461  0.01315391]\n",
      " [0.9865159  0.0134841 ]\n",
      " [0.9101009  0.08989918]\n",
      " [0.905978   0.09402198]\n",
      " [0.9175549  0.08244506]\n",
      " [0.91486025 0.08513978]\n",
      " [0.90275025 0.09724974]\n",
      " [0.9520911  0.0479089 ]\n",
      " [0.98979425 0.01020581]\n",
      " [0.99602544 0.00397459]\n",
      " [0.9957944  0.00420556]\n",
      " [0.99461764 0.00538242]\n",
      " [0.99162006 0.00837988]\n",
      " [0.80140895 0.19859113]\n",
      " [0.00979036 0.9902097 ]\n",
      " [0.00538875 0.99461126]\n",
      " [0.01437195 0.985628  ]\n",
      " [0.00881769 0.9911823 ]\n",
      " [0.01038889 0.98961115]\n",
      " [0.01321059 0.9867894 ]\n",
      " [0.00412907 0.9958709 ]\n",
      " [0.00454385 0.9954561 ]\n",
      " [0.00497085 0.99502915]\n",
      " [0.00418169 0.9958183 ]\n",
      " [0.12398246 0.8760176 ]\n",
      " [0.9914927  0.00850736]\n",
      " [0.98465204 0.01534795]\n",
      " [0.98468286 0.0153171 ]\n",
      " [0.9929472  0.00705278]\n",
      " [0.9955686  0.00443142]\n",
      " [0.9959888  0.0040112 ]\n",
      " [0.9962346  0.00376541]\n",
      " [0.99339277 0.00660717]\n",
      " [0.994526   0.00547404]\n",
      " [0.99338806 0.00661188]\n",
      " [0.99170333 0.00829669]\n",
      " [0.1122533  0.88774675]\n",
      " [0.00307626 0.99692374]\n",
      " [0.00308708 0.99691296]\n",
      " [0.00307765 0.9969223 ]\n",
      " [0.00553399 0.994466  ]\n",
      " [0.00904683 0.9909532 ]\n",
      " [0.00645295 0.993547  ]\n",
      " [0.9436252  0.05637476]\n",
      " [0.97795576 0.02204427]\n",
      " [0.98340964 0.0165904 ]\n",
      " [0.9835217  0.01647827]\n",
      " [0.9879347  0.0120653 ]\n",
      " [0.9822072  0.0177928 ]\n",
      " [0.99416935 0.00583067]\n",
      " [0.9922355  0.00776448]\n",
      " [0.9825508  0.01744925]\n",
      " [0.9751819  0.02481817]\n",
      " [0.9881379  0.01186208]\n",
      " [0.9713921  0.02860791]\n",
      " [0.9320102  0.06798981]\n",
      " [0.92058975 0.07941021]\n",
      " [0.9276782  0.07232183]\n",
      " [0.9325009  0.06749915]\n",
      " [0.92429537 0.07570458]\n",
      " [0.98909646 0.01090351]\n",
      " [0.985269   0.01473092]\n",
      " [0.9792795  0.02072055]\n",
      " [0.97426605 0.0257339 ]\n",
      " [0.67566496 0.32433507]\n",
      " [0.97763115 0.02236884]\n",
      " [0.98886275 0.01113727]\n",
      " [0.98439145 0.01560849]\n",
      " [0.98808795 0.01191203]\n",
      " [0.9756581  0.02434186]\n",
      " [0.9957832  0.00421681]\n",
      " [0.9897703  0.01022976]\n",
      " [0.9756073  0.02439273]\n",
      " [0.22562964 0.77437043]\n",
      " [0.10708129 0.89291877]\n",
      " [0.10228401 0.897716  ]\n",
      " [0.00935879 0.9906412 ]\n",
      " [0.00936219 0.9906378 ]\n",
      " [0.01219878 0.9878012 ]\n",
      " [0.00402573 0.99597424]\n",
      " [0.01067815 0.9893218 ]\n",
      " [0.00307627 0.99692374]\n",
      " [0.00307627 0.99692374]\n",
      " [0.01412474 0.9858752 ]\n",
      " [0.08257331 0.91742665]\n",
      " [0.1602308  0.8397692 ]\n",
      " [0.32637444 0.6736256 ]\n",
      " [0.01799011 0.9820098 ]\n",
      " [0.79654926 0.20345074]\n",
      " [0.9918997  0.0081003 ]\n",
      " [0.9927918  0.00720822]\n",
      " [0.99304175 0.00695821]\n",
      " [0.99477357 0.0052264 ]\n",
      " [0.9949327  0.00506729]\n",
      " [0.9962398  0.00376021]\n",
      " [0.996029   0.00397103]\n",
      " [0.99527895 0.0047211 ]\n",
      " [0.9954758  0.00452413]\n",
      " [0.9876274  0.01237254]\n",
      " [0.949973   0.050027  ]\n",
      " [0.9133326  0.08666743]\n",
      " [0.9213912  0.07860874]\n",
      " [0.9916544  0.0083456 ]\n",
      " [0.99189717 0.00810281]\n",
      " [0.991065   0.00893501]\n",
      " [0.9514621  0.048538  ]\n",
      " [0.6073501  0.39264992]\n",
      " [0.99134094 0.00865909]\n",
      " [0.9915388  0.00846111]\n",
      " [0.9902124  0.00978763]\n",
      " [0.99065065 0.00934928]\n",
      " [0.98728406 0.01271598]\n",
      " [0.9954998  0.00450015]\n",
      " [0.9957151  0.00428495]\n",
      " [0.9953229  0.00467708]\n",
      " [0.9957074  0.00429255]\n",
      " [0.99537116 0.00462888]\n",
      " [0.9608328  0.03916727]\n",
      " [0.9957573  0.00424269]\n",
      " [0.9913578  0.00864218]\n",
      " [0.9756015  0.02439849]\n",
      " [0.9860022  0.01399783]\n",
      " [0.9951568  0.00484316]\n",
      " [0.98925614 0.01074384]\n",
      " [0.9939302  0.00606982]\n",
      " [0.9949339  0.00506612]\n",
      " [0.99642015 0.00357992]\n",
      " [0.99121946 0.00878049]\n",
      " [0.9882626  0.01173742]\n",
      " [0.98834807 0.01165192]\n",
      " [0.98828363 0.01171631]\n",
      " [0.9880086  0.01199136]\n",
      " [0.9867039  0.01329614]\n",
      " [0.9203186  0.07968148]\n",
      " [0.4157339  0.584266  ]\n",
      " [0.01430469 0.9856953 ]\n",
      " [0.35940588 0.6405941 ]\n",
      " [0.21747419 0.7825258 ]\n",
      " [0.46547782 0.5345222 ]\n",
      " [0.07682642 0.9231736 ]\n",
      " [0.03797327 0.96202666]\n",
      " [0.00552408 0.9944759 ]\n",
      " [0.00745744 0.9925425 ]\n",
      " [0.6892984  0.31070158]\n",
      " [0.01257327 0.98742676]\n",
      " [0.45332298 0.546677  ]\n",
      " [0.79948074 0.20051926]\n",
      " [0.99449956 0.00550036]\n",
      " [0.9962136  0.00378634]\n",
      " [0.9962603  0.00373975]\n",
      " [0.9956371  0.00436286]\n",
      " [0.9913442  0.00865581]\n",
      " [0.9917862  0.00821378]\n",
      " [0.98693085 0.01306922]\n",
      " [0.80558664 0.19441336]\n",
      " [0.44453675 0.5554633 ]\n",
      " [0.01158296 0.98841697]\n",
      " [0.00756615 0.9924339 ]\n",
      " [0.00648497 0.993515  ]\n",
      " [0.00402668 0.9959733 ]\n",
      " [0.00416907 0.99583095]\n",
      " [0.01040687 0.98959315]\n",
      " [0.00412644 0.9958735 ]\n",
      " [0.00408048 0.9959195 ]\n",
      " [0.728298   0.27170196]\n",
      " [0.93077266 0.06922738]\n",
      " [0.9747652  0.0252348 ]\n",
      " [0.03595498 0.96404505]\n",
      " [0.4232219  0.5767781 ]\n",
      " [0.00705568 0.9929443 ]\n",
      " [0.00408026 0.9959197 ]\n",
      " [0.00801433 0.9919857 ]\n",
      " [0.9117214  0.08827858]\n",
      " [0.00363466 0.9963653 ]\n",
      " [0.00473329 0.9952667 ]\n",
      " [0.00434871 0.9956513 ]\n",
      " [0.00434147 0.9956585 ]\n",
      " [0.00411941 0.9958806 ]\n",
      " [0.00933866 0.9906613 ]\n",
      " [0.00307653 0.9969235 ]\n",
      " [0.00319275 0.9968072 ]\n",
      " [0.00398412 0.99601585]\n",
      " [0.00343723 0.9965628 ]\n",
      " [0.00943509 0.99056494]\n",
      " [0.00415493 0.99584514]\n",
      " [0.00413506 0.995865  ]\n",
      " [0.0030767  0.99692327]\n",
      " [0.00313506 0.996865  ]\n",
      " [0.00541049 0.99458957]\n",
      " [0.00870113 0.99129885]\n",
      " [0.04975687 0.9502432 ]\n",
      " [0.0521455  0.9478545 ]\n",
      " [0.1970969  0.8029032 ]\n",
      " [0.95974445 0.0402555 ]\n",
      " [0.9921469  0.00785315]\n",
      " [0.9080454  0.0919546 ]\n",
      " [0.18722948 0.8127706 ]\n",
      " [0.00503845 0.9949615 ]\n",
      " [0.00446789 0.99553204]\n",
      " [0.0033053  0.9966947 ]\n",
      " [0.00409614 0.9959039 ]\n",
      " [0.00419039 0.9958097 ]\n",
      " [0.00335596 0.996644  ]\n",
      " [0.03380874 0.9661913 ]\n",
      " [0.00965044 0.99034953]\n",
      " [0.00466136 0.9953387 ]\n",
      " [0.00612994 0.99387   ]\n",
      " [0.00353901 0.996461  ]\n",
      " [0.00355044 0.9964496 ]\n",
      " [0.02968327 0.9703167 ]\n",
      " [0.01491873 0.98508126]\n",
      " [0.27799615 0.7220038 ]\n",
      " [0.09319647 0.90680355]\n",
      " [0.0655989  0.9344011 ]\n",
      " [0.02212703 0.97787297]\n",
      " [0.00615072 0.99384934]\n",
      " [0.00594067 0.9940593 ]\n",
      " [0.89961207 0.10038797]\n",
      " [0.12388815 0.87611187]\n",
      " [0.07329536 0.92670465]\n",
      " [0.12748398 0.87251604]\n",
      " [0.9690317  0.03096831]\n",
      " [0.9935806  0.00641939]\n",
      " [0.9951133  0.00488667]\n",
      " [0.9949509  0.00504911]\n",
      " [0.99554896 0.00445101]\n",
      " [0.99444973 0.00555023]\n",
      " [0.9904323  0.00956769]\n",
      " [0.9924833  0.00751664]\n",
      " [0.99459016 0.00540988]\n",
      " [0.9943423  0.00565771]\n",
      " [0.993411   0.00658894]\n",
      " [0.9885449  0.01145514]\n",
      " [0.9871085  0.0128915 ]\n",
      " [0.9888151  0.01118484]\n",
      " [0.99185866 0.00814133]\n",
      " [0.98985845 0.01014152]\n",
      " [0.99576974 0.0042303 ]\n",
      " [0.9958413  0.0041586 ]\n",
      " [0.9495568  0.0504432 ]\n",
      " [0.9310951  0.06890488]\n",
      " [0.93764573 0.06235433]\n",
      " [0.93419266 0.06580737]\n",
      " [0.91877323 0.08122674]\n",
      " [0.87263674 0.12736325]\n",
      " [0.9896625  0.01033745]\n",
      " [0.9745642  0.02543581]\n",
      " [0.9922328  0.00776714]\n",
      " [0.9947443  0.00525575]\n",
      " [0.99415886 0.00584119]\n",
      " [0.9947113  0.00528869]\n",
      " [0.98774624 0.01225369]\n",
      " [0.9602229  0.03977706]\n",
      " [0.98816144 0.01183864]\n",
      " [0.98751444 0.01248555]\n",
      " [0.95035964 0.04964031]\n",
      " [0.7662759  0.23372412]\n",
      " [0.0075042  0.9924958 ]\n",
      " [0.8551613  0.1448387 ]\n",
      " [0.95271957 0.04728042]\n",
      " [0.92614377 0.07385622]\n",
      " [0.97864157 0.02135841]\n",
      " [0.9895733  0.0104267 ]\n",
      " [0.9913357  0.00866431]\n",
      " [0.99170965 0.00829034]\n",
      " [0.9942036  0.00579633]\n",
      " [0.9919299  0.00807016]\n",
      " [0.9919031  0.00809692]\n",
      " [0.99117106 0.00882893]\n",
      " [0.9862764  0.01372365]\n",
      " [0.9138679  0.08613217]\n",
      " [0.9185445  0.08145554]\n",
      " [0.9315637  0.0684363 ]\n",
      " [0.92061573 0.07938428]\n",
      " [0.9590795  0.04092047]\n",
      " [0.9917753  0.00822467]\n",
      " [0.9918208  0.00817918]\n",
      " [0.9913254  0.0086747 ]\n",
      " [0.9916345  0.00836557]\n",
      " [0.99596316 0.00403685]\n",
      " [0.99612    0.00388011]\n",
      " [0.9957962  0.00420376]\n",
      " [0.9954482  0.00455185]\n",
      " [0.99720263 0.00279741]\n",
      " [0.9968959  0.00310409]\n",
      " [0.9824724  0.01752751]\n",
      " [0.99184847 0.00815149]\n",
      " [0.96719915 0.03280092]\n",
      " [0.9711953  0.02880471]\n",
      " [0.99158937 0.00841065]\n",
      " [0.9564329  0.0435671 ]\n",
      " [0.99173576 0.00826424]\n",
      " [0.9746975  0.02530249]\n",
      " [0.31518096 0.684819  ]\n",
      " [0.94442207 0.05557797]\n",
      " [0.9686062  0.03139384]\n",
      " [0.93194526 0.06805474]\n",
      " [0.9832275  0.01677253]\n",
      " [0.68162316 0.3183768 ]\n",
      " [0.42888892 0.5711111 ]\n",
      " [0.02334507 0.9766549 ]\n",
      " [0.00910566 0.9908943 ]\n",
      " [0.00594319 0.9940568 ]\n",
      " [0.00350418 0.9964958 ]\n",
      " [0.00417072 0.9958293 ]\n",
      " [0.00623873 0.99376124]\n",
      " [0.00619343 0.9938066 ]\n",
      " [0.00408867 0.99591136]\n",
      " [0.00307894 0.99692106]\n",
      " [0.00477346 0.9952265 ]\n",
      " [0.00415293 0.99584705]\n",
      " [0.00308557 0.9969144 ]\n",
      " [0.00964862 0.9903514 ]\n",
      " [0.00962969 0.99037033]\n",
      " [0.01346684 0.98653316]\n",
      " [0.00965321 0.99034685]\n",
      " [0.00965029 0.99034977]\n",
      " [0.860407   0.1395931 ]\n",
      " [0.94343245 0.05656755]\n",
      " [0.9857983  0.01420178]\n",
      " [0.9949291  0.00507099]\n",
      " [0.9961617  0.00383829]\n",
      " [0.99617684 0.0038231 ]\n",
      " [0.9962872  0.00371274]\n",
      " [0.99499834 0.00500163]\n",
      " [0.9948256  0.00517448]\n",
      " [0.9913223  0.00867767]\n",
      " [0.9917943  0.00820566]\n",
      " [0.9881273  0.01187272]\n",
      " [0.9907334  0.00926658]\n",
      " [0.9906294  0.00937057]\n",
      " [0.9663068  0.03369319]\n",
      " [0.9345722  0.06542783]\n",
      " [0.9405183  0.05948163]\n",
      " [0.9815161  0.01848389]\n",
      " [0.9758242  0.02417578]\n",
      " [0.99642193 0.00357815]\n",
      " [0.99647236 0.00352769]\n",
      " [0.99630105 0.00369897]\n",
      " [0.9962315  0.00376854]\n",
      " [0.9747112  0.02528886]\n",
      " [0.98119074 0.01880924]\n",
      " [0.994998   0.00500207]\n",
      " [0.97701126 0.02298874]\n",
      " [0.99456656 0.00543343]\n",
      " [0.96511227 0.03488772]\n",
      " [0.9367148  0.06328516]\n",
      " [0.92267436 0.07732563]\n",
      " [0.97616935 0.02383062]\n",
      " [0.69818085 0.30181912]\n",
      " [0.6347322  0.3652678 ]\n",
      " [0.95486367 0.04513632]\n",
      " [0.9495493  0.05045071]\n",
      " [0.9480306  0.0519694 ]\n",
      " [0.95223665 0.04776331]\n",
      " [0.99290997 0.00709004]\n",
      " [0.99599445 0.00400549]\n",
      " [0.9960348  0.00396524]\n",
      " [0.99487203 0.00512795]\n",
      " [0.25927535 0.7407246 ]\n",
      " [0.01716874 0.98283124]\n",
      " [0.00654265 0.9934574 ]\n",
      " [0.00983973 0.9901603 ]\n",
      " [0.01134496 0.988655  ]\n",
      " [0.01045034 0.98954964]\n",
      " [0.00938209 0.99061793]\n",
      " [0.88210124 0.11789878]\n",
      " [0.96099293 0.03900704]\n",
      " [0.9938991  0.00610083]\n",
      " [0.99414957 0.00585042]\n",
      " [0.96148306 0.03851693]\n",
      " [0.9340969  0.06590321]\n",
      " [0.995717   0.00428302]\n",
      " [0.9907047  0.00929532]\n",
      " [0.98600173 0.01399826]\n",
      " [0.8869803  0.11301976]\n",
      " [0.14959145 0.8504086 ]\n",
      " [0.00820849 0.9917915 ]\n",
      " [0.02134029 0.9786597 ]\n",
      " [0.01095587 0.9890441 ]\n",
      " [0.08977948 0.91022056]\n",
      " [0.10139481 0.8986052 ]\n",
      " [0.97755855 0.0224414 ]\n",
      " [0.9840926  0.01590738]\n",
      " [0.99147683 0.0085232 ]\n",
      " [0.9913434  0.00865662]\n",
      " [0.99255073 0.00744931]\n",
      " [0.9932104  0.00678963]\n",
      " [0.9919962  0.00800376]\n",
      " [0.9960318  0.0039682 ]\n",
      " [0.9503811  0.04961888]\n",
      " [0.9891249  0.01087509]\n",
      " [0.9961373  0.00386261]\n",
      " [0.9906784  0.00932155]\n",
      " [0.6343899  0.36561012]\n",
      " [0.7707499  0.22925007]\n",
      " [0.57424104 0.42575893]\n",
      " [0.91563    0.08437001]\n",
      " [0.32972693 0.67027307]\n",
      " [0.74184173 0.2581582 ]\n",
      " [0.9886234  0.01137661]\n",
      " [0.7688428  0.23115723]\n",
      " [0.94160455 0.0583954 ]\n",
      " [0.9540517  0.04594833]\n",
      " [0.971334   0.02866604]\n",
      " [0.9857913  0.01420869]\n",
      " [0.9916583  0.00834166]\n",
      " [0.9689413  0.03105874]\n",
      " [0.9958118  0.0041882 ]\n",
      " [0.9955206  0.0044794 ]\n",
      " [0.9958949  0.00410511]\n",
      " [0.9941591  0.00584089]\n",
      " [0.9963044  0.00369563]\n",
      " [0.9955864  0.00441359]\n",
      " [0.99594444 0.00405554]\n",
      " [0.9929878  0.0070122 ]\n",
      " [0.9923011  0.00769891]\n",
      " [0.99030614 0.00969386]\n",
      " [0.990794   0.00920601]\n",
      " [0.9962735  0.00372651]\n",
      " [0.99096054 0.00903939]\n",
      " [0.9882606  0.01173943]\n",
      " [0.99554956 0.00445042]\n",
      " [0.98646533 0.01353472]\n",
      " [0.9860029  0.01399714]\n",
      " [0.9802719  0.01972817]\n",
      " [0.9873932  0.01260681]\n",
      " [0.99623233 0.00376772]\n",
      " [0.9962178  0.00378219]\n",
      " [0.99559045 0.00440955]\n",
      " [0.9958631  0.00413694]\n",
      " [0.99271095 0.00728905]\n",
      " [0.9726878  0.02731226]\n",
      " [0.99169475 0.00830526]\n",
      " [0.9827443  0.01725571]\n",
      " [0.99222445 0.00777556]\n",
      " [0.9173997  0.08260035]\n",
      " [0.6828847  0.3171153 ]\n",
      " [0.01313032 0.98686963]\n",
      " [0.04530106 0.9546989 ]\n",
      " [0.1821301  0.81786996]\n",
      " [0.6660676  0.3339324 ]\n",
      " [0.98794204 0.01205794]\n",
      " [0.9654444  0.03455561]\n",
      " [0.08532614 0.9146738 ]\n",
      " [0.00977506 0.9902249 ]\n",
      " [0.01591935 0.9840806 ]\n",
      " [0.85092837 0.1490717 ]\n",
      " [0.805291   0.194709  ]\n",
      " [0.980703   0.01929693]\n",
      " [0.94365484 0.05634522]\n",
      " [0.9868444  0.0131556 ]\n",
      " [0.99205935 0.00794064]\n",
      " [0.986296   0.01370401]\n",
      " [0.99134153 0.00865846]\n",
      " [0.9913248  0.00867518]\n",
      " [0.9909847  0.00901529]\n",
      " [0.98817635 0.01182369]\n",
      " [0.99138826 0.00861168]\n",
      " [0.99140185 0.00859817]\n",
      " [0.99505085 0.00494916]\n",
      " [0.99601245 0.00398753]\n",
      " [0.9959831  0.00401689]]\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions shape:\", predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c724d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decision(test,pred):\n",
    "    h = np.array(pred)\n",
    "    action = []\n",
    "    status = \"N\"\n",
    "    for i in range(len(h)):\n",
    "        if h[i][0] == max(h[i]):\n",
    "            h[i] = [1,0]\n",
    "            if status == \"N\":\n",
    "                action.append(\"Buy\")\n",
    "                status = \"Buy\"\n",
    "            else:\n",
    "                action.append(\"Hold\")\n",
    "        else:\n",
    "            h[i] = [0,1]\n",
    "            if status == \"Buy\":\n",
    "                action.append(\"Sell\")\n",
    "                status = \"N\"\n",
    "            else:\n",
    "                action.append(\"Hold\")\n",
    "                \n",
    "    backtest = test[[\"open\"]][10:]\n",
    "    backtest.columns = [\"Open\"]\n",
    "    return (backtest,pd.DataFrame(action,index=test[10:].index,columns=[\"Action\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ce3418ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20180117</th>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180118</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180119</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180122</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180123</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191224</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191226</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191227</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191230</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191231</th>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>493 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Action\n",
       "DATE           \n",
       "20180117    Buy\n",
       "20180118   Hold\n",
       "20180119   Hold\n",
       "20180122   Hold\n",
       "20180123   Hold\n",
       "...         ...\n",
       "20191224   Hold\n",
       "20191226   Hold\n",
       "20191227   Hold\n",
       "20191230   Hold\n",
       "20191231   Hold\n",
       "\n",
       "[493 rows x 1 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtestdata,final_pred = convert_decision(test_X,predictions)\n",
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffa3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4ef890",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3faf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Input ##########################\n",
    "# For hist_price_data: index=[\"date\"], columns = [\"Open\"]\n",
    "# For pred_action: index=[\"date\"], columns = [\"Action\"] (Buy/Sell)\n",
    "################### Output #########################\n",
    "# 1. trading record\n",
    "# 2. total profit\n",
    "class backtest:\n",
    "    hpd = \"\"\n",
    "    pred_action=\"\"\n",
    "    trade_record=pd.DataFrame(index=[],\n",
    "                              columns=[\"Action\",\"Price\",\"Position\",\"Cash\",\"Pos_Bal\",\"Cash_Bal\",\"Cum_Profit\"],\n",
    "                             )\n",
    "    capital = 0\n",
    "    cash_balance = 0\n",
    "    profit = 0\n",
    "    handle_fee = 0\n",
    "    position = 0\n",
    "    last_price = 0\n",
    "    \n",
    "    def __init__(self,hist_price_data,pred_action,capital,handling_fee):\n",
    "        self.hpd = hist_price_data\n",
    "        self.pred_action = pred_action\n",
    "        self.capital = capital\n",
    "        self.cash_balance = capital\n",
    "        self.handle_fee = handling_fee\n",
    "        trade_record=pd.DataFrame(index=[],\n",
    "                                  columns=[\"Action\",\"Price\",\"Position\",\"Cash\",\"Pos_Bal\",\"Cash_Bal\",\"Cum_Profit\"],\n",
    "                                 )\n",
    "        \n",
    "    def start_test(self):        \n",
    "        # For loop to iterate the data\n",
    "        for ind in self.pred_action.index:\n",
    "            # Update latest price\n",
    "            self.last_price = self.hpd.loc[ind,\"Open\"]\n",
    "            \n",
    "            if self.pred_action.loc[ind,\"Action\"].lower() == \"buy\":\n",
    "                self.buy(ind,self.hpd.loc[ind,\"Open\"])\n",
    "            elif self.pred_action.loc[ind,\"Action\"].lower() == \"sell\":\n",
    "                self.sell(ind,self.hpd.loc[ind,\"Open\"])\n",
    "            else:\n",
    "                print(\"Did not buy at \" + str(ind))\n",
    "            \n",
    "                \n",
    "        \n",
    "    def mark_down_record(self,date,action,price,pos_delta,cash_delta):\n",
    "        self.trade_record.loc[date,\"Action\"] = action\n",
    "        self.trade_record.loc[date,\"Price\"] = price\n",
    "        self.trade_record.loc[date,\"Position\"] = pos_delta\n",
    "        self.trade_record.loc[date,\"Cash\"] = cash_delta\n",
    "        \n",
    "        self.trade_record.loc[date,\"Pos_Bal\"] = round(self.position,4)\n",
    "        self.trade_record.loc[date,\"Cash_Bal\"] = round(self.cash_balance,3)\n",
    "        self.trade_record.loc[date,\"Cum_Profit\"] = round(self.get_profit(),3)\n",
    "        \n",
    "    def buy(self,date,price):\n",
    "        # Assume use all money to buy all\n",
    "        buy_flag = False\n",
    "        buy_pos = floor(self.cash_balance / price)\n",
    "        for i in range(buy_pos):\n",
    "            act_buy_pos = buy_pos - i\n",
    "            total_amt = act_buy_pos*price*(1+self.handle_fee)\n",
    "            if self.cash_balance > total_amt:\n",
    "                self.position += act_buy_pos\n",
    "                self.cash_balance -= total_amt\n",
    "                self.mark_down_record(date,\n",
    "                                 \"Buy\",\n",
    "                                 price,\n",
    "                                 act_buy_pos,\n",
    "                                 -total_amt)\n",
    "                print(\"Bought at\",date,\"with price =\", price)\n",
    "                buy_flag = True\n",
    "                break\n",
    "        if not buy_flag:\n",
    "            print(\"You do not have enough money to buy!\")\n",
    "    \n",
    "    def sell(self,date,price):\n",
    "        # Assume sell all position\n",
    "        sell_pos = self.position\n",
    "        total_amt = sell_pos*price*(1-self.handle_fee)\n",
    "        if self.position >= 1:\n",
    "            self.position -= sell_pos\n",
    "            self.cash_balance += total_amt\n",
    "            self.mark_down_record(date,\n",
    "                             \"Sell\",\n",
    "                             price,\n",
    "                             -sell_pos,\n",
    "                             total_amt)\n",
    "            print(\"Sold at\",date,\"with price =\", price)\n",
    "        else:\n",
    "            print(\"You do not have enough position to sell!\")\n",
    "    \n",
    "    def get_profit(self):\n",
    "        return self.get_cash_balance()+self.get_last_price()*self.get_position()-self.get_capital()\n",
    "    \n",
    "    def get_capital(self):\n",
    "        return self.capital\n",
    "    \n",
    "    def get_last_price(self):\n",
    "        return self.last_price\n",
    "    \n",
    "    def get_cash_balance(self):\n",
    "        return self.cash_balance\n",
    "    \n",
    "    def get_position(self):\n",
    "        return self.position\n",
    "    \n",
    "    def get_amounnt(self):\n",
    "        return self.capital+self.profit\n",
    "    \n",
    "    def print_trade_record(self):\n",
    "        print(self.trade_record)\n",
    "    \n",
    "    def print_profit(self):\n",
    "        print(\"Current Profit:\",self.get_profit())\n",
    "    \n",
    "    def export_trade_record(self,stock):\n",
    "        # Save the trade record to the path\n",
    "        self.trade_record.to_csv(\"trade_record/\"+stock+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e54b2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(backtest,capital):\n",
    "    balance = capital\n",
    "    first_price = backtest.iloc[0,0]\n",
    "    last_price = backtest.iloc[-1,0]\n",
    "    buy_pos = floor(capital / first_price)\n",
    "    total_amt = buy_pos*first_price*(1+handling_fee)\n",
    "    while capital < total_amt:\n",
    "        buy_pos -= 1\n",
    "        total_amt = buy_pos*first_price*(1+handling_fee)\n",
    "    position = buy_pos\n",
    "    balance -= total_amt\n",
    "    print(\"If buy at\", backtest.index[0],\"with price =\",first_price)\n",
    "    print(\"and do nothing\")\n",
    "    print(\"Current Profit:\",last_price*position+balance-capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7e3a8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_1 = backtest(backtestdata,final_pred,10000,0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "975f6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bought at 20180117 with price = 42.181\n",
      "Did not buy at 20180118\n",
      "Did not buy at 20180119\n",
      "Did not buy at 20180122\n",
      "Did not buy at 20180123\n",
      "Did not buy at 20180124\n",
      "Sold at 20180125 with price = 41.783\n",
      "Did not buy at 20180126\n",
      "Did not buy at 20180129\n",
      "Did not buy at 20180130\n",
      "Did not buy at 20180131\n",
      "Did not buy at 20180201\n",
      "Did not buy at 20180202\n",
      "Did not buy at 20180205\n",
      "Did not buy at 20180206\n",
      "Did not buy at 20180207\n",
      "Did not buy at 20180208\n",
      "Did not buy at 20180209\n",
      "Did not buy at 20180212\n",
      "Did not buy at 20180213\n",
      "Bought at 20180214 with price = 39.197\n",
      "Did not buy at 20180215\n",
      "Did not buy at 20180216\n",
      "Did not buy at 20180220\n",
      "Did not buy at 20180221\n",
      "Did not buy at 20180222\n",
      "Did not buy at 20180223\n",
      "Did not buy at 20180226\n",
      "Did not buy at 20180227\n",
      "Did not buy at 20180228\n",
      "Did not buy at 20180301\n",
      "Did not buy at 20180302\n",
      "Did not buy at 20180305\n",
      "Did not buy at 20180306\n",
      "Did not buy at 20180307\n",
      "Did not buy at 20180308\n",
      "Did not buy at 20180309\n",
      "Did not buy at 20180312\n",
      "Did not buy at 20180313\n",
      "Did not buy at 20180314\n",
      "Did not buy at 20180315\n",
      "Did not buy at 20180316\n",
      "Did not buy at 20180319\n",
      "Did not buy at 20180320\n",
      "Sold at 20180321 with price = 42.087\n",
      "Did not buy at 20180322\n",
      "Did not buy at 20180323\n",
      "Did not buy at 20180326\n",
      "Did not buy at 20180327\n",
      "Did not buy at 20180328\n",
      "Did not buy at 20180329\n",
      "Did not buy at 20180402\n",
      "Did not buy at 20180403\n",
      "Did not buy at 20180404\n",
      "Did not buy at 20180405\n",
      "Bought at 20180406 with price = 41.105\n",
      "Did not buy at 20180409\n",
      "Did not buy at 20180410\n",
      "Did not buy at 20180411\n",
      "Did not buy at 20180412\n",
      "Did not buy at 20180413\n",
      "Did not buy at 20180416\n",
      "Did not buy at 20180417\n",
      "Did not buy at 20180418\n",
      "Did not buy at 20180419\n",
      "Did not buy at 20180420\n",
      "Sold at 20180423 with price = 40.112\n",
      "Did not buy at 20180424\n",
      "Did not buy at 20180425\n",
      "Did not buy at 20180426\n",
      "Did not buy at 20180427\n",
      "Did not buy at 20180430\n",
      "Did not buy at 20180501\n",
      "Bought at 20180502 with price = 42.131\n",
      "Did not buy at 20180503\n",
      "Did not buy at 20180504\n",
      "Did not buy at 20180507\n",
      "Did not buy at 20180508\n",
      "Did not buy at 20180509\n",
      "Did not buy at 20180510\n",
      "Did not buy at 20180511\n",
      "Did not buy at 20180514\n",
      "Did not buy at 20180515\n",
      "Did not buy at 20180516\n",
      "Did not buy at 20180517\n",
      "Did not buy at 20180518\n",
      "Did not buy at 20180521\n",
      "Did not buy at 20180522\n",
      "Did not buy at 20180523\n",
      "Did not buy at 20180524\n",
      "Did not buy at 20180525\n",
      "Did not buy at 20180529\n",
      "Did not buy at 20180530\n",
      "Did not buy at 20180531\n",
      "Did not buy at 20180601\n",
      "Did not buy at 20180604\n",
      "Did not buy at 20180605\n",
      "Did not buy at 20180606\n",
      "Did not buy at 20180607\n",
      "Did not buy at 20180608\n",
      "Did not buy at 20180611\n",
      "Did not buy at 20180612\n",
      "Did not buy at 20180613\n",
      "Sold at 20180614 with price = 46.235\n",
      "Did not buy at 20180615\n",
      "Did not buy at 20180618\n",
      "Did not buy at 20180619\n",
      "Did not buy at 20180620\n",
      "Did not buy at 20180621\n",
      "Did not buy at 20180622\n",
      "Did not buy at 20180625\n",
      "Did not buy at 20180626\n",
      "Did not buy at 20180627\n",
      "Did not buy at 20180628\n",
      "Did not buy at 20180629\n",
      "Did not buy at 20180702\n",
      "Did not buy at 20180703\n",
      "Did not buy at 20180705\n",
      "Bought at 20180706 with price = 44.754\n",
      "Did not buy at 20180709\n",
      "Did not buy at 20180710\n",
      "Did not buy at 20180711\n",
      "Did not buy at 20180712\n",
      "Did not buy at 20180713\n",
      "Did not buy at 20180716\n",
      "Did not buy at 20180717\n",
      "Did not buy at 20180718\n",
      "Did not buy at 20180719\n",
      "Did not buy at 20180720\n",
      "Did not buy at 20180723\n",
      "Did not buy at 20180724\n",
      "Did not buy at 20180725\n",
      "Did not buy at 20180726\n",
      "Did not buy at 20180727\n",
      "Did not buy at 20180730\n",
      "Did not buy at 20180731\n",
      "Did not buy at 20180801\n",
      "Did not buy at 20180802\n",
      "Did not buy at 20180803\n",
      "Did not buy at 20180806\n",
      "Did not buy at 20180807\n",
      "Did not buy at 20180808\n",
      "Did not buy at 20180809\n",
      "Did not buy at 20180810\n",
      "Did not buy at 20180813\n",
      "Did not buy at 20180814\n",
      "Did not buy at 20180815\n",
      "Did not buy at 20180816\n",
      "Did not buy at 20180817\n",
      "Did not buy at 20180820\n",
      "Did not buy at 20180821\n",
      "Did not buy at 20180822\n",
      "Did not buy at 20180823\n",
      "Did not buy at 20180824\n",
      "Did not buy at 20180827\n",
      "Did not buy at 20180828\n",
      "Did not buy at 20180829\n",
      "Did not buy at 20180830\n",
      "Did not buy at 20180831\n",
      "Did not buy at 20180904\n",
      "Did not buy at 20180905\n",
      "Did not buy at 20180906\n",
      "Did not buy at 20180907\n",
      "Did not buy at 20180910\n",
      "Sold at 20180911 with price = 52.8\n",
      "Did not buy at 20180912\n",
      "Did not buy at 20180913\n",
      "Did not buy at 20180914\n",
      "Did not buy at 20180917\n",
      "Did not buy at 20180918\n",
      "Did not buy at 20180919\n",
      "Did not buy at 20180920\n",
      "Did not buy at 20180921\n",
      "Bought at 20180924 with price = 52.517\n",
      "Sold at 20180925 with price = 53.222\n",
      "Did not buy at 20180926\n",
      "Bought at 20180927 with price = 54.212\n",
      "Did not buy at 20180928\n",
      "Did not buy at 20181001\n",
      "Did not buy at 20181002\n",
      "Did not buy at 20181003\n",
      "Did not buy at 20181004\n",
      "Did not buy at 20181005\n",
      "Did not buy at 20181008\n",
      "Did not buy at 20181009\n",
      "Sold at 20181010 with price = 54.608\n",
      "Did not buy at 20181011\n",
      "Did not buy at 20181012\n",
      "Did not buy at 20181015\n",
      "Did not buy at 20181016\n",
      "Did not buy at 20181017\n",
      "Did not buy at 20181018\n",
      "Did not buy at 20181019\n",
      "Did not buy at 20181022\n",
      "Bought at 20181023 with price = 52.274\n",
      "Did not buy at 20181024\n",
      "Did not buy at 20181025\n",
      "Sold at 20181026 with price = 52.292\n",
      "Did not buy at 20181029\n",
      "Did not buy at 20181030\n",
      "Did not buy at 20181031\n",
      "Did not buy at 20181101\n",
      "Bought at 20181102 with price = 50.754\n",
      "Sold at 20181105 with price = 49.484\n",
      "Did not buy at 20181106\n",
      "Did not buy at 20181107\n",
      "Did not buy at 20181108\n",
      "Did not buy at 20181109\n",
      "Did not buy at 20181112\n",
      "Did not buy at 20181113\n",
      "Did not buy at 20181114\n",
      "Did not buy at 20181115\n",
      "Did not buy at 20181116\n",
      "Did not buy at 20181119\n",
      "Did not buy at 20181120\n",
      "Did not buy at 20181121\n",
      "Did not buy at 20181123\n",
      "Did not buy at 20181126\n",
      "Did not buy at 20181127\n",
      "Did not buy at 20181128\n",
      "Did not buy at 20181129\n",
      "Did not buy at 20181130\n",
      "Did not buy at 20181203\n",
      "Bought at 20181204 with price = 43.979\n",
      "Did not buy at 20181206\n",
      "Did not buy at 20181207\n",
      "Sold at 20181210 with price = 40.104\n",
      "Did not buy at 20181211\n",
      "Did not buy at 20181212\n",
      "Did not buy at 20181213\n",
      "Did not buy at 20181214\n",
      "Did not buy at 20181217\n",
      "Did not buy at 20181218\n",
      "Did not buy at 20181219\n",
      "Did not buy at 20181220\n",
      "Did not buy at 20181221\n",
      "Did not buy at 20181224\n",
      "Did not buy at 20181226\n",
      "Did not buy at 20181227\n",
      "Did not buy at 20181228\n",
      "Did not buy at 20181231\n",
      "Did not buy at 20190102\n",
      "Did not buy at 20190103\n",
      "Did not buy at 20190104\n",
      "Did not buy at 20190107\n",
      "Did not buy at 20190108\n",
      "Did not buy at 20190109\n",
      "Bought at 20190110 with price = 37.066\n",
      "Sold at 20190111 with price = 37.159\n",
      "Did not buy at 20190114\n",
      "Did not buy at 20190115\n",
      "Bought at 20190116 with price = 37.209\n",
      "Did not buy at 20190117\n",
      "Did not buy at 20190118\n",
      "Did not buy at 20190122\n",
      "Did not buy at 20190123\n",
      "Did not buy at 20190124\n",
      "Did not buy at 20190125\n",
      "Did not buy at 20190128\n",
      "Did not buy at 20190129\n",
      "Did not buy at 20190130\n",
      "Did not buy at 20190131\n",
      "Did not buy at 20190201\n",
      "Did not buy at 20190204\n",
      "Did not buy at 20190205\n",
      "Did not buy at 20190206\n",
      "Did not buy at 20190207\n",
      "Did not buy at 20190208\n",
      "Did not buy at 20190211\n",
      "Did not buy at 20190212\n",
      "Did not buy at 20190213\n",
      "Did not buy at 20190214\n",
      "Did not buy at 20190215\n",
      "Did not buy at 20190219\n",
      "Did not buy at 20190220\n",
      "Did not buy at 20190221\n",
      "Did not buy at 20190222\n",
      "Did not buy at 20190225\n",
      "Did not buy at 20190226\n",
      "Did not buy at 20190227\n",
      "Did not buy at 20190228\n",
      "Did not buy at 20190301\n",
      "Did not buy at 20190304\n",
      "Did not buy at 20190305\n",
      "Did not buy at 20190306\n",
      "Did not buy at 20190307\n",
      "Did not buy at 20190308\n",
      "Sold at 20190311 with price = 42.837\n",
      "Bought at 20190312 with price = 43.937\n",
      "Did not buy at 20190313\n",
      "Did not buy at 20190314\n",
      "Did not buy at 20190315\n",
      "Did not buy at 20190318\n",
      "Did not buy at 20190319\n",
      "Did not buy at 20190320\n",
      "Did not buy at 20190321\n",
      "Did not buy at 20190322\n",
      "Did not buy at 20190325\n",
      "Did not buy at 20190326\n",
      "Did not buy at 20190327\n",
      "Did not buy at 20190328\n",
      "Did not buy at 20190329\n",
      "Did not buy at 20190401\n",
      "Did not buy at 20190402\n",
      "Did not buy at 20190403\n",
      "Did not buy at 20190404\n",
      "Did not buy at 20190405\n",
      "Did not buy at 20190408\n",
      "Did not buy at 20190409\n",
      "Did not buy at 20190410\n",
      "Did not buy at 20190411\n",
      "Did not buy at 20190412\n",
      "Did not buy at 20190415\n",
      "Did not buy at 20190416\n",
      "Did not buy at 20190417\n",
      "Did not buy at 20190418\n",
      "Did not buy at 20190422\n",
      "Did not buy at 20190423\n",
      "Did not buy at 20190424\n",
      "Did not buy at 20190425\n",
      "Did not buy at 20190426\n",
      "Did not buy at 20190429\n",
      "Did not buy at 20190430\n",
      "Sold at 20190501 with price = 51.23\n",
      "Bought at 20190502 with price = 51.222\n",
      "Did not buy at 20190503\n",
      "Did not buy at 20190506\n",
      "Did not buy at 20190507\n",
      "Did not buy at 20190508\n",
      "Sold at 20190509 with price = 48.914\n",
      "Did not buy at 20190510\n",
      "Did not buy at 20190513\n",
      "Did not buy at 20190514\n",
      "Did not buy at 20190515\n",
      "Did not buy at 20190516\n",
      "Did not buy at 20190517\n",
      "Did not buy at 20190520\n",
      "Did not buy at 20190521\n",
      "Did not buy at 20190522\n",
      "Did not buy at 20190523\n",
      "Did not buy at 20190524\n",
      "Did not buy at 20190528\n",
      "Did not buy at 20190529\n",
      "Did not buy at 20190530\n",
      "Did not buy at 20190531\n",
      "Did not buy at 20190603\n",
      "Did not buy at 20190604\n",
      "Bought at 20190605 with price = 45.154\n",
      "Did not buy at 20190606\n",
      "Did not buy at 20190607\n",
      "Did not buy at 20190610\n",
      "Did not buy at 20190611\n",
      "Did not buy at 20190612\n",
      "Did not buy at 20190613\n",
      "Did not buy at 20190614\n",
      "Did not buy at 20190617\n",
      "Did not buy at 20190618\n",
      "Did not buy at 20190619\n",
      "Did not buy at 20190620\n",
      "Did not buy at 20190621\n",
      "Did not buy at 20190624\n",
      "Did not buy at 20190625\n",
      "Did not buy at 20190626\n",
      "Did not buy at 20190627\n",
      "Did not buy at 20190628\n",
      "Did not buy at 20190701\n",
      "Did not buy at 20190702\n",
      "Did not buy at 20190703\n",
      "Did not buy at 20190705\n",
      "Did not buy at 20190708\n",
      "Did not buy at 20190709\n",
      "Did not buy at 20190710\n",
      "Did not buy at 20190711\n",
      "Did not buy at 20190712\n",
      "Did not buy at 20190715\n",
      "Did not buy at 20190716\n",
      "Did not buy at 20190717\n",
      "Did not buy at 20190718\n",
      "Did not buy at 20190719\n",
      "Did not buy at 20190722\n",
      "Did not buy at 20190723\n",
      "Did not buy at 20190724\n",
      "Did not buy at 20190725\n",
      "Did not buy at 20190726\n",
      "Did not buy at 20190729\n",
      "Did not buy at 20190730\n",
      "Did not buy at 20190731\n",
      "Did not buy at 20190801\n",
      "Did not buy at 20190802\n",
      "Sold at 20190805 with price = 48.512\n",
      "Did not buy at 20190806\n",
      "Did not buy at 20190807\n",
      "Did not buy at 20190808\n",
      "Did not buy at 20190809\n",
      "Did not buy at 20190812\n",
      "Did not buy at 20190813\n",
      "Bought at 20190814 with price = 49.968\n",
      "Did not buy at 20190815\n",
      "Did not buy at 20190816\n",
      "Did not buy at 20190819\n",
      "Did not buy at 20190820\n",
      "Did not buy at 20190821\n",
      "Did not buy at 20190822\n",
      "Did not buy at 20190823\n",
      "Did not buy at 20190826\n",
      "Did not buy at 20190827\n",
      "Sold at 20190828 with price = 50.203\n",
      "Did not buy at 20190829\n",
      "Did not buy at 20190830\n",
      "Did not buy at 20190903\n",
      "Did not buy at 20190904\n",
      "Did not buy at 20190905\n",
      "Bought at 20190906 with price = 52.649\n",
      "Did not buy at 20190909\n",
      "Did not buy at 20190910\n",
      "Did not buy at 20190911\n",
      "Did not buy at 20190912\n",
      "Did not buy at 20190913\n",
      "Did not buy at 20190916\n",
      "Did not buy at 20190917\n",
      "Did not buy at 20190918\n",
      "Did not buy at 20190919\n",
      "Did not buy at 20190920\n",
      "Did not buy at 20190923\n",
      "Did not buy at 20190924\n",
      "Did not buy at 20190925\n",
      "Did not buy at 20190926\n",
      "Did not buy at 20190927\n",
      "Sold at 20190930 with price = 54.337\n",
      "Bought at 20191001 with price = 55.362\n",
      "Did not buy at 20191002\n",
      "Did not buy at 20191003\n",
      "Did not buy at 20191004\n",
      "Did not buy at 20191007\n",
      "Did not buy at 20191008\n",
      "Did not buy at 20191009\n",
      "Did not buy at 20191010\n",
      "Did not buy at 20191011\n",
      "Did not buy at 20191014\n",
      "Did not buy at 20191015\n",
      "Did not buy at 20191016\n",
      "Did not buy at 20191017\n",
      "Did not buy at 20191018\n",
      "Did not buy at 20191021\n",
      "Did not buy at 20191022\n",
      "Did not buy at 20191023\n",
      "Did not buy at 20191024\n",
      "Did not buy at 20191025\n",
      "Did not buy at 20191028\n",
      "Did not buy at 20191029\n",
      "Did not buy at 20191030\n",
      "Did not buy at 20191031\n",
      "Did not buy at 20191101\n",
      "Did not buy at 20191104\n",
      "Did not buy at 20191105\n",
      "Did not buy at 20191106\n",
      "Did not buy at 20191107\n",
      "Did not buy at 20191108\n",
      "Did not buy at 20191111\n",
      "Did not buy at 20191112\n",
      "Did not buy at 20191113\n",
      "Did not buy at 20191114\n",
      "Did not buy at 20191115\n",
      "Did not buy at 20191118\n",
      "Did not buy at 20191119\n",
      "Did not buy at 20191120\n",
      "Did not buy at 20191121\n",
      "Did not buy at 20191122\n",
      "Sold at 20191125 with price = 64.812\n",
      "Did not buy at 20191126\n",
      "Did not buy at 20191127\n",
      "Bought at 20191129 with price = 65.768\n",
      "Did not buy at 20191202\n",
      "Did not buy at 20191203\n",
      "Sold at 20191204 with price = 64.405\n",
      "Did not buy at 20191205\n",
      "Did not buy at 20191206\n",
      "Bought at 20191209 with price = 66.61\n",
      "Did not buy at 20191210\n",
      "Did not buy at 20191211\n",
      "Did not buy at 20191212\n",
      "Did not buy at 20191213\n",
      "Did not buy at 20191216\n",
      "Did not buy at 20191217\n",
      "Did not buy at 20191218\n",
      "Did not buy at 20191219\n",
      "Did not buy at 20191220\n",
      "Did not buy at 20191223\n",
      "Did not buy at 20191224\n",
      "Did not buy at 20191226\n",
      "Did not buy at 20191227\n",
      "Did not buy at 20191230\n",
      "Did not buy at 20191231\n"
     ]
    }
   ],
   "source": [
    "backtest_1.start_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "46fde8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Action   Price Position          Cash Pos_Bal   Cash_Bal Cum_Profit\n",
      "20180117    Buy  42.181      236  -9974.625432     236     25.375    -19.909\n",
      "20180125   Sell  41.783     -236   9841.066424       0   9866.441   -133.559\n",
      "20180214    Buy  39.197      251  -9858.123894     251      8.317   -153.236\n",
      "20180321   Sell  42.087     -251  10542.709326       0  10551.026    551.026\n",
      "20180406    Buy  41.105      256  -10543.92576     256      7.101    529.981\n",
      "20180423   Sell  40.112     -256  10248.134656       0  10255.235    255.235\n",
      "20180502    Buy  42.131      242 -10216.093404     242     39.142    234.844\n",
      "20180614   Sell  46.235     -242   11166.49226       0  11205.634   1205.634\n",
      "20180706    Buy  44.754      249 -11166.033492     249     39.601   1183.347\n",
      "20180911   Sell    52.8     -249    13120.9056       0  13160.506   3160.506\n",
      "20180924    Buy  52.517      250   -13155.5085     250      4.998   3134.248\n",
      "20180925   Sell  53.222     -250     13278.889       0  13283.887   3283.887\n",
      "20180927    Buy  54.212      244 -13254.183456     244     29.703   3257.431\n",
      "20181010   Sell  54.608     -244  13297.703296       0  13327.407   3327.407\n",
      "20181023    Buy  52.274      254 -13304.151192     254     23.255   3300.851\n",
      "20181026   Sell  52.292     -254  13255.603664       0  13278.859   3278.859\n",
      "20181102    Buy  50.754      261 -13273.287588     261      5.572   3252.366\n",
      "20181105   Sell  49.484     -261  12889.493352       0  12895.065   2895.065\n",
      "20181204    Buy  43.979      292 -12867.551736     292     27.513   2869.381\n",
      "20181210   Sell  40.104     -292  11686.947264       0   11714.46    1714.46\n",
      "20190110    Buy  37.066      315  -11699.14158     315     15.319   1691.109\n",
      "20190111   Sell  37.159     -315   11681.67483       0  11696.994   1696.994\n",
      "20190116    Buy  37.209      313 -11669.709834     313     27.284   1673.701\n",
      "20190311   Sell  42.837     -313  13381.165038       0  13408.449   3408.449\n",
      "20190312    Buy  43.937      304 -13383.561696     304     24.887   3381.735\n",
      "20190501   Sell   51.23     -304   15542.77216       0  15567.659   5567.659\n",
      "20190502    Buy  51.222      303 -15551.306532     303     16.353   5536.619\n",
      "20190509   Sell  48.914     -303  14791.300116       0  14807.653   4807.653\n",
      "20190605    Buy  45.154      327 -14794.888716     327     12.764   4778.122\n",
      "20190805   Sell  48.512     -327  15831.697152       0  15844.461   5844.461\n",
      "20190814    Buy  49.968      316 -15821.467776     316     22.994   5812.882\n",
      "20190828   Sell  50.203     -316  15832.419704       0  15855.413   5855.413\n",
      "20190906    Buy  52.649      300   -15826.2894     300     29.124   5823.824\n",
      "20190930   Sell  54.337     -300    16268.4978       0  16297.622   6297.622\n",
      "20191001    Buy  55.362      293 -16253.508132     293     44.114    6265.18\n",
      "20191125   Sell  64.812     -293  18951.936168       0   18996.05    8996.05\n",
      "20191129    Buy  65.768      288 -18979.066368     288     16.983   8958.167\n",
      "20191204   Sell  64.405     -288   18511.54272       0  18528.526   8528.526\n",
      "20191209    Buy   66.61      277  -18487.87194     277     40.654   8491.624\n",
      "==========================\n",
      "Current Profit: 9853.079102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "backtest_1.print_trade_record()\n",
    "print('==========================')\n",
    "backtest_1.export_trade_record(\"AAPL\")\n",
    "backtest_1.print_profit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "99cd5d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If buy at 20180117 with price = 42.181\n",
      "Current Profit: 6905.274568000001\n"
     ]
    }
   ],
   "source": [
    "# If do nothing\n",
    "do_nothing(backtestdata,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f3418",
   "metadata": {},
   "source": [
    "### Other data range testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cbd8555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bought at 20190116 with price = 37.209\n",
      "Did not buy at 20190117\n",
      "Did not buy at 20190118\n",
      "Did not buy at 20190122\n",
      "Did not buy at 20190123\n",
      "Did not buy at 20190124\n",
      "Did not buy at 20190125\n",
      "Did not buy at 20190128\n",
      "Did not buy at 20190129\n",
      "Did not buy at 20190130\n",
      "Did not buy at 20190131\n",
      "Did not buy at 20190201\n",
      "Did not buy at 20190204\n",
      "Did not buy at 20190205\n",
      "Did not buy at 20190206\n",
      "Did not buy at 20190207\n",
      "Did not buy at 20190208\n",
      "Did not buy at 20190211\n",
      "Did not buy at 20190212\n",
      "Did not buy at 20190213\n",
      "Did not buy at 20190214\n",
      "Did not buy at 20190215\n",
      "Did not buy at 20190219\n",
      "Did not buy at 20190220\n",
      "Did not buy at 20190221\n",
      "Did not buy at 20190222\n",
      "Did not buy at 20190225\n",
      "Did not buy at 20190226\n",
      "Did not buy at 20190227\n",
      "Did not buy at 20190228\n",
      "Did not buy at 20190301\n",
      "Did not buy at 20190304\n",
      "Did not buy at 20190305\n",
      "Did not buy at 20190306\n",
      "Did not buy at 20190307\n",
      "Did not buy at 20190308\n",
      "Sold at 20190311 with price = 42.837\n",
      "Bought at 20190312 with price = 43.937\n",
      "Did not buy at 20190313\n",
      "Did not buy at 20190314\n",
      "Did not buy at 20190315\n",
      "Did not buy at 20190318\n",
      "Did not buy at 20190319\n",
      "Did not buy at 20190320\n",
      "Did not buy at 20190321\n",
      "Did not buy at 20190322\n",
      "Did not buy at 20190325\n",
      "Did not buy at 20190326\n",
      "Did not buy at 20190327\n",
      "Did not buy at 20190328\n",
      "Did not buy at 20190329\n",
      "Did not buy at 20190401\n",
      "Did not buy at 20190402\n",
      "Did not buy at 20190403\n",
      "Did not buy at 20190404\n",
      "Did not buy at 20190405\n",
      "Did not buy at 20190408\n",
      "Did not buy at 20190409\n",
      "Did not buy at 20190410\n",
      "Did not buy at 20190411\n",
      "Did not buy at 20190412\n",
      "Did not buy at 20190415\n",
      "Did not buy at 20190416\n",
      "Did not buy at 20190417\n",
      "Did not buy at 20190418\n",
      "Did not buy at 20190422\n",
      "Did not buy at 20190423\n",
      "Did not buy at 20190424\n",
      "Did not buy at 20190425\n",
      "Did not buy at 20190426\n",
      "Did not buy at 20190429\n",
      "Did not buy at 20190430\n",
      "Sold at 20190501 with price = 51.23\n",
      "Bought at 20190502 with price = 51.222\n",
      "Did not buy at 20190503\n",
      "Did not buy at 20190506\n",
      "Did not buy at 20190507\n",
      "Did not buy at 20190508\n",
      "Sold at 20190509 with price = 48.914\n",
      "Did not buy at 20190510\n",
      "Did not buy at 20190513\n",
      "Did not buy at 20190514\n",
      "Did not buy at 20190515\n",
      "Did not buy at 20190516\n",
      "Did not buy at 20190517\n",
      "Did not buy at 20190520\n",
      "Did not buy at 20190521\n",
      "Did not buy at 20190522\n",
      "Did not buy at 20190523\n",
      "Did not buy at 20190524\n",
      "Did not buy at 20190528\n",
      "Did not buy at 20190529\n",
      "Did not buy at 20190530\n",
      "Did not buy at 20190531\n",
      "Did not buy at 20190603\n",
      "Did not buy at 20190604\n",
      "Bought at 20190605 with price = 45.154\n",
      "Did not buy at 20190606\n",
      "Did not buy at 20190607\n",
      "Did not buy at 20190610\n",
      "Did not buy at 20190611\n",
      "Did not buy at 20190612\n",
      "Did not buy at 20190613\n",
      "Did not buy at 20190614\n",
      "Did not buy at 20190617\n",
      "Did not buy at 20190618\n",
      "Did not buy at 20190619\n",
      "Did not buy at 20190620\n",
      "Did not buy at 20190621\n",
      "Did not buy at 20190624\n",
      "Did not buy at 20190625\n",
      "Did not buy at 20190626\n",
      "Did not buy at 20190627\n",
      "Did not buy at 20190628\n",
      "Did not buy at 20190701\n",
      "Did not buy at 20190702\n",
      "Did not buy at 20190703\n",
      "Did not buy at 20190705\n",
      "Did not buy at 20190708\n",
      "Did not buy at 20190709\n",
      "Did not buy at 20190710\n",
      "Did not buy at 20190711\n",
      "Did not buy at 20190712\n",
      "Did not buy at 20190715\n",
      "Did not buy at 20190716\n",
      "Did not buy at 20190717\n",
      "Did not buy at 20190718\n",
      "Did not buy at 20190719\n",
      "Did not buy at 20190722\n",
      "Did not buy at 20190723\n",
      "Did not buy at 20190724\n",
      "Did not buy at 20190725\n",
      "Did not buy at 20190726\n",
      "Did not buy at 20190729\n",
      "Did not buy at 20190730\n",
      "Did not buy at 20190731\n",
      "Did not buy at 20190801\n",
      "Did not buy at 20190802\n",
      "Sold at 20190805 with price = 48.512\n",
      "Did not buy at 20190806\n",
      "Did not buy at 20190807\n",
      "Did not buy at 20190808\n",
      "Did not buy at 20190809\n",
      "Did not buy at 20190812\n",
      "Did not buy at 20190813\n",
      "Bought at 20190814 with price = 49.968\n",
      "Did not buy at 20190815\n",
      "Did not buy at 20190816\n",
      "Did not buy at 20190819\n",
      "Did not buy at 20190820\n",
      "Did not buy at 20190821\n",
      "Did not buy at 20190822\n",
      "Did not buy at 20190823\n",
      "Did not buy at 20190826\n",
      "Did not buy at 20190827\n",
      "Sold at 20190828 with price = 50.203\n",
      "Did not buy at 20190829\n",
      "Did not buy at 20190830\n",
      "Did not buy at 20190903\n",
      "Did not buy at 20190904\n",
      "Did not buy at 20190905\n",
      "Bought at 20190906 with price = 52.649\n",
      "Did not buy at 20190909\n",
      "Did not buy at 20190910\n",
      "Did not buy at 20190911\n",
      "Did not buy at 20190912\n",
      "Did not buy at 20190913\n",
      "Did not buy at 20190916\n",
      "Did not buy at 20190917\n",
      "Did not buy at 20190918\n",
      "Did not buy at 20190919\n",
      "Did not buy at 20190920\n",
      "Did not buy at 20190923\n",
      "Did not buy at 20190924\n",
      "Did not buy at 20190925\n",
      "Did not buy at 20190926\n",
      "Did not buy at 20190927\n",
      "Sold at 20190930 with price = 54.337\n",
      "Bought at 20191001 with price = 55.362\n",
      "Did not buy at 20191002\n",
      "Did not buy at 20191003\n",
      "Did not buy at 20191004\n",
      "Did not buy at 20191007\n",
      "Did not buy at 20191008\n",
      "Did not buy at 20191009\n",
      "Did not buy at 20191010\n",
      "Did not buy at 20191011\n",
      "Did not buy at 20191014\n",
      "Did not buy at 20191015\n",
      "Did not buy at 20191016\n",
      "Did not buy at 20191017\n",
      "Did not buy at 20191018\n",
      "Did not buy at 20191021\n",
      "Did not buy at 20191022\n",
      "Did not buy at 20191023\n",
      "Did not buy at 20191024\n",
      "Did not buy at 20191025\n",
      "Did not buy at 20191028\n",
      "Did not buy at 20191029\n",
      "Did not buy at 20191030\n",
      "Did not buy at 20191031\n",
      "Did not buy at 20191101\n",
      "Did not buy at 20191104\n",
      "Did not buy at 20191105\n",
      "Did not buy at 20191106\n",
      "Did not buy at 20191107\n",
      "Did not buy at 20191108\n",
      "Did not buy at 20191111\n",
      "Did not buy at 20191112\n",
      "Did not buy at 20191113\n",
      "Did not buy at 20191114\n",
      "Did not buy at 20191115\n",
      "Did not buy at 20191118\n",
      "Did not buy at 20191119\n",
      "Did not buy at 20191120\n",
      "Did not buy at 20191121\n",
      "Did not buy at 20191122\n",
      "Sold at 20191125 with price = 64.812\n",
      "Did not buy at 20191126\n",
      "Did not buy at 20191127\n",
      "Bought at 20191129 with price = 65.768\n",
      "Did not buy at 20191202\n",
      "Did not buy at 20191203\n",
      "Sold at 20191204 with price = 64.405\n",
      "Did not buy at 20191205\n",
      "Did not buy at 20191206\n",
      "Bought at 20191209 with price = 66.61\n",
      "Did not buy at 20191210\n",
      "Did not buy at 20191211\n",
      "Did not buy at 20191212\n",
      "Did not buy at 20191213\n",
      "Did not buy at 20191216\n",
      "Did not buy at 20191217\n",
      "Did not buy at 20191218\n",
      "Did not buy at 20191219\n",
      "Did not buy at 20191220\n",
      "Did not buy at 20191223\n",
      "Did not buy at 20191224\n",
      "Did not buy at 20191226\n",
      "Did not buy at 20191227\n",
      "Did not buy at 20191230\n",
      "Did not buy at 20191231\n",
      "Did not buy at 20200102\n",
      "Did not buy at 20200103\n",
      "Did not buy at 20200106\n",
      "Did not buy at 20200107\n",
      "Did not buy at 20200108\n",
      "Did not buy at 20200109\n",
      "Did not buy at 20200110\n",
      "Did not buy at 20200113\n",
      "Did not buy at 20200114\n",
      "Did not buy at 20200115\n",
      "Did not buy at 20200116\n",
      "Did not buy at 20200117\n",
      "Did not buy at 20200121\n",
      "Did not buy at 20200122\n",
      "Did not buy at 20200123\n",
      "Did not buy at 20200124\n",
      "Did not buy at 20200127\n",
      "Sold at 20200128 with price = 77.118\n",
      "Did not buy at 20200129\n",
      "Bought at 20200130 with price = 79.081\n",
      "Did not buy at 20200131\n",
      "Did not buy at 20200203\n",
      "Did not buy at 20200204\n",
      "Sold at 20200205 with price = 79.811\n",
      "Bought at 20200206 with price = 79.579\n",
      "Did not buy at 20200207\n",
      "Did not buy at 20200210\n",
      "Did not buy at 20200211\n",
      "Did not buy at 20200212\n",
      "Did not buy at 20200213\n",
      "Did not buy at 20200214\n",
      "Did not buy at 20200218\n",
      "Did not buy at 20200219\n",
      "Sold at 20200220 with price = 79.781\n",
      "Did not buy at 20200221\n",
      "Did not buy at 20200224\n",
      "Did not buy at 20200225\n",
      "Did not buy at 20200226\n",
      "Did not buy at 20200227\n",
      "Did not buy at 20200228\n",
      "Did not buy at 20200302\n",
      "Did not buy at 20200303\n",
      "Bought at 20200304 with price = 73.306\n",
      "Did not buy at 20200305\n",
      "Did not buy at 20200306\n",
      "Did not buy at 20200309\n",
      "Sold at 20200310 with price = 68.53\n",
      "Did not buy at 20200311\n",
      "Did not buy at 20200312\n",
      "Did not buy at 20200313\n",
      "Did not buy at 20200316\n",
      "Did not buy at 20200317\n",
      "Did not buy at 20200318\n",
      "Did not buy at 20200319\n",
      "Did not buy at 20200320\n",
      "Did not buy at 20200323\n",
      "Did not buy at 20200324\n",
      "Did not buy at 20200325\n",
      "Bought at 20200326 with price = 60.96\n",
      "Did not buy at 20200327\n",
      "Did not buy at 20200330\n",
      "Did not buy at 20200331\n",
      "Did not buy at 20200401\n",
      "Did not buy at 20200402\n",
      "Did not buy at 20200403\n",
      "Did not buy at 20200406\n",
      "Did not buy at 20200407\n",
      "Did not buy at 20200408\n",
      "Did not buy at 20200409\n",
      "Did not buy at 20200413\n",
      "Did not buy at 20200414\n",
      "Did not buy at 20200415\n",
      "Did not buy at 20200416\n",
      "Did not buy at 20200417\n",
      "Did not buy at 20200420\n",
      "Did not buy at 20200421\n",
      "Did not buy at 20200422\n",
      "Did not buy at 20200423\n",
      "Did not buy at 20200424\n",
      "Did not buy at 20200427\n",
      "Did not buy at 20200428\n",
      "Did not buy at 20200429\n",
      "Did not buy at 20200430\n",
      "Did not buy at 20200501\n",
      "Did not buy at 20200504\n",
      "Did not buy at 20200505\n",
      "Did not buy at 20200506\n",
      "Did not buy at 20200507\n",
      "Did not buy at 20200508\n",
      "Did not buy at 20200511\n",
      "Did not buy at 20200512\n",
      "Did not buy at 20200513\n",
      "Did not buy at 20200514\n",
      "Did not buy at 20200515\n",
      "Did not buy at 20200518\n",
      "Did not buy at 20200519\n",
      "Did not buy at 20200520\n",
      "Did not buy at 20200521\n",
      "Did not buy at 20200522\n",
      "Did not buy at 20200526\n",
      "Did not buy at 20200527\n",
      "Did not buy at 20200528\n",
      "Did not buy at 20200529\n",
      "Did not buy at 20200601\n",
      "Did not buy at 20200602\n",
      "Did not buy at 20200603\n",
      "Did not buy at 20200604\n",
      "Did not buy at 20200605\n",
      "Did not buy at 20200608\n",
      "Did not buy at 20200609\n",
      "Did not buy at 20200610\n",
      "Did not buy at 20200611\n",
      "Did not buy at 20200612\n",
      "Did not buy at 20200615\n",
      "Did not buy at 20200616\n",
      "Did not buy at 20200617\n",
      "Did not buy at 20200618\n",
      "Did not buy at 20200619\n",
      "Did not buy at 20200622\n",
      "Did not buy at 20200623\n",
      "Did not buy at 20200624\n",
      "Did not buy at 20200625\n",
      "Did not buy at 20200626\n",
      "Did not buy at 20200629\n",
      "Did not buy at 20200630\n",
      "Did not buy at 20200701\n",
      "Did not buy at 20200702\n",
      "Did not buy at 20200706\n",
      "Did not buy at 20200707\n",
      "Did not buy at 20200708\n",
      "Did not buy at 20200709\n",
      "Did not buy at 20200710\n",
      "Did not buy at 20200713\n",
      "Did not buy at 20200714\n",
      "Did not buy at 20200715\n",
      "Did not buy at 20200716\n",
      "Did not buy at 20200717\n",
      "Did not buy at 20200720\n",
      "Did not buy at 20200721\n",
      "Did not buy at 20200722\n",
      "Did not buy at 20200723\n",
      "Sold at 20200724 with price = 90.242\n",
      "Did not buy at 20200727\n",
      "Did not buy at 20200728\n",
      "Did not buy at 20200729\n",
      "Did not buy at 20200730\n",
      "Did not buy at 20200731\n",
      "Bought at 20200803 with price = 107.31\n",
      "Did not buy at 20200804\n",
      "Did not buy at 20200805\n",
      "Did not buy at 20200806\n",
      "Did not buy at 20200807\n",
      "Did not buy at 20200810\n",
      "Did not buy at 20200811\n",
      "Did not buy at 20200812\n",
      "Did not buy at 20200813\n",
      "Did not buy at 20200814\n",
      "Did not buy at 20200817\n",
      "Did not buy at 20200818\n",
      "Did not buy at 20200819\n",
      "Did not buy at 20200820\n",
      "Did not buy at 20200821\n",
      "Did not buy at 20200824\n",
      "Did not buy at 20200825\n",
      "Did not buy at 20200826\n",
      "Did not buy at 20200827\n",
      "Did not buy at 20200828\n",
      "Did not buy at 20200831\n",
      "Did not buy at 20200901\n",
      "Did not buy at 20200902\n",
      "Did not buy at 20200903\n",
      "Did not buy at 20200904\n",
      "Sold at 20200908 with price = 113.22\n",
      "Did not buy at 20200909\n",
      "Did not buy at 20200910\n",
      "Did not buy at 20200911\n",
      "Did not buy at 20200914\n",
      "Did not buy at 20200915\n",
      "Did not buy at 20200916\n",
      "Did not buy at 20200917\n",
      "Did not buy at 20200918\n",
      "Did not buy at 20200921\n",
      "Did not buy at 20200922\n",
      "Did not buy at 20200923\n",
      "Did not buy at 20200924\n",
      "Did not buy at 20200925\n",
      "Did not buy at 20200928\n",
      "Did not buy at 20200929\n",
      "Bought at 20200930 with price = 113.06\n",
      "Did not buy at 20201001\n",
      "Did not buy at 20201002\n",
      "Did not buy at 20201005\n",
      "Did not buy at 20201006\n",
      "Did not buy at 20201007\n",
      "Did not buy at 20201008\n",
      "Did not buy at 20201009\n",
      "Did not buy at 20201012\n",
      "Did not buy at 20201013\n",
      "Did not buy at 20201014\n",
      "Did not buy at 20201015\n",
      "Did not buy at 20201016\n",
      "Did not buy at 20201019\n",
      "Did not buy at 20201020\n",
      "Did not buy at 20201021\n",
      "Did not buy at 20201022\n",
      "Sold at 20201023 with price = 115.65\n",
      "Did not buy at 20201026\n",
      "Did not buy at 20201027\n",
      "Did not buy at 20201028\n",
      "Did not buy at 20201029\n",
      "Did not buy at 20201030\n",
      "Did not buy at 20201102\n",
      "Did not buy at 20201103\n",
      "Did not buy at 20201104\n",
      "Did not buy at 20201105\n",
      "Bought at 20201106 with price = 117.76\n",
      "Did not buy at 20201109\n",
      "Did not buy at 20201110\n",
      "Did not buy at 20201111\n",
      "Did not buy at 20201112\n",
      "Did not buy at 20201113\n",
      "Did not buy at 20201116\n",
      "Did not buy at 20201117\n",
      "Did not buy at 20201118\n",
      "Did not buy at 20201119\n",
      "Did not buy at 20201120\n",
      "Did not buy at 20201123\n",
      "Sold at 20201124 with price = 113.38\n",
      "Did not buy at 20201125\n",
      "Did not buy at 20201127\n",
      "Did not buy at 20201130\n",
      "Bought at 20201201 with price = 120.45\n",
      "Did not buy at 20201202\n",
      "Did not buy at 20201203\n",
      "Did not buy at 20201204\n",
      "Did not buy at 20201207\n",
      "Did not buy at 20201208\n",
      "Did not buy at 20201209\n",
      "Did not buy at 20201210\n",
      "Did not buy at 20201211\n",
      "Did not buy at 20201214\n",
      "Sold at 20201215 with price = 123.75\n",
      "Bought at 20201216 with price = 126.81\n",
      "Did not buy at 20201217\n",
      "Did not buy at 20201218\n",
      "Did not buy at 20201221\n",
      "Did not buy at 20201222\n",
      "Did not buy at 20201223\n",
      "Did not buy at 20201224\n",
      "Did not buy at 20201228\n",
      "Did not buy at 20201229\n",
      "Did not buy at 20201230\n",
      "Did not buy at 20201231\n",
      "Did not buy at 20210104\n",
      "Did not buy at 20210105\n",
      "Did not buy at 20210106\n",
      "Sold at 20210107 with price = 127.76\n",
      "Did not buy at 20210108\n",
      "Bought at 20210111 with price = 128.59\n",
      "Sold at 20210112 with price = 127.9\n",
      "Did not buy at 20210113\n",
      "Did not buy at 20210114\n",
      "Did not buy at 20210115\n",
      "Did not buy at 20210119\n",
      "Did not buy at 20210120\n",
      "Bought at 20210121 with price = 133.17\n",
      "Did not buy at 20210122\n",
      "Did not buy at 20210125\n",
      "Did not buy at 20210126\n",
      "Did not buy at 20210127\n",
      "Did not buy at 20210128\n",
      "Did not buy at 20210129\n",
      "Did not buy at 20210201\n",
      "Sold at 20210202 with price = 135.1\n",
      "Did not buy at 20210203\n",
      "Did not buy at 20210204\n",
      "Did not buy at 20210205\n",
      "Did not buy at 20210208\n",
      "Bought at 20210209 with price = 136.19\n",
      "Did not buy at 20210210\n",
      "Did not buy at 20210211\n",
      "Did not buy at 20210212\n",
      "Did not buy at 20210216\n",
      "Sold at 20210217 with price = 130.83\n",
      "Did not buy at 20210218\n",
      "Did not buy at 20210219\n",
      "Did not buy at 20210222\n",
      "Did not buy at 20210223\n",
      "Did not buy at 20210224\n",
      "Did not buy at 20210225\n",
      "Did not buy at 20210226\n",
      "Did not buy at 20210301\n",
      "Did not buy at 20210302\n",
      "Did not buy at 20210303\n",
      "Did not buy at 20210304\n",
      "Did not buy at 20210305\n",
      "Did not buy at 20210308\n",
      "Did not buy at 20210309\n",
      "Did not buy at 20210310\n",
      "Did not buy at 20210311\n",
      "Did not buy at 20210312\n",
      "Did not buy at 20210315\n",
      "Did not buy at 20210316\n",
      "Bought at 20210317 with price = 123.65\n",
      "Did not buy at 20210318\n",
      "Did not buy at 20210319\n",
      "Did not buy at 20210322\n",
      "Did not buy at 20210323\n",
      "Did not buy at 20210324\n",
      "Did not buy at 20210325\n",
      "Did not buy at 20210326\n",
      "Sold at 20210329 with price = 121.26\n",
      "Did not buy at 20210330\n",
      "Did not buy at 20210331\n",
      "Did not buy at 20210401\n",
      "Bought at 20210405 with price = 123.48\n",
      "Did not buy at 20210406\n",
      "Did not buy at 20210407\n",
      "Did not buy at 20210408\n",
      "Did not buy at 20210409\n",
      "Did not buy at 20210412\n",
      "Did not buy at 20210413\n",
      "Did not buy at 20210414\n",
      "Did not buy at 20210415\n",
      "Did not buy at 20210416\n",
      "Did not buy at 20210419\n",
      "Did not buy at 20210420\n",
      "Did not buy at 20210421\n",
      "Did not buy at 20210422\n",
      "Did not buy at 20210423\n",
      "Did not buy at 20210426\n",
      "Did not buy at 20210427\n",
      "Did not buy at 20210428\n",
      "Did not buy at 20210429\n",
      "Did not buy at 20210430\n",
      "Sold at 20210503 with price = 131.62\n",
      "Did not buy at 20210504\n",
      "Did not buy at 20210505\n",
      "Did not buy at 20210506\n",
      "Did not buy at 20210507\n",
      "Did not buy at 20210510\n",
      "Did not buy at 20210511\n",
      "Did not buy at 20210512\n",
      "Did not buy at 20210513\n",
      "Did not buy at 20210514\n",
      "Did not buy at 20210517\n",
      "Did not buy at 20210518\n",
      "Did not buy at 20210519\n",
      "Did not buy at 20210520\n",
      "Did not buy at 20210521\n",
      "Bought at 20210524 with price = 125.82\n",
      "Did not buy at 20210525\n",
      "Did not buy at 20210526\n",
      "Did not buy at 20210527\n",
      "Did not buy at 20210528\n",
      "Did not buy at 20210601\n",
      "Sold at 20210602 with price = 124.1\n",
      "Did not buy at 20210603\n",
      "Did not buy at 20210604\n",
      "Did not buy at 20210607\n",
      "Bought at 20210608 with price = 126.41\n",
      "Did not buy at 20210609\n",
      "Did not buy at 20210610\n",
      "Did not buy at 20210611\n",
      "Did not buy at 20210614\n",
      "Did not buy at 20210615\n",
      "Did not buy at 20210616\n",
      "Did not buy at 20210617\n",
      "Did not buy at 20210618\n",
      "Did not buy at 20210621\n",
      "Did not buy at 20210622\n",
      "Did not buy at 20210623\n",
      "Did not buy at 20210624\n",
      "Did not buy at 20210625\n",
      "Did not buy at 20210628\n",
      "Did not buy at 20210629\n",
      "Did not buy at 20210630\n",
      "Did not buy at 20210701\n",
      "Did not buy at 20210702\n",
      "Did not buy at 20210706\n",
      "Did not buy at 20210707\n",
      "Did not buy at 20210708\n",
      "Did not buy at 20210709\n",
      "Did not buy at 20210712\n",
      "Did not buy at 20210713\n",
      "Did not buy at 20210714\n",
      "Did not buy at 20210715\n",
      "Did not buy at 20210716\n",
      "Did not buy at 20210719\n",
      "Did not buy at 20210720\n",
      "Did not buy at 20210721\n",
      "Did not buy at 20210722\n",
      "Did not buy at 20210723\n",
      "Did not buy at 20210726\n",
      "Did not buy at 20210727\n",
      "Did not buy at 20210728\n",
      "Did not buy at 20210729\n",
      "Sold at 20210730 with price = 144.16\n",
      "Did not buy at 20210802\n",
      "Did not buy at 20210803\n",
      "Did not buy at 20210804\n",
      "Did not buy at 20210805\n",
      "Did not buy at 20210806\n",
      "Bought at 20210809 with price = 146.2\n",
      "Did not buy at 20210810\n",
      "Did not buy at 20210811\n",
      "Did not buy at 20210812\n",
      "Did not buy at 20210813\n",
      "Did not buy at 20210816\n",
      "Did not buy at 20210817\n",
      "Did not buy at 20210818\n",
      "Did not buy at 20210819\n",
      "Did not buy at 20210820\n",
      "Did not buy at 20210823\n",
      "Did not buy at 20210824\n",
      "Did not buy at 20210825\n",
      "Did not buy at 20210826\n",
      "Did not buy at 20210827\n",
      "Did not buy at 20210830\n",
      "Did not buy at 20210831\n",
      "Did not buy at 20210901\n",
      "Did not buy at 20210902\n",
      "Did not buy at 20210903\n",
      "Did not buy at 20210907\n",
      "Did not buy at 20210908\n",
      "Did not buy at 20210909\n",
      "Did not buy at 20210910\n",
      "Did not buy at 20210913\n",
      "Sold at 20210914 with price = 150.35\n",
      "Did not buy at 20210915\n",
      "Did not buy at 20210916\n",
      "Did not buy at 20210917\n",
      "Did not buy at 20210920\n",
      "Did not buy at 20210921\n",
      "Did not buy at 20210922\n",
      "Did not buy at 20210923\n",
      "Did not buy at 20210924\n",
      "Did not buy at 20210927\n",
      "Did not buy at 20210928\n",
      "Did not buy at 20210929\n",
      "Did not buy at 20210930\n",
      "         Action   Price Position          Cash Pos_Bal   Cash_Bal Cum_Profit\n",
      "20180117    Buy  42.181      236  -9974.625432     236     25.375    -19.909\n",
      "20180125   Sell  41.783     -236   9841.066424       0   9866.441   -133.559\n",
      "20180214    Buy  39.197      251  -9858.123894     251      8.317   -153.236\n",
      "20180321   Sell  42.087     -251  10542.709326       0  10551.026    551.026\n",
      "20180406    Buy  41.105      256  -10543.92576     256      7.101    529.981\n",
      "...         ...     ...      ...           ...     ...        ...        ...\n",
      "20210602   Sell   124.1     -208    25761.1744       0  25884.635  15884.635\n",
      "20210608    Buy  126.41      204  -25839.21528     204      45.42   15833.06\n",
      "20210730   Sell  144.16     -204   29349.82272       0  29395.242  19395.242\n",
      "20210809    Buy   146.2      200     -29298.48     200     96.762  19336.762\n",
      "20210914   Sell  150.35     -200      30009.86       0  30106.622  20106.622\n",
      "\n",
      "[75 rows x 7 columns]\n",
      "==========================\n",
      "Current Profit: 20106.622416000013\n"
     ]
    }
   ],
   "source": [
    "abc_test = custom_split(data,start = 20190101,end = 20210930)\n",
    "tf_abc_test= transform_X_data_to_tensor(abc_test,num_day_to_predict)\n",
    "predictions = loaded_model.predict(tf_abc_test)\n",
    "backtest_data,pred = convert_decision(abc_test,predictions)\n",
    "capital = 10000\n",
    "handling_fee = 0.002 \n",
    "backtest_abc = backtest(backtest_data,pred,capital,handling_fee)\n",
    "backtest_abc.start_test()\n",
    "backtest_abc.print_trade_record()\n",
    "print('==========================')\n",
    "backtest_abc.export_trade_record(\"AAPL (abc)\")\n",
    "backtest_abc.print_profit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ed63b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If buy at 20190116 with price = 37.209\n",
      "Current Profit: 28508.923976\n"
     ]
    }
   ],
   "source": [
    "do_nothing(backtest_data,capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81668d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7b545f4",
   "metadata": {},
   "source": [
    "# Support Vectore Rgression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21802f8",
   "metadata": {},
   "source": [
    "### Split the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f3ccc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = custom_split(data,start = 20130101,end = 20171031)\n",
    "valid_X = custom_split(data,start = 20171101,end = 20181231)\n",
    "test_X = custom_split(data,start = 20190101,end = 20201231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da48d1",
   "metadata": {},
   "source": [
    "### Label the target result (opening price on 11th day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ffecbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we use 10 days price data to predict closing price of the 11th day\n",
    "num_day_to_predict = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6072236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_result_target_price(X,num_day = 10,result_col_name = \"result_price\"):\n",
    "    y = pd.DataFrame(np.nan, index=X.index, columns=[result_col_name])\n",
    "    for i in range(10,len(X)):\n",
    "        y.loc[X.index[i-1],result_col_name] = X.loc[X.index[i],\"open\"]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fb0d2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = produce_result_target_price(train_X,num_day_to_predict)\n",
    "valid_y = produce_result_target_price(valid_X,num_day_to_predict)\n",
    "test_y = produce_result_target_price(test_X,num_day_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4a234817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20190102</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190103</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190104</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190107</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190108</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190109</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190110</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190111</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190114</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190115</th>\n",
       "      <td>37.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190116</th>\n",
       "      <td>37.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190117</th>\n",
       "      <td>38.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190118</th>\n",
       "      <td>38.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190122</th>\n",
       "      <td>37.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190123</th>\n",
       "      <td>37.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190124</th>\n",
       "      <td>37.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190125</th>\n",
       "      <td>37.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190128</th>\n",
       "      <td>37.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190129</th>\n",
       "      <td>39.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190130</th>\n",
       "      <td>40.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          result_price\n",
       "DATE                  \n",
       "20190102           NaN\n",
       "20190103           NaN\n",
       "20190104           NaN\n",
       "20190107           NaN\n",
       "20190108           NaN\n",
       "20190109           NaN\n",
       "20190110           NaN\n",
       "20190111           NaN\n",
       "20190114           NaN\n",
       "20190115        37.209\n",
       "20190116        37.477\n",
       "20190117        38.280\n",
       "20190118        38.013\n",
       "20190122        37.464\n",
       "20190123        37.454\n",
       "20190124        37.794\n",
       "20190125        37.866\n",
       "20190128        37.975\n",
       "20190129        39.680\n",
       "20190130        40.375"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0fc2b",
   "metadata": {},
   "source": [
    "### Transform the 10-day data into one vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "649e01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_x_data_to_one_vector(X,num_day = 10):\n",
    "    col_name = []\n",
    "    for i in range(num_day):\n",
    "        for j in X.columns:\n",
    "            col_name.append(j+\"-\"+str(i))\n",
    "    new_X = pd.DataFrame(np.nan, index=X.index, columns=col_name)\n",
    "    \n",
    "    for i in range(len(X)-9):\n",
    "        for col in col_name:\n",
    "            split_list = col.split(\"-\")\n",
    "            new_X.loc[X.index[i],col] = X.loc[X.index[i+int(split_list[1])],split_list[0]]\n",
    "    \n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1d04662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_10 = transform_x_data_to_one_vector(train_X,num_day_to_predict)\n",
    "valid_X_10 = transform_x_data_to_one_vector(valid_X,num_day_to_predict)\n",
    "test_X_10 = transform_x_data_to_one_vector(test_X,num_day_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "68228e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open-0</th>\n",
       "      <th>high-0</th>\n",
       "      <th>low-0</th>\n",
       "      <th>close-0</th>\n",
       "      <th>volume-0</th>\n",
       "      <th>boll-0</th>\n",
       "      <th>boll_ub-0</th>\n",
       "      <th>boll_lb-0</th>\n",
       "      <th>macd-0</th>\n",
       "      <th>macdh-0</th>\n",
       "      <th>...</th>\n",
       "      <th>volume-9</th>\n",
       "      <th>boll-9</th>\n",
       "      <th>boll_ub-9</th>\n",
       "      <th>boll_lb-9</th>\n",
       "      <th>macd-9</th>\n",
       "      <th>macdh-9</th>\n",
       "      <th>macds-9</th>\n",
       "      <th>rsi_11-9</th>\n",
       "      <th>rsi_14-9</th>\n",
       "      <th>rsi_21-9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20201203</th>\n",
       "      <td>122.95</td>\n",
       "      <td>123.21</td>\n",
       "      <td>121.64</td>\n",
       "      <td>122.37</td>\n",
       "      <td>79338657.0</td>\n",
       "      <td>117.9900</td>\n",
       "      <td>122.997838</td>\n",
       "      <td>112.982162</td>\n",
       "      <td>1.401129</td>\n",
       "      <td>0.596780</td>\n",
       "      <td>...</td>\n",
       "      <td>98670020.0</td>\n",
       "      <td>120.3710</td>\n",
       "      <td>128.205931</td>\n",
       "      <td>112.536069</td>\n",
       "      <td>2.307306</td>\n",
       "      <td>0.516992</td>\n",
       "      <td>1.790315</td>\n",
       "      <td>68.375418</td>\n",
       "      <td>65.754417</td>\n",
       "      <td>61.740894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201204</th>\n",
       "      <td>122.03</td>\n",
       "      <td>122.29</td>\n",
       "      <td>120.95</td>\n",
       "      <td>121.68</td>\n",
       "      <td>78628125.0</td>\n",
       "      <td>118.1610</td>\n",
       "      <td>123.434189</td>\n",
       "      <td>112.887811</td>\n",
       "      <td>1.524915</td>\n",
       "      <td>0.576453</td>\n",
       "      <td>...</td>\n",
       "      <td>94803157.0</td>\n",
       "      <td>120.9025</td>\n",
       "      <td>129.328724</td>\n",
       "      <td>112.476276</td>\n",
       "      <td>2.557105</td>\n",
       "      <td>0.613432</td>\n",
       "      <td>1.943673</td>\n",
       "      <td>70.004702</td>\n",
       "      <td>67.108619</td>\n",
       "      <td>62.687681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201207</th>\n",
       "      <td>121.74</td>\n",
       "      <td>123.98</td>\n",
       "      <td>121.68</td>\n",
       "      <td>123.18</td>\n",
       "      <td>87025034.0</td>\n",
       "      <td>118.4135</td>\n",
       "      <td>124.144216</td>\n",
       "      <td>112.682784</td>\n",
       "      <td>1.724178</td>\n",
       "      <td>0.620572</td>\n",
       "      <td>...</td>\n",
       "      <td>193446146.0</td>\n",
       "      <td>121.3020</td>\n",
       "      <td>129.920225</td>\n",
       "      <td>112.683775</td>\n",
       "      <td>2.561737</td>\n",
       "      <td>0.494452</td>\n",
       "      <td>2.067286</td>\n",
       "      <td>61.991552</td>\n",
       "      <td>61.167224</td>\n",
       "      <td>59.180247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201208</th>\n",
       "      <td>123.78</td>\n",
       "      <td>124.39</td>\n",
       "      <td>122.52</td>\n",
       "      <td>123.79</td>\n",
       "      <td>82394044.0</td>\n",
       "      <td>118.8140</td>\n",
       "      <td>124.879583</td>\n",
       "      <td>112.748417</td>\n",
       "      <td>1.909308</td>\n",
       "      <td>0.644562</td>\n",
       "      <td>...</td>\n",
       "      <td>121821249.0</td>\n",
       "      <td>121.8440</td>\n",
       "      <td>130.629283</td>\n",
       "      <td>113.058717</td>\n",
       "      <td>2.660618</td>\n",
       "      <td>0.474666</td>\n",
       "      <td>2.185952</td>\n",
       "      <td>65.344800</td>\n",
       "      <td>63.818244</td>\n",
       "      <td>60.943515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201209</th>\n",
       "      <td>123.94</td>\n",
       "      <td>125.36</td>\n",
       "      <td>120.44</td>\n",
       "      <td>121.21</td>\n",
       "      <td>115416170.0</td>\n",
       "      <td>119.1030</td>\n",
       "      <td>125.039109</td>\n",
       "      <td>113.166891</td>\n",
       "      <td>1.826782</td>\n",
       "      <td>0.449629</td>\n",
       "      <td>...</td>\n",
       "      <td>170147519.0</td>\n",
       "      <td>122.7410</td>\n",
       "      <td>131.525208</td>\n",
       "      <td>113.956792</td>\n",
       "      <td>2.997341</td>\n",
       "      <td>0.649111</td>\n",
       "      <td>2.348230</td>\n",
       "      <td>71.728940</td>\n",
       "      <td>69.103755</td>\n",
       "      <td>64.672046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201210</th>\n",
       "      <td>119.94</td>\n",
       "      <td>123.30</td>\n",
       "      <td>119.59</td>\n",
       "      <td>122.67</td>\n",
       "      <td>81693881.0</td>\n",
       "      <td>119.2900</td>\n",
       "      <td>125.435119</td>\n",
       "      <td>113.144881</td>\n",
       "      <td>1.857774</td>\n",
       "      <td>0.384497</td>\n",
       "      <td>...</td>\n",
       "      <td>88638209.0</td>\n",
       "      <td>123.5265</td>\n",
       "      <td>132.062951</td>\n",
       "      <td>114.990049</td>\n",
       "      <td>3.153607</td>\n",
       "      <td>0.644302</td>\n",
       "      <td>2.509305</td>\n",
       "      <td>68.225040</td>\n",
       "      <td>66.454125</td>\n",
       "      <td>63.069773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201211</th>\n",
       "      <td>121.86</td>\n",
       "      <td>122.19</td>\n",
       "      <td>119.99</td>\n",
       "      <td>121.84</td>\n",
       "      <td>87348270.0</td>\n",
       "      <td>119.4495</td>\n",
       "      <td>125.689539</td>\n",
       "      <td>113.209461</td>\n",
       "      <td>1.794674</td>\n",
       "      <td>0.257117</td>\n",
       "      <td>...</td>\n",
       "      <td>55188152.0</td>\n",
       "      <td>124.3195</td>\n",
       "      <td>132.656887</td>\n",
       "      <td>115.982113</td>\n",
       "      <td>3.320669</td>\n",
       "      <td>0.649091</td>\n",
       "      <td>2.671578</td>\n",
       "      <td>69.995053</td>\n",
       "      <td>67.908870</td>\n",
       "      <td>64.095178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201214</th>\n",
       "      <td>122.03</td>\n",
       "      <td>122.78</td>\n",
       "      <td>120.97</td>\n",
       "      <td>121.21</td>\n",
       "      <td>79447525.0</td>\n",
       "      <td>119.5750</td>\n",
       "      <td>125.852420</td>\n",
       "      <td>113.297580</td>\n",
       "      <td>1.674528</td>\n",
       "      <td>0.109577</td>\n",
       "      <td>...</td>\n",
       "      <td>125071132.0</td>\n",
       "      <td>125.3200</td>\n",
       "      <td>134.259761</td>\n",
       "      <td>116.380239</td>\n",
       "      <td>3.789442</td>\n",
       "      <td>0.894291</td>\n",
       "      <td>2.895151</td>\n",
       "      <td>76.663435</td>\n",
       "      <td>73.647994</td>\n",
       "      <td>68.392452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201215</th>\n",
       "      <td>123.75</td>\n",
       "      <td>127.30</td>\n",
       "      <td>123.54</td>\n",
       "      <td>127.28</td>\n",
       "      <td>158312610.0</td>\n",
       "      <td>119.9520</td>\n",
       "      <td>127.114412</td>\n",
       "      <td>112.789588</td>\n",
       "      <td>2.045530</td>\n",
       "      <td>0.384463</td>\n",
       "      <td>...</td>\n",
       "      <td>121616061.0</td>\n",
       "      <td>126.1075</td>\n",
       "      <td>135.285634</td>\n",
       "      <td>116.929366</td>\n",
       "      <td>3.968345</td>\n",
       "      <td>0.858555</td>\n",
       "      <td>3.109790</td>\n",
       "      <td>70.046495</td>\n",
       "      <td>68.546679</td>\n",
       "      <td>65.225111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201216</th>\n",
       "      <td>126.81</td>\n",
       "      <td>127.77</td>\n",
       "      <td>125.97</td>\n",
       "      <td>127.21</td>\n",
       "      <td>98670020.0</td>\n",
       "      <td>120.3710</td>\n",
       "      <td>128.205931</td>\n",
       "      <td>112.536069</td>\n",
       "      <td>2.307306</td>\n",
       "      <td>0.516992</td>\n",
       "      <td>...</td>\n",
       "      <td>96905301.0</td>\n",
       "      <td>126.6545</td>\n",
       "      <td>136.138453</td>\n",
       "      <td>117.170547</td>\n",
       "      <td>3.971549</td>\n",
       "      <td>0.689408</td>\n",
       "      <td>3.282142</td>\n",
       "      <td>66.082143</td>\n",
       "      <td>65.461242</td>\n",
       "      <td>63.280762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201217</th>\n",
       "      <td>128.30</td>\n",
       "      <td>128.98</td>\n",
       "      <td>127.44</td>\n",
       "      <td>128.10</td>\n",
       "      <td>94803157.0</td>\n",
       "      <td>120.9025</td>\n",
       "      <td>129.328724</td>\n",
       "      <td>112.476276</td>\n",
       "      <td>2.557105</td>\n",
       "      <td>0.613432</td>\n",
       "      <td>...</td>\n",
       "      <td>99582282.0</td>\n",
       "      <td>127.1325</td>\n",
       "      <td>136.700226</td>\n",
       "      <td>117.564774</td>\n",
       "      <td>3.847433</td>\n",
       "      <td>0.452233</td>\n",
       "      <td>3.395200</td>\n",
       "      <td>62.624162</td>\n",
       "      <td>62.762762</td>\n",
       "      <td>61.571416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201218</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201221</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201222</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201223</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201224</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201228</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201229</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201230</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201231</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          open-0  high-0   low-0  close-0     volume-0    boll-0   boll_ub-0  \\\n",
       "DATE                                                                           \n",
       "20201203  122.95  123.21  121.64   122.37   79338657.0  117.9900  122.997838   \n",
       "20201204  122.03  122.29  120.95   121.68   78628125.0  118.1610  123.434189   \n",
       "20201207  121.74  123.98  121.68   123.18   87025034.0  118.4135  124.144216   \n",
       "20201208  123.78  124.39  122.52   123.79   82394044.0  118.8140  124.879583   \n",
       "20201209  123.94  125.36  120.44   121.21  115416170.0  119.1030  125.039109   \n",
       "20201210  119.94  123.30  119.59   122.67   81693881.0  119.2900  125.435119   \n",
       "20201211  121.86  122.19  119.99   121.84   87348270.0  119.4495  125.689539   \n",
       "20201214  122.03  122.78  120.97   121.21   79447525.0  119.5750  125.852420   \n",
       "20201215  123.75  127.30  123.54   127.28  158312610.0  119.9520  127.114412   \n",
       "20201216  126.81  127.77  125.97   127.21   98670020.0  120.3710  128.205931   \n",
       "20201217  128.30  128.98  127.44   128.10   94803157.0  120.9025  129.328724   \n",
       "20201218     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201221     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201222     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201223     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201224     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201228     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201229     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201230     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "20201231     NaN     NaN     NaN      NaN          NaN       NaN         NaN   \n",
       "\n",
       "           boll_lb-0    macd-0   macdh-0  ...     volume-9    boll-9  \\\n",
       "DATE                                      ...                          \n",
       "20201203  112.982162  1.401129  0.596780  ...   98670020.0  120.3710   \n",
       "20201204  112.887811  1.524915  0.576453  ...   94803157.0  120.9025   \n",
       "20201207  112.682784  1.724178  0.620572  ...  193446146.0  121.3020   \n",
       "20201208  112.748417  1.909308  0.644562  ...  121821249.0  121.8440   \n",
       "20201209  113.166891  1.826782  0.449629  ...  170147519.0  122.7410   \n",
       "20201210  113.144881  1.857774  0.384497  ...   88638209.0  123.5265   \n",
       "20201211  113.209461  1.794674  0.257117  ...   55188152.0  124.3195   \n",
       "20201214  113.297580  1.674528  0.109577  ...  125071132.0  125.3200   \n",
       "20201215  112.789588  2.045530  0.384463  ...  121616061.0  126.1075   \n",
       "20201216  112.536069  2.307306  0.516992  ...   96905301.0  126.6545   \n",
       "20201217  112.476276  2.557105  0.613432  ...   99582282.0  127.1325   \n",
       "20201218         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201221         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201222         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201223         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201224         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201228         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201229         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201230         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "20201231         NaN       NaN       NaN  ...          NaN       NaN   \n",
       "\n",
       "           boll_ub-9   boll_lb-9    macd-9   macdh-9   macds-9   rsi_11-9  \\\n",
       "DATE                                                                        \n",
       "20201203  128.205931  112.536069  2.307306  0.516992  1.790315  68.375418   \n",
       "20201204  129.328724  112.476276  2.557105  0.613432  1.943673  70.004702   \n",
       "20201207  129.920225  112.683775  2.561737  0.494452  2.067286  61.991552   \n",
       "20201208  130.629283  113.058717  2.660618  0.474666  2.185952  65.344800   \n",
       "20201209  131.525208  113.956792  2.997341  0.649111  2.348230  71.728940   \n",
       "20201210  132.062951  114.990049  3.153607  0.644302  2.509305  68.225040   \n",
       "20201211  132.656887  115.982113  3.320669  0.649091  2.671578  69.995053   \n",
       "20201214  134.259761  116.380239  3.789442  0.894291  2.895151  76.663435   \n",
       "20201215  135.285634  116.929366  3.968345  0.858555  3.109790  70.046495   \n",
       "20201216  136.138453  117.170547  3.971549  0.689408  3.282142  66.082143   \n",
       "20201217  136.700226  117.564774  3.847433  0.452233  3.395200  62.624162   \n",
       "20201218         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201221         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201222         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201223         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201224         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201228         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201229         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201230         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "20201231         NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "\n",
       "           rsi_14-9   rsi_21-9  \n",
       "DATE                            \n",
       "20201203  65.754417  61.740894  \n",
       "20201204  67.108619  62.687681  \n",
       "20201207  61.167224  59.180247  \n",
       "20201208  63.818244  60.943515  \n",
       "20201209  69.103755  64.672046  \n",
       "20201210  66.454125  63.069773  \n",
       "20201211  67.908870  64.095178  \n",
       "20201214  73.647994  68.392452  \n",
       "20201215  68.546679  65.225111  \n",
       "20201216  65.461242  63.280762  \n",
       "20201217  62.762762  61.571416  \n",
       "20201218        NaN        NaN  \n",
       "20201221        NaN        NaN  \n",
       "20201222        NaN        NaN  \n",
       "20201223        NaN        NaN  \n",
       "20201224        NaN        NaN  \n",
       "20201228        NaN        NaN  \n",
       "20201229        NaN        NaN  \n",
       "20201230        NaN        NaN  \n",
       "20201231        NaN        NaN  \n",
       "\n",
       "[20 rows x 140 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_10.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7794f",
   "metadata": {},
   "source": [
    "### Drop out rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a6098786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_row_y(y,num_day = 10):\n",
    "    drop_list = [y.index[i] for i in range(num_day-1)]\n",
    "    drop_list.append(y.index[-1])\n",
    "    return y.drop(drop_list)\n",
    "\n",
    "def drop_nan_row_X_10(X,num_day = 10):\n",
    "    drop_list = [X.index[-i] for i in range(1,num_day+1)]\n",
    "    return X.drop(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "45b25d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_X_10 = drop_nan_row_X_10(train_X_10,num_day_to_predict)\n",
    "new_train_y = drop_nan_row_y(train_y,num_day_to_predict)\n",
    "\n",
    "new_valid_X_10  = drop_nan_row_X_10(valid_X_10,num_day_to_predict)\n",
    "new_valid_y = drop_nan_row_y(valid_y,num_day_to_predict)\n",
    "\n",
    "new_test_X_10  = drop_nan_row_X_10(test_X_10,num_day_to_predict)\n",
    "new_test_y = drop_nan_row_y(test_y,num_day_to_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "58c9ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy for reverting prediction result\n",
    "old_train_X_10 = new_train_X_10\n",
    "old_train_y = new_train_y\n",
    "old_valid_X_10 = new_valid_X_10\n",
    "old_valid_y = new_valid_y\n",
    "old_test_X_10 = new_test_X_10\n",
    "old_test_y = new_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9aac47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize data row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e9eb67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_by_row(X,y):\n",
    "    norm_X = X.sub(X.mean(axis=0), axis=1).div(X.std(axis=0), axis=1)\n",
    "    \n",
    "    col_list =[] \n",
    "    for col in X.columns:\n",
    "        if \"open\" in col:\n",
    "            col_list.append(col)\n",
    "    \n",
    "    temp_y = y.loc[:,\"result_price\"]\n",
    "    mean_10_day = np.mean(X.loc[:,col_list],axis=1)\n",
    "    mean_10_day.index = temp_y.index\n",
    "    std_10_day = np.std(X.loc[:,col_list],axis=1)\n",
    "    std_10_day.index = temp_y.index\n",
    "\n",
    "    norm_y = pd.DataFrame((temp_y-mean_10_day)/std_10_day,\n",
    "                          columns=[\"result_price\"])\n",
    "                         \n",
    "    return (norm_X,norm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "61b352f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_X_10,new_train_y = normalize_data_by_row(new_train_X_10,new_train_y)\n",
    "new_valid_X_10,new_valid_y = normalize_data_by_row(new_valid_X_10,new_valid_y)\n",
    "new_test_X_10,new_test_y = normalize_data_by_row(new_test_X_10,new_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5f349",
   "metadata": {},
   "source": [
    "### Model Training and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "af245af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1000 ): 0.7235581352332495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1100 ): 0.7239241818834927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1200 ): 0.7245001509788604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1300 ): 0.7250177684605499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1400 ): 0.7247148893057564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1500 ): 0.7246616695515463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1600 ): 0.724784026887165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1700 ): 0.7247970544595882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1800 ): 0.7251653972992237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 1900 ): 0.7253681949889405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2000 ): 0.7254456268581024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2100 ): 0.7251926827640618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2200 ): 0.7248080504978928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2300 ): 0.7245806460530338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2400 ): 0.7246226561483711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2500 ): 0.7246982243696107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2600 ): 0.7244217374336381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2700 ): 0.7242689573866239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2800 ): 0.7241051975093571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 2900 ): 0.7244479107761488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (c = 3000 ): 0.7242927405851719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa8b0549590>]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Support Vector Regressor to fit the data\n",
    "\n",
    "# C : Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "c_list = [i*100 for i in range(10,31)]\n",
    "svr_conf_list = []\n",
    "for c in c_list:\n",
    "    # gamma : Kernel coefficient\n",
    "    gamma = 0.00001\n",
    "\n",
    "    # radial basis function kernel\n",
    "    kernel=\"rbf\"\n",
    "\n",
    "    svr = SVR(kernel=kernel, C=c, gamma=gamma)\n",
    "    svr.fit(new_train_X_10, new_train_y)\n",
    "\n",
    "    svr_prediction = svr.predict(new_valid_X_10)\n",
    "    svr_confidence = svr.score(new_valid_X_10, new_valid_y)\n",
    "    print(\"svr confidence (c =\", c,\"):\", svr_confidence)\n",
    "    svr_conf_list.append(svr_confidence)\n",
    "    \n",
    "plt.plot(c_list, svr_conf_list, label = \"C to SVR Confid\", color = 'Black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0fe71653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose C = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cfe105d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 1e-07 ): 0.47287864805978785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 5e-07 ): 0.6475434324100067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 1e-06 ): 0.683127776588939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 5e-06 ): 0.7227733340863576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 1e-05 ): 0.7254456268581024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 5e-05 ): 0.694233234018548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 0.0001 ): 0.6589458003049283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence (gamma = 0.0005 ): 0.3569402866376521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa8ab7c3a90>]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Support Vector Regressor to fit the data\n",
    "\n",
    "# gamma : Kernel coefficient\n",
    "\n",
    "gamma_list = [0.0000001,0.0000005,0.000001,0.000005,0.00001,0.00005,0.0001,0.0005]\n",
    "svr_conf_list = []\n",
    "for gamma in gamma_list:\n",
    "    # C : Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "    c = 2000\n",
    "\n",
    "    # radial basis function kernel\n",
    "    kernel=\"rbf\"\n",
    "\n",
    "    svr = SVR(kernel=kernel, C=c, gamma=gamma)\n",
    "    svr.fit(new_train_X_10, new_train_y)\n",
    "\n",
    "    svr_prediction = svr.predict(new_valid_X_10)\n",
    "    svr_confidence = svr.score(new_valid_X_10, new_valid_y)\n",
    "    print(\"svr confidence (gamma =\", gamma,\"):\", svr_confidence)\n",
    "    svr_conf_list.append(svr_confidence)\n",
    "    \n",
    "plt.plot(gamma_list, svr_conf_list, label = \"gamma to SVR Confid\", color = 'Black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a428d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose gamma = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "93daf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20190115</th>\n",
       "      <td>0.952664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190116</th>\n",
       "      <td>1.444389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190117</th>\n",
       "      <td>2.540515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190118</th>\n",
       "      <td>1.778729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190122</th>\n",
       "      <td>0.525108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201223</th>\n",
       "      <td>1.291615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201224</th>\n",
       "      <td>1.856580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201228</th>\n",
       "      <td>2.645082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201229</th>\n",
       "      <td>1.363024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201230</th>\n",
       "      <td>0.750691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          result_price\n",
       "DATE                  \n",
       "20190115      0.952664\n",
       "20190116      1.444389\n",
       "20190117      2.540515\n",
       "20190118      1.778729\n",
       "20190122      0.525108\n",
       "...                ...\n",
       "20201223      1.291615\n",
       "20201224      1.856580\n",
       "20201228      2.645082\n",
       "20201229      1.363024\n",
       "20201230      0.750691\n",
       "\n",
       "[495 rows x 1 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c216623",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ee51dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonykwok/opt/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr confidence: 0.5085239228056166\n"
     ]
    }
   ],
   "source": [
    "# C : Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "c = 2000\n",
    "\n",
    "# gamma : Kernel coefficient\n",
    "gamma = 0.00001\n",
    "\n",
    "# radial basis function kernel\n",
    "kernel=\"rbf\"\n",
    "\n",
    "svr = SVR(kernel=kernel, C=c, gamma=gamma)\n",
    "svr.fit(new_train_X_10, new_train_y)\n",
    "\n",
    "svr_prediction = svr.predict(new_test_X_10)\n",
    "svr_confidence = svr.score(new_test_X_10, new_test_y)\n",
    "print(\"svr confidence:\", svr_confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ed79dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prediction(X,y,pred):\n",
    "    col_list =[] \n",
    "    for col in X.columns:\n",
    "        if \"open\" in col:\n",
    "            col_list.append(col)\n",
    "        \n",
    "    mean_10_day = np.mean(X.loc[:,col_list],axis=1)\n",
    "    mean_10_day.index = y.index\n",
    "    std_10_day = np.std(X.loc[:,col_list],axis=1)\n",
    "    std_10_day.index = y.index\n",
    "    a = pd.DataFrame(index = y.index,columns=['mean','std','pred'])\n",
    "    a.loc[:,\"mean\"] = mean_10_day\n",
    "    a.loc[:,\"std\"] = std_10_day\n",
    "    a.loc[:,\"pred\"] = pred\n",
    "    a.loc[:,\"converted\"] = pred*std_10_day+mean_10_day\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3dad5385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "20190115     36.188778\n",
       "20190116     36.437432\n",
       "20190117     36.774204\n",
       "20190118     37.175170\n",
       "20190122     36.812467\n",
       "               ...    \n",
       "20201223    130.354244\n",
       "20201224    131.408131\n",
       "20201228    136.413308\n",
       "20201229    133.559331\n",
       "20201230    132.070397\n",
       "Name: converted, Length: 495, dtype: float64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = convert_prediction(old_test_X_10,old_test_y,svr_prediction)\n",
    "converted_pred = pred_df.loc[:,\"converted\"]\n",
    "converted_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a666d07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20190115</th>\n",
       "      <td>37.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190116</th>\n",
       "      <td>37.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190117</th>\n",
       "      <td>38.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190118</th>\n",
       "      <td>38.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20190122</th>\n",
       "      <td>37.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201223</th>\n",
       "      <td>130.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201224</th>\n",
       "      <td>133.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201228</th>\n",
       "      <td>137.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201229</th>\n",
       "      <td>134.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201230</th>\n",
       "      <td>133.450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          result_price\n",
       "DATE                  \n",
       "20190115        37.209\n",
       "20190116        37.477\n",
       "20190117        38.280\n",
       "20190118        38.013\n",
       "20190122        37.464\n",
       "...                ...\n",
       "20201223       130.700\n",
       "20201224       133.360\n",
       "20201228       137.400\n",
       "20201229       134.910\n",
       "20201230       133.450\n",
       "\n",
       "[495 rows x 1 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7402b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoluted Error\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_23895/3014078297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Absoluted Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_test_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconverted_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## Mean Absolute Error of the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m     ):\n\u001b[0;32m-> 1936\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marraylike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_ufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[0;31m# ideally we would define this to avoid the getattr checks, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# for binary ops, use our custom dunder methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_dispatch_ufunc_to_dunder_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/ops_dispatch.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rsub__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__mul__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4998\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5000\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;31m# We do not pass dtype to ensure that the Series constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m         \u001b[0;31m#  does inference in the case where `result` has object-dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2763\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2764\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "print(\"Mean Absoluted Error\")\n",
    "np.mean(abs(np.array(old_test_y.loc[:,])-converted_pred)) ## Mean Absolute Error of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaae93c",
   "metadata": {},
   "source": [
    "### Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3785eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(old_test_y.index, old_test_y[\"result_price\"], label = \"Actual\", color = 'Black')\n",
    "plt.plot(reverted_predict_y.index, reverted_predict_y, label = \"Predicted\", color = 'Orange')\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.title(\"Prediction of \"+stock.upper()+\" using SVR\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"plot/SVR/\"+stock.upper()+\"-day(\"+str(num_day_to_predict)+\")kernel(\"+kernel+\")C(\"+str(C)+\")gamma(\"+str(gamma)+\")confidence(\"+str(round(svm_confidence,5))+\").jpg\",\n",
    "            dpi=600)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353c71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
